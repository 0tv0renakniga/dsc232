(electronic music) - [Instructor] Hi. We talked about root mean square there and the variety of ways in
which it can be used to model. What I want to do now is
open the scene a little bit and compare root mean squared
error and predictive models and classification models
to generative models, which is the whole other
approach to modeling. So to talk about it let's talk about a simple toy example where a computer receives a telephone call and measures the pitch of
the voice of the caller and then decides on
the gender of the call. Okay, so that's, the
gender is what we're trying to understand, what
we're trying to predict. So there's the caller, makes a call to the
telephone, to the computer, and then either decides that
it's either a male or a female. Okay, so what is the generative
approach to modeling this? The generative approach
says there are two classes, males and females, and we wanna characterize
the distribution of each one. So to do that, we first
measure, we have this data that we collected, and we measure the mean and the variance of the female, of the male class, and then
the mean and the variance of the female class. Okay, so the female
tend to be higher pitch and so we get a higher
mean for the female. And now because we don't know
much about the distribution we say okay, let's assume
the distribution is Gaussian. And so we fit a Gaussian
to the mean and variance of the male, and to the mean
and variance of the female. And now when we have that we can make the optimal decision based on where these two graphs cross. Okay, so this is the what is called the base optimal decision assuming that the data is
really generated by Gaussian. Okay? So that's generative modeling, and it has very good properties when the data is really
generated according to the assumed model, which is a Gaussian in this case. The predictive approach
is quite different. It takes the same data that we had before, but instead of trying to
characterize the statistics of this data, it just tries to find a good
rule based on this data. Okay, so it compares all
of the possible thresholds. We're assuming that the thresholds are the rule that we want and then it plots the number
of mistakes for each threshold. So this is this graph. What we have at each, at each location is for a threshold that goes here, what is the error on the training data or the empirical error? And we also potentially try thresholds where the threshold says
it's lower than something then it's a female, but that rule gets this
error that is pretty high. So we can ignore that case. And now based on these, we decide on one of the
minimum that is the threshold that we're going to use. And in this case, we see
that we have two minimum. And so the question is what do we do then? And the answer is it doesn't matter just choose one of them. So just from that, you
immediately say, well that cannot be optimal, right? That's true. However, it is much more
robust in the following sense. So suppose that we have
data that looks like this. So here we have females
that are higher in pitch that these males, but here are four
people, four measurements that are very high pitched and are mixed both male and female. Okay. So these clearly are outliers, but and we would be happy
to simply be wrong on them, but the problem is that they're going to change our estimates very much. So here is the mean and standard deviation that we get for the male, and then from the female. And the Gaussians that
we collect for them, that we fit for them are these two. The blue line for the male and the red line for the female. And now the threshold is
going to be somewhere here where these graphs cross. And if we look at this threshold, it says anything that is
below is a male, okay. But we're going to be now wrong on all of these female, right. And why are we wrong? Because our assumption
that the data is generated by a Gaussian is wrong. So these few examples
have a very high influence on the means of the estimate. What happens if we do, if we let me clear that. What happens if we do for the same data, we take the discriminative approach? Okay, so in this case, what we have is this is the graph of the minimum of the
errors for each threshold. So we see that these
males and females here have some impact but not a very big impact on what is the minimum. They really, they don't
affect the global minimum. So the global minimum is what we choose and indeed this is a very good threshold. Okay. So what can we say in general? In general, we can say that using the discriminative approach, finding the rule that
has the minimum error on the training data has guarantees of being almost optimal. It's not quite optimal if the data is generated
according to what you assume in the generative case,
but it is much more robust. It doesn't depend on any assumptions about the underlying distribution. Now, if you are an avid statistician and you want to defend using
a model that is generative, you'd say well, if I
used this distribution that allows for a very long tail then I would actually
find the right threshold. And that might be true. The problem is that
finding the right shape of the distribution also takes
significant amount of data. And so the question is, are we going to really study, collect enough data that we can know what is the characteristics
of the distribution? Or are we just going to
use the data that we have to optimize our threshold? So that's really the difference. Okay, so this goes to the late Leo Breiman who had a a nice kind of way of discriminating between
statistics and machine learning. He said in statistics, you take data and you make from it an
estimate of what the world is. And then you use decision
theory to make predictions. But in machine learning,
you just jump over this step and you go directly from data
to predictions and actions. Now, what we also know today is that that is a kind of black box approach and sometimes this black box approach that doesn't relate to anything that we can understand
in nature or in the data is limited, but in principle, it allows you to jump over estimating what the state of the world is and just finding a good prediction rule. Okay, so to distinguish between those, I'm going to distinguish
generative and predictive and I'm going to talk
both about classification, what we talked about here,
and about root mean square, which we talked about a few videos ago. And so the goal here in generative is to get probability
estimates of the distribution. The goal when you're predictive if you're discriminative,
it's classification. So something that tells
you which class the data is and the prediction rule,
which tells you a point in vector space, which is where
the data is estimated to be. The performance measure
here is likelihood. This one is misclassification rate and here it is, root mean square. Okay? And the mismatch problem
here you have outliers if you have a mismatch, meaning that you're using
a model that is not correct and here you just have misclassification or potentially in RMS, large errors, okay. So you have some handle
on what's happening with things that don't fit your model. Okay, so to summarize, generative models the goal is to explain
how data is generated. And in predictive models the goal is to predict the
property such as a label or as a vector that describes the depth of the snow. Generative models are more
accurate when they're correct. Okay, so when you can
find the exact, the type the exact form of the distribution, then generative models are more accurate. On the other hand, predictive models are more
robust against wrong modeling. And finally classification models are more robust against outliers. So they don't really change
their prediction very much. If you have a few outliers with the wrong, with classes that don't
fit the general model, they don't change the best
model of the classification. Okay. So that ends the generative
versus predictive description. And I'll see you the next time.