Here are the converted Markdown files based on your instructions. I have combined the weather notebooks into `7.1_notebook.md` and separated the others as requested, preserving all details and outputs.

### 7.1_notebook.md

*(Contains: Processing Weather Files, Weather Analysis - VT, and Weather Analysis - MA)*

```markdown
# Processing Weather Files

### NOAA Global Historical Climatology Network Daily (GHCN-D)

the noaa data was downloaded from an S3 bucket descried here

https://registry.opendata.aws/noaa-ghcn/

#### The files and subdirs in noaa

### Main Data Directory
* **98G	csv.gz**

### Documentation
* 36K	ghcn-daily-by_year-format.rtf
* 6.5K	ghcnd-countries.txt
* 32M	ghcnd-inventory.txt
* 6.5K	ghcnd-states.txt
* 9.9M	ghcnd-stations.txt
* 6.5K	ghcnd-version.txt
* 36K	index.html
* 4.1M	mingle-list.txt
* 6.5K	readme-by_year.txt
* 30K	readme.txt
* 6.5K	status-by_year.txt
* 41K	status.txt

### Logs
* 37M	download.log

```python
import pandas as pd

```

```python
!head -4 file.sizes
"""
RETURNS ->

total 102208244
-rw-r--r-- 1 yfreund root      532654 Apr  5 15:59 1750.csv
-rw-r--r-- 1 yfreund root       25276 Apr  5 16:00 1763.csv
-rw-r--r-- 1 yfreund root       25283 Apr  5 16:00 1764.csv
"""

```

```python
!tail -4 file.sizes
"""
RETURNS ->

-rw-r--r-- 1 yfreund root  1220722777 Apr  5 15:57 2020.csv
-rw-r--r-- 1 yfreund root  1207975107 Apr  5 15:58 2021.csv
-rw-r--r-- 1 yfreund root   276207286 Apr  5 15:59 2022.csv
-rw-r----- 1 yfreund users          0 Apr  9 16:22 file.sizes
"""

```

```python
i=0
D={}
with open('file.sizes','r') as S:
    S.readline(); S.readline()
    for line in S.readlines():
        if 'file' in line:
            continue
        #print(line[:-1])
        L=line.split()
        size=int(L[4])
        year=int(L[-1][:-4])
        #print(year,size)
        D[year]=size

```

```python
years=[]
sizes=[]
for year in sorted(D.keys()):
    years.append(year)
    sizes.append(D[year])

```

```python
%pylab inline
"""
RETURNS ->

%pylab is deprecated, use %matplotlib inline and import the required libraries.
Populating the interactive namespace from numpy and matplotlib
"""

```

```python
#figure(figsize=[15,10])
plot(years,sizes)
grid()
title('Size of GHCN-Daily data files (in Bytes) by year')
ylabel('Bytes')
xlabel('Year')
"""
RETURNS ->

[Plot/Image Output: Size of GHCN-Daily data files by year]
"""

```

---

# Weather Analysis - VT Voc Quiz 4

## Weather Data : Initial Visualization

### For VT

```python
state="VT"

```

```python
#sc.stop()

```

```python
import pandas as pd
import numpy as np
import sklearn as sk
%pylab inline
"""
RETURNS ->

%pylab is deprecated, use %matplotlib inline and import the required libraries.
Populating the interactive namespace from numpy and matplotlib
"""

```

```python
%%time
%run lib/startup-voc.py
"""
RETURNS ->

172.17.0.2
sparkContext= <SparkContext master=local[2] appName=pyspark-shell>

    pandas as    pd 	version=2.0.3 	required version>=0.19.2

     numpy as    np 	version=1.24.4 	required version>=1.12.0

   sklearn as    sk 	version=1.3.1 	required version>=0.18.1

module urllib has no version
   pyspark as pyspark 	version=3.5.0 	required version>=2.1.0

ipywidgets as ipywidgets 	version=8.1.1 	required version>=6.0.0

version of ipwidgets= 8.1.1

measurements is a Dataframe (and table) with 12720632 records
stations is a Dataframe (and table) with 119503 records
weather is a Dataframe (and table) which is a join of measurements and stations with 12720632 records
CPU times: user 181 ms, sys: 33.1 ms, total: 214 ms
Wall time: 5.4 s
"""

```

```python
import warnings  # Suppress Warnings
warnings.filterwarnings('ignore')

_figsize=(10,7)

```

## Read Data

### show 3 rows of joined weather+stations table

```python
%%time
### Total number of stations
stations.count()
"""
RETURNS ->

CPU times: user 675 µs, sys: 77 µs, total: 752 µs
Wall time: 55.2 ms
119503
"""

```

```python
%%time
weather=measurements.join(stations,on='station')
weather.show(3)
"""
RETURNS ->

+-----------+-----------+----+--------------------+--------+---------+---------+------------+-------------------+-----+-------+
|    Station|Measurement|Year|              Values|latitude|longitude|elevation|  dist2coast|               name|state|country|
+-----------+-----------+----+--------------------+--------+---------+---------+------------+-------------------+-----+-------+
|AG000060390|       TAVG|2022|[79 00 6C 00 66 0...| 36.7167|     3.25|     24.0|   8.0234375| ALGER-DAR EL BEIDA|     |Algeria|
|AGE00147716|       TAVG|2022|[85 00 83 00 7C 0...|    35.1|    -1.85|     83.0|0.5224609375|NEMOURS (GHAZAOUET)|     |Algeria|
|AGM00060360|       TMIN|2022|[5A 00 19 FC 4B 0...|  36.822|    7.809|      4.9|  3.16015625|             ANNABA|     |Algeria|
+-----------+-----------+----+--------------------+--------+---------+---------+------------+-------------------+-----+-------+
only showing top 3 rows

CPU times: user 2.43 ms, sys: 1.19 ms, total: 3.61 ms
Wall time: 892 ms
"""

```

```python
weather.count()
"""
RETURNS ->

12720632
"""

```

```python
sqlContext.registerDataFrameAsTable(weather,'weather')

```

```python
ms=['TMAX', 'SNOW', 'SNWD', 'TMIN', 'PRCP', 'TOBS']
# ms=['TMAX', 'TMIN', 'TOBS']
cms='or\n'.join(['Measurement="%s" '%(m) for m in ms])

##  read all data for state
Query="""
SELECT *
FROM weather
WHERE state="%s" and 
(%s)
"""%(state,cms)
print(Query)
"""
RETURNS ->


SELECT *
FROM weather
WHERE state="VT" and 
(Measurement="TMAX" or
Measurement="SNOW" or
Measurement="SNWD" or
Measurement="TMIN" or
Measurement="PRCP" or
Measurement="TOBS" )

"""

```

```python
%%time
weather_df=sqlContext.sql(Query)
print('number of rows in result=',weather_df.count())
weather_df.show(2)
"""
RETURNS ->

number of rows in result= 27068
+-----------+-----------+----+--------------------+--------+---------+---------+----------+----------------+-----+-------------+
|    Station|Measurement|Year|              Values|latitude|longitude|elevation|dist2coast|            name|state|      country|
+-----------+-----------+----+--------------------+--------+---------+---------+----------+----------------+-----+-------------+
|US1VTAD0019|       SNOW|2022|[00 00 00 00 0D 0...| 44.1006| -73.1172|    151.5|     222.0|NEW HAVEN 2.4 SE|   VT|United States|
|US1VTAD0019|       SNWD|2022|[33 00 00 00 19 F...| 44.1006| -73.1172|    151.5|     222.0|NEW HAVEN 2.4 SE|   VT|United States|
+-----------+-----------+----+--------------------+--------+---------+---------+----------+----------------+-----+-------------+
only showing top 2 rows

CPU times: user 1.02 ms, sys: 2.18 ms, total: 3.2 ms
Wall time: 1.52 s
"""

```

### read or compute statistics information for state.

```python
%%time
import os.path
from lib.computeStatistics import computeStatistics
from pickle import dump,load

weather_dir=parquet_root+'/weather_statistics/'
if not os.path.isdir(weather_dir):
    os.mkdir(weather_dir)

pkl_filename=parquet_root+'weather-statistics/'+state+'-'+','.join(ms)+'.pkl'
print(pkl_filename)
!ls $pkl_filename
"""
RETURNS ->

../Data/Weather/weather/datasets/weather-statistics/VT-TMAX,SNOW,SNWD,TMIN,PRCP,TOBS.pkl
../Data/Weather/weather/datasets/weather-statistics/VT-TMAX,SNOW,SNWD,TMIN,PRCP,TOBS.pkl
CPU times: user 4.95 ms, sys: 3.54 ms, total: 8.49 ms
Wall time: 215 ms
"""

```

```python
%%time
if os.path.isfile(pkl_filename):   
    print('precomputed statistics file exists')
    with open(pkl_filename,'br') as pkl_file:
        STAT=load(pkl_file)
else:
    print('computing statistics')
    STAT=computeStatistics(sqlContext,weather_df,measurements=ms)
    with open(pkl_filename,'bw') as pkl_file:
        dump(STAT,pkl_file)

STAT.keys()
"""
RETURNS ->

precomputed statistics file exists
CPU times: user 0 ns, sys: 21.2 ms, total: 21.2 ms
Wall time: 34.9 ms
dict_keys(['TMAX', 'SNOW', 'SNWD', 'TMIN', 'PRCP', 'TOBS'])
"""

```

```python
ms=['TMAX', 'SNOW', 'SNWD', 'TMIN', 'PRCP', 'TOBS']
# ms=['TMAX', 'TMIN', 'TOBS']
cms='or\n'.join(['Measurement="%s" '%(m) for m in ms])
cms
"""
RETURNS ->

'Measurement="TMAX" or\nMeasurement="SNOW" or\nMeasurement="SNWD" or\nMeasurement="TMIN" or\nMeasurement="PRCP" or\nMeasurement="TOBS" '
"""

```

```python
print("   Name  \t                 Description             \t  Size")
print("-"*80)
print('\n'.join(["%10s\t%40s\t%s"%(s[0],s[1],str(s[2])) for s in STAT_Descriptions]))
"""
RETURNS ->

   Name  	                 Description             	  Size
--------------------------------------------------------------------------------
     UnDef	      sample of number of undefs per row	vector whose length varies between measurements
        NE	         count of defined values per day	(366,)
SortedVals	                        Sample of values	vector whose length varies between measurements
      mean	                              mean value	()
       std	                                     std	()
    low100	                               bottom 1%	()
   high100	                                  top 1%	()
   low1000	                             bottom 0.1%	()
  high1000	                                top 0.1%	()
         E	                   Sum of values per day	(366,)
      Mean	                                    E/NE	(366,)
         O	                   Sum of outer products	(366, 366)
        NO	               counts for outer products	(366, 366)
       Cov	                                    O/NO	(366, 366)
       Var	  The variance per day = diagonal of Cov	(366,)
    eigval	                        PCA eigen-values	(366,)
    eigvec	                       PCA eigen-vectors	(366, 366)
"""

```

### print statistics for TOBS

```python
S=STAT['TOBS']
for key in ['mean', 'std', 'low100', 'high100']:
    element=S[key]
    print(key,'=',end='')
    if type(element)==numpy.float64 or type(element)==numpy.float16:
        print('%6.2f'%element)
    elif type(element)==numpy.ndarray:
        print (element)
    else:
        print('unidentified type=',type(element))
"""
RETURNS ->

mean =  5.40
std = 11.71
low100 =-19.41
high100 =-19.41
"""

```

```python
%pylab inline
measurement='TOBS'
Sobs=STAT[measurement]['SortedVals']

#figure(figsize=[15,10])
n_obs=Sobs.shape[0]
p=arange(0,1,1/n_obs)
plot(Sobs,p)
title('CDF of '+measurement)
grid()
"""
RETURNS ->

%pylab is deprecated, use %matplotlib inline and import the required libraries.
Populating the interactive namespace from numpy and matplotlib
[Plot/Image Output: CDF of TOBS]
"""

```

### distribution of undefined elements in yearly measurements.

```python
for m in ms:
    figure()
    undef=STAT[m]['UnDef']
    hist(undef,bins=arange(0,400,10))
    title(m)
    grid()
"""
RETURNS ->

[Plot/Image Output: Histograms for TMAX, SNOW, SNWD, TMIN, PRCP, TOBS]
"""

```

### Mean and STD of temperature

```python
def plot_mean_std(m):
    #figure(figsize=[15,10])
    Mean=STAT[m]['Mean']
    Var=STAT[m]['Var']
    plot(Mean,label='Mean')
    plot(Mean+sqrt(Var),label='Mean+std')
    plot(Mean-sqrt(Var),label='Mean-std')
    grid()
    legend()
    title(m)

```

```python
plot_mean_std('TMAX')
"""
RETURNS ->

[Plot/Image Output: Mean/STD for TMAX]
"""

```

```python
def plot_pair(meas,plot_function):
    figure(figsize=(12,4))
    subplot(1,2,1); plot_function(meas[0])
    subplot(1,2,2); plot_function(meas[1])

```

```python
plot_pair(['TMIN', 'TMAX'],plot_mean_std)
"""
RETURNS ->

[Plot/Image Output: Pair plots for TMIN and TMAX]
"""

```

```python
plot_pair(['SNOW', 'SNWD'],plot_mean_std)
"""
RETURNS ->

[Plot/Image Output: Pair plots for SNOW and SNWD]
"""

```

---

# Weather Analysis - MA Voc Quiz 4

## Weather Data : Initial Visualization

### Massachusetts

```python
state="MA"

```

```python
#sc.stop()

```

```python
import pandas as pd
import numpy as np
import sklearn as sk
%pylab inline
"""
RETURNS ->

%pylab is deprecated, use %matplotlib inline and import the required libraries.
Populating the interactive namespace from numpy and matplotlib
"""

```

```python
%%time
%run lib/startup-voc.py
"""
RETURNS ->

172.17.0.2
sparkContext= <SparkContext master=local[2] appName=pyspark-shell>

    pandas as    pd 	version=2.0.3 	required version>=0.19.2

     numpy as    np 	version=1.24.4 	required version>=1.12.0

   sklearn as    sk 	version=1.3.1 	required version>=0.18.1

module urllib has no version
   pyspark as pyspark 	version=3.5.0 	required version>=2.1.0

ipywidgets as ipywidgets 	version=8.1.1 	required version>=6.0.0

version of ipwidgets= 8.1.1

measurements is a Dataframe (and table) with 12720632 records
stations is a Dataframe (and table) with 119503 records
weather is a Dataframe (and table) which is a join of measurements and stations with 12720632 records
CPU times: user 202 ms, sys: 16.9 ms, total: 219 ms
Wall time: 5.71 s
"""

```

```python
sc
"""
RETURNS ->

<SparkContext master=local[2] appName=pyspark-shell>
"""

```

```python
import warnings  # Suppress Warnings
warnings.filterwarnings('ignore')

_figsize=(10,7)

```

## Read Data

### show 3 rows of joined weather+stations table

```python
### Total number of stations
stations.count()
"""
RETURNS ->

119503
"""

```

```python
%%time
weather=measurements.join(stations,on='station')
weather.show(3)
"""
RETURNS ->

+-----------+-----------+----+--------------------+--------+---------+---------+------------+-------------------+-----+-------+
|    Station|Measurement|Year|              Values|latitude|longitude|elevation|  dist2coast|               name|state|country|
+-----------+-----------+----+--------------------+--------+---------+---------+------------+-------------------+-----+-------+
|AG000060390|       TAVG|2022|[79 00 6C 00 66 0...| 36.7167|     3.25|     24.0|   8.0234375| ALGER-DAR EL BEIDA|     |Algeria|
|AGE00147716|       TAVG|2022|[85 00 83 00 7C 0...|    35.1|    -1.85|     83.0|0.5224609375|NEMOURS (GHAZAOUET)|     |Algeria|
|AGM00060360|       TMIN|2022|[5A 00 19 FC 4B 0...|  36.822|    7.809|      4.9|  3.16015625|             ANNABA|     |Algeria|
+-----------+-----------+----+--------------------+--------+---------+---------+------------+-------------------+-----+-------+
only showing top 3 rows

CPU times: user 2.15 ms, sys: 0 ns, total: 2.15 ms
Wall time: 875 ms
"""

```

```python
sqlContext.registerDataFrameAsTable(weather,'weather')

```

```python
ms=['TMAX', 'SNOW', 'SNWD', 'TMIN', 'PRCP', 'TOBS']
# ms=['TMAX', 'TMIN', 'TOBS']
cms='or\n'.join(['Measurement="%s" '%(m) for m in ms])

##  read all data for state
Query="""
SELECT *
FROM weather
WHERE state="%s" and 
(%s)
"""%(state,cms)
print(Query)
"""
RETURNS ->


SELECT *
FROM weather
WHERE state="MA" and 
(Measurement="TMAX" or
Measurement="SNOW" or
Measurement="SNWD" or
Measurement="TMIN" or
Measurement="PRCP" or
Measurement="TOBS" )

"""

```

```python
%%time
weather_df=sqlContext.sql(Query)
print('number of rows in result=',weather_df.count())
weather_df.show(2)
"""
RETURNS ->

number of rows in result= 49710
+-----------+-----------+----+--------------------+--------+---------+---------+-------------+--------------------+-----+-------------+
|    Station|Measurement|Year|              Values|latitude|longitude|elevation|   dist2coast|                name|state|      country|
+-----------+-----------+----+--------------------+--------+---------+---------+-------------+--------------------+-----+-------------+
|US1MABA0017|       PRCP|2022|[00 00 77 00 00 0...| 41.5828| -70.5803|     12.8| 0.9072265625|EAST FALMOUTH 1.2...|   MA|United States|
|US1MABA0018|       SNOW|2022|[00 00 00 00 19 F...| 41.5818| -70.5257|      9.8|0.94091796875|     WAQUOIT 0.6 SSW|   MA|United States|
+-----------+-----------+----+--------------------+--------+---------+---------+-------------+--------------------+-----+-------------+
only showing top 2 rows

CPU times: user 2.8 ms, sys: 0 ns, total: 2.8 ms
Wall time: 1.5 s
"""

```

### read or compute statistics information for state.

```python
%%time
import os.path
from lib.computeStatistics import computeStatistics
from pickle import dump,load

pkl_filename=parquet_root+'weather-statistics/'+state+'-'+','.join(ms)+'.pkl'
print(pkl_filename)
!ls $pkl_filename
"""
RETURNS ->

../Data/Weather/weather/datasets/weather-statistics/MA-TMAX,SNOW,SNWD,TMIN,PRCP,TOBS.pkl
../Data/Weather/weather/datasets/weather-statistics/MA-TMAX,SNOW,SNWD,TMIN,PRCP,TOBS.pkl
CPU times: user 2.44 ms, sys: 5.91 ms, total: 8.36 ms
Wall time: 217 ms
"""

```

```python
%%time
if os.path.isfile(pkl_filename):   
    print('precomputed statistics file exists')
    with open(pkl_filename,'br') as pkl_file:
        STAT=load(pkl_file)
else:
    print('computing statistics')
    STAT=computeStatistics(sqlContext,weather_df,measurements=ms)
    with open(pkl_filename,'bw') as pkl_file:
        dump(STAT,pkl_file)

STAT.keys()
"""
RETURNS ->

precomputed statistics file exists
CPU times: user 0 ns, sys: 27.7 ms, total: 27.7 ms
Wall time: 38 ms
dict_keys(['TMAX', 'SNOW', 'SNWD', 'TMIN', 'PRCP', 'TOBS'])
"""

```

```python
print("   Name  \t                 Description             \t  Size")
print("-"*80)
print('\n'.join(["%10s\t%40s\t%s"%(s[0],s[1],str(s[2])) for s in STAT_Descriptions]))
"""
RETURNS ->

   Name  	                 Description             	  Size
--------------------------------------------------------------------------------
     UnDef	      sample of number of undefs per row	vector whose length varies between measurements
        NE	         count of defined values per day	(366,)
SortedVals	                        Sample of values	vector whose length varies between measurements
      mean	                              mean value	()
       std	                                     std	()
    low100	                               bottom 1%	()
   high100	                                  top 1%	()
   low1000	                             bottom 0.1%	()
  high1000	                                top 0.1%	()
         E	                   Sum of values per day	(366,)
      Mean	                                    E/NE	(366,)
         O	                   Sum of outer products	(366, 366)
        NO	               counts for outer products	(366, 366)
       Cov	                                    O/NO	(366, 366)
       Var	  The variance per day = diagonal of Cov	(366,)
    eigval	                        PCA eigen-values	(366,)
    eigvec	                       PCA eigen-vectors	(366, 366)
"""

```

### print statistics for TOBS

```python
S=STAT['TOBS']
for key in ['mean', 'std', 'low100', 'high100']:
    element=S[key]
    print(key,'=',end='')
    if type(element)==numpy.float64 or type(element)==numpy.float16:
        print('%6.2f'%element)
    elif type(element)==numpy.ndarray:
        print (element)
    else:
        print('unidentified type=',type(element))
"""
RETURNS ->

mean =  8.31
std = 10.31
low100 =-11.70
high100 = 30.59
"""

```

```python
%pylab inline
measurement='TOBS'
Sobs=STAT[measurement]['SortedVals']

#figure(figsize=[15,10])
n_obs=Sobs.shape[0]
p=arange(0,1,1/n_obs)
plot(Sobs,p)
title('CDF of '+measurement)
grid()
"""
RETURNS ->

%pylab is deprecated, use %matplotlib inline and import the required libraries.
Populating the interactive namespace from numpy and matplotlib
[Plot/Image Output: CDF of TOBS]
"""

```

### distribution of undefined elements in yearly measurements.

```python
for m in ms:
    figure()
    undef=STAT[m]['UnDef']
    hist(undef,bins=arange(0,400,10))
    title(m)
    grid()
"""
RETURNS ->

[Plot/Image Output: Histograms for TMAX, SNOW, SNWD, TMIN, PRCP, TOBS]
"""

```

### Mean and STD of temperature

```python
def plot_mean_std(m):
    #figure(figsize=[15,10])
    Mean=STAT[m]['Mean']
    Var=STAT[m]['Var']
    plot(Mean,label='Mean')
    plot(Mean+sqrt(Var),label='Mean+std')
    plot(Mean-sqrt(Var),label='Mean-std')
    grid()
    legend()
    title(m)

```

```python
plot_mean_std('TMAX')
"""
RETURNS ->

[Plot/Image Output: Mean/STD for TMAX]
"""

```

```python
def plot_pair(meas,plot_function):
    figure(figsize=(12,4))
    subplot(1,2,1); plot_function(meas[0])
    subplot(1,2,2); plot_function(meas[1])

```

```python
plot_pair(['TMIN', 'TMAX'],plot_mean_std)
"""
RETURNS ->

[Plot/Image Output: Pair plots for TMIN and TMAX]
"""

```

```python
plot_pair(['SNOW', 'SNWD'],plot_mean_std)
"""
RETURNS ->

[Plot/Image Output: Pair plots for SNOW and SNWD]
"""

```

## Conclusion

* We loaded the weather data from Parquet files.
* We explored statistics for the data.
* We explored where there are a lot of empty cells - limits the accuracy of the statistics.
* We visualized different measurements as a function of the day of the year.
* **next** Using PCA for more refined analysis.


