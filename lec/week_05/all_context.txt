--- start{10.1_lec.pdf} ---
 The RMSE Methodology
(Root Mean Square Error )
 Real World




Mathematical
   Model
The best constant prediction is the
              mean




•
PCA based prediction


                           ox

                   xoo
                  o    x
               x x
                o
                  x
          xo
      o
      x
Regression based Prediction




                        o
                        x
                  x o
                  oox
               x ox
               o x
           x
           o
       o
       x
K-means clustering
    Percentage of variance explained




•
             PCA block diagram

      K Eigenvectors




                                                 Approximate
 K vector
                                Model           Reconstruction
coeﬃcients
                                                 Of the vector


                                                        or
                                                       r
                                                     er
                                                ed
                                              ar
                   Single year,station,     u
                                          Sq
                      measurement
                          vector
     Spatial/temporal block
            diagram
 Average per Year                      Fix measurement
Average per station                   Fix coeﬃcient index




Station
                           Model           Estim. Coeﬃcient i
 year


                                                     ro r
                                              d   er
                                           re
                      • Station         ua
                                      Sq
                      • Year
                      • Value of
                        Coeﬃcient i
  Model-based approximation
        block diagram
        Learned Model
         Parameters                         Some things are ﬁxed




 Compressed
Representation                                     Approximate
                             Model
      Of                                          Reconstruction
 Single Point

                                                          or
                                                         r
                                                       er
                                                  ed
                                                ar
                                              u
                                            Sq
                        Single Data Point
          Extensive and Intensive properties
     • Extensive: size scales with size of data
     • Intensive: size does not scale with size of data
        Learned Model
         Parameters



 Compressed
Representation                                           Approximate
                                Model
      Of                                                Reconstruction
 Single Point

                                                                 o r
                                                                r
                                                             er
                                                        ed
                                                      ar
                                                    u
                                                  Sq
                           Single Data Point
    What is a good model?




•
 Model-based
• Given d-dimensional data
                           approximation
 points. (here d=2)

• Fit a simple model with few        parameters:
 parameters. (Here Curve)
                                                    0
• Each data point is
 approximated by the closest
 point on the curve.

• The point on the curve is       error      0.52

 represented by a few numbers
                                                        1
 (dimensionality reduction)

• The distance between the
 point and the approximation is
 the error.

• We want the average
 root-mean-square-error
 (RMSE) or Percentage of
    Summary




•
   Variations on a theme


• diﬀerent model geometric: lines, curves, points

• diﬀerent measures of error: distance from projection,
  distance from closest point ﬁxing x, non-euclidean
  distances…
            Diﬀerent approaches to
•
             statistical modelling
    Diﬀerent Models for Diﬀerent Goals

    •   RMSE approach: Model maps each example to a simpliﬁed approximation.

        •   Examples PCA, Regression, K-means

        •   Performance measured using average reconstruction error (RMSE)

        •   Other Goal: capture parameters of true distribution.

    •   Generative approach: model is a generator of examples.

        •   Examples: Mixture of Gaussians, HMMs

        •   Generated distribution approximates true distribution.

        •   Performance measured using likelihood.

        •   Main Goal: capture parameters of true distribution.

    •   Discriminative approach: model predicts output given input.

        •   Examples: Perceptron, Decision trees, Neural Networks

        •   Generated predictor approximated input -> output relationship.

        •   Performance measured using average loss (RMSE, # of mistakes, …)
  Example of generative vs
    predictive modeling
• Computer receives telephone call
• Measures Pitch of voice
• Decides gender of caller

                                     Mal
                                     e
            Human
            Voice


                                           17
              Generative modeling

                    mean1   mean
                            2
Probabilit




             var1                  var2
y




                                          Voice
                                          Pitch
Discriminative approach
       [Vapnik
       85]




                     Voice
                     Pitch
          Well Behaved Data



• Data for male/female generated according to normal distributions.
• Estimated Mean and variance converge quickly to true values.
• Generative method converges (much) faster than discriminative method.
• Only viable method when number of training examples is small
   (<10,000).
              Ill-behaved data


                mean
                       mean
                1
 Probabilit




                       2
mistakes
No. of
 y




                                 Voice
                                 Pitch
      Traditional Statistics vs.
         Machine Learning

                    Machine
                    Learning
                  Estimated    Prediction
Dat                 world          s
 a    Statistic                 Actions
                    state
      s
                 Deep Learning
• In the discriminative family.
• Claim: because we have many labeled training examples,
  we can use as many parameters as we want and never
  overfit.
• Approach: Use models with many parameters, and train
  until training error 0.
• Famous success cases: Alpha-Go, machine
  translation.
• Problems:
  – Restricted labels: can we make use of unlabeled data
    (auto-encoders vs PCA)
  – Collecting a representative training set can be        23
Back To Kmeans




                 24

--- end {10.1_lec.pdf} ---
--- start{10.1_transcript.txt} ---
(upbeat music) - [Instructor] So we have worked with principal component analysis, and saw what it can do for us and how it can explain
the variance in the data. And this points to a more
general set of methods, where principle component
analysis is just one of them, all based on minimizing
root mean square error. So we're going to do a
little review of them and just about the way of thinking about a problem that lends itself to doing root mean square
error based modeling. So we have a situation where
we're getting some data. The data comes from the real world. It's not data that we generated. So therefore we don't know
exactly how to model it. But what we do is we find some kind of simple mathematical model. This is this blue box. And using this model, we generate predictions of
the different measurements. And so if our model is good, then our predictions will
be close to the values that they correspond to. The way that we typically would measure how close they are in this RMSE approach is that we take the difference between x, the vector that we observe, and o, the vector that we predict, and take the square of
the norm of that, okay? And then we average that
over all our examples so that the root mean squared error. So this is because of the root of the mean of the squared error, okay? So that's the basic idea
of what we're going to do. So let's look at the simplest case in which we're just trying
to predict a set of numbers. Okay, so we have numbers,
data points x1, x2, xn in R to the D, in Euclidean
space to the D, dimension D. And then we want to find a single point such that the mean squared error to the points that were given is minimum. And it turns out that the thing that minimizes this
RMSE is the mean, okay? So the mean of all of the points gives us the best point
in terms of representing all of these points with
a single point, okay? That's a nice property of squared error. Next, let's think about what we do when we do principle component analysis. So now we have data that is in R2. These data points that we see here, and we want to find
the top K eigenvectors. Now here, the top K, the
total dimension is 2, so the top K has to be 1. So we have some dimension one, and we basically take as the
reconstruction or the output, we take the mean plus the
projection on the first vector. And how does that look? That looks like something
like a line that is like that. That is the direction of v1. And when we want to
measure how good this is, we measure the distance from each point to the closest point on the line, to the projection on the line. That is the thing that we're minimizing. And that's the thing that gives us, if these distances on average are small, then we say that this is a good
representation of the data. So that's the kind of thing that we have been doing
in the weather data. There's another approach. There's another approach
to explaining data. And that is if you have
one of the coordinates, let's say y here, which is
the preferred coordinate. That's the coordinate we want to predict. All the others are just input. In that case, what we're
talking about is regression. We have a set of points,
x1, y1, x2, y2 and so on where x is a vector
and y is just a scalar. And then, the thing that we're
trying to predict is y. So we generate y hat, which is a linear
combination of a constant plus a coefficient multiplied
by the ith coordinate, okay? When we do that, we get a line that is slightly different than the line we got before. And that is because the
distances that we're minimizing are these vertical distances
between x and the line, the point on the line, rather than the closest point on the line. So this is based on assuming that y is the most important thing
that we're trying to predict. And we basically generate
the approximations as the points that are on the line and that are at the same x
value as the point itself. And the RMS Error here is
when we take the difference between yi and yi hat, okay? So now it's a difference that
is going on the vertical axis. Now let's talk about something different. K-means clustering. You're probably all familiar
with finding the k points that are closest to all the points. The one mean we already
said it's the mean. It's the mean of all the data. But the two mean or three mean is a more complicated problem, where we're trying to
find here two points, such that for each point,
the closest point to it is the closest representative
for each of the data points, the closest representative
to this data point. That's the distance that we care about, and that's what we're trying to minimize. So the RMS Error here in this case is xi, the data itself minus oi where oi is the representative
closest to the point xi. Okay, so all of these generate quantities that are like variance, that are like sum of squares of things. And so what we can say is
how much of the variance are we explaining using our model? So RMSE changes with the
units of the measure, right? So if we measure in kilometers or in millimeters or in centimeters, we get different values. We want to normalize it somehow. And so the standard way is to normalize by seeing what is the trivial model. What is the model that
is the most basic model at the level below us? So for PCA we said it's the mean, okay? For K-means, it would
probably also be the mean because that's the explanation
using one center, okay? So for PCA we use the RMSE k. So how much of the variance is explained by the top k eigenvectors, or what's the sum of the top K eigenvalues divided by the sum of
all of the eigenvalues? And similarly for K-means, the explanation power, the RMSE of k, divided by the RMSE of
using just one center, okay? So in both of these cases, we're looking at how much
of the explanatory power, how much are we improving over the explanatory power
of the most basic model. Okay, so we can think about this as when we're talking about
PCA and the other methods, we can think about it
as an iterative thing where we're generating a model. The model lets us do low dimensional
representation of the data, and then we do a reconstruction from this low dimensional representation and measure the error. So here it is for PCA. We get k eigenvectors, and then we take a particular point, so a single year station
measurement vector. And from that, we take
the k top coefficients. So these are the K top coefficients that we get from our model. And then we can plug
it back into our model and get an approximate
reconstruction, right? So that was what we had when we looked at the depth of the snow. We would have the representation
using just three numbers. And then from these three numbers, we reconstruct a signal that is supposed to be close to the original signal, okay? And then we measure the error by the squared error
between this and this, okay? All right, now a similar thing
we've done when we studied whether the spatial effect
or the temporal effect are more important in terms of snow depth or in terms of the second
coefficient for snow depth, okay? So what did we do in this case? We had the average per year
and average per station. So we had two averages that
could be used as a model. And then given station year, we could calculate the average, we could calculate a
prediction for how much, just this vector of averages
and this vector of averages if we use them together, how do they approximate the value of the actual second coefficient? And then that's the
estimation of coefficient 1. And we compute the
squared error again, okay? So in this case, the model itself is not a PCA model, it's a model of combining averages, but the general technology,
the method is the same. So to say things in more generality. So we fix some things. So that's what we fix in the model. And then we learn the
model parameters, right? So from the data, we learned
parameters for the model. Then when we have a single data point, we get a compressed representation
of that single point, and then we get an
approximate reconstruction. And from this reconstruction, we calculate the squared error, and the minimization of the
squared error is the goal of choosing the model. So sometimes it can be
done in closed form, like in principle component
analysis or regression, but sometimes it actually
has to be done using some kind of iterative
algorithm just like in K-means. Okay, so this brings us to distinction that I think is useful to know in general. So suppose that we have
some data x1 to xn. And we can distinguish
two types of things. One is extensive. So these are quantities
that scale linearly with N. So like for instance, the
coefficients that we have for each one of the
stations, it scales linearly with the number of station
years that we have, okay? So it reduces the number from
365 times N to 3 times N, but it still scales like N, so it's an extensive quantity. An intensive quantity is a quantity that doesn't scale as fast
with the amount of data. So it scales maybe not at all, or it scales logarithmically
typically in terms of how many more bits we need
to express this thing. In the situation that we're in, the model is an intensive property, so it's relatively, especially
when you have a lot of data, the model takes very little space. And then this single data point is one of the N data
points, so it's extensive. This is extensive, and
then this is extensive. But the model that combined
these things together or the parameters that define the model, those are intensive. So you can think about them as these are the essence of our distribution that doesn't change very much if we get more and more and more samples. It changes a little
bit, but not very much. So what is a good model? From x, from the original
data point, to r, we have dimensionality
reducing mapping, okay? X and r and o, the output,
these are all extensive, okay? A good model is one which
reduces the dimension with only a small increase
in the RMSE, okay? So we can always say that a model is simply remembering all of the data, but a model that is more useful
is a model which is small and compresses the data, and doesn't create a lot of noise, a lot of error in the RMSE sense, okay? So when N is large, then a
larger model is justified. So basically, when we have more and more data, we can actually get our
accuracy of our parameters to be better and better. And that will require a few more bits. Typically, something like log N bits. Why log N? You can think about taking
an average of N numbers. The average basically
involves taking the sum. So you are summing N numbers. So you get a number that
is about N times larger. And the number of bits that you need in order to store that number
exactly is log N, okay? So that is why the parameters, the storage for the parameters
typically scales like log N. Or it doesn't scale at all
if you basically decide, no, beyond this accuracy, I don't really want to
know the accuracy anymore. I don't want any more
accuracy in my estimates. Suppose we're given some
d-dimensional data here, 2-dimensional data, okay? These our data points. And we don't really know
much about this data, but it looks like it actually is organized along this curve, okay? Okay, so what are we going to do is we're going to fit to this model with a small number of parameters. Let's say this curve that we put here. This curve is going to be our model, okay? So it's a simplified
represent approximation of this point cloud, how this
point cloud organized, okay? And so each data is approximated by taking the closest
point on the model, okay? So if we take this point here, then it's going to be
mapped to this point here, which is on the curve and is the closest point
to the blue circle. One useful thing about
putting things on the curve is that now we don't need to represent it using two parameters, but there is enough to use one parameter. So we can think about this
parameter scaling from 0 to 1, which basically goes
smoothly along the curve. And so this point can
be represented as 0.52, a single number, okay? And that is what we call
dimensionality reduction. The distance that we
have between the point and the approximation is
the error, the RMSE error. We want the average
root mean squared error or percentage of the variance
explained to be high. So to summarize. Models in general are small
approximate representations of the data distribution. Each data point x is mapped into a smaller representation rt. So that's the representation. And the model maps the rt back
to a reconstruction, okay? So we go from x to a
compressed representation. Let's say the top eigenvectors. And then from that three top coefficients, we represent it back as a vector. And then we measure the
distance between them. And the average distance, that's the quantity
we're trying to minimize. While at the same time having the data be as compressed as possible. So the more compressed the data is, the worse the error. But that's the trade off
that we're looking for. Okay, so the model and model
parameters are intensive. Those kind of capture important quantities that don't scale with
the number of examples. And the x, r, t and o are extensive. Xt to rt is a dimensionality
reducing mapping. So to compare these relationship to other approaches and statistics, we have the RMSE approach,
which we talked about. We can have goals that are
different than the RMSE. We want the parameters to capture something about the true distribution. So we endow these parameters
that we're measuring by a meaningfulness beyond
being able to reconstruct. A generative approach
is a different approach, which basically says what we're trying to do is create a mechanism that generates data that
looks like our data. So examples of that are
mixtures of Gaussians and HMMs. Generated distribution
approximates the true distribution. And the performance is
measured using likelihood. The main goal is to capture the parameters of the distribution. So what we had here is another goal. This is here the main goal. So we want to know what is really the rate at which the temperature
of the water is increasing. So we are really interested in the estimation of that
underlying parameter. Discriminative approach is where the model predicts
output given the input. So examples of that are perceptron, decision trees, neural networks. Generated predictor approximate the input output relationship. And performance is measured
using average loss. It can be RMSE, it can
be number of mistakes, it can be log loss. There are various things. But what we're trying to do here is specific to mapping
an input to an output. Like regression is of that type. And there is no other goal. So the point here when we're
doing these kind of modeling is just to get a good predictor, not to try to understand what
are the underlying parameters.
--- end {10.1_transcript.txt} ---
--- start{10.2_lec.pdf} ---
10.2 Data in
High
Dimensions
DSC 232R, Class 10: RMS
1 Data in High Dimensions/ The Curse of
Dimensionality
   Random vectors in high dimension behave very differently from low dimensional
    vectors
   Our intuition fails us
   For a better understanding, rely on the law of large numbers and central limit
    theorem
2.1 The Length of a Random Vector


               

2.2 What is
the
Explanation?
2.3 What about the Distance Between
Two Random Vectors
Drawn from the same gaussian
3 Two Gaussians


3.1 The Length of a Random Vector

   If the data is two dimensional, it behaves like we would expect – most vectors have
    length close to 5
   As the dimension increases, the length of the vectors becomes dominated by the
    coordinates with mean 0
3.2 Compare with a Single Gaussian


3.3 Distance Between Two Points


3.4 K-nearest-neighbors Fails in High
Dimensions

4 Dot Products


4.1 Summary

   The 2d and 3D intuitions break down at high dimensions. The lengths of vectors,
    the distance between vectors and the dot products between vectors all become
    highly concentrated and therefor not informative
   The curse of dimensionality: Most statistical methods break down at high
    dimensions
   A way out: some high dimensional data has low intrinsic dimension
   For example: small number of PCA eigenvectors explain alarge fraction of the
    variance

--- end {10.2_lec.pdf} ---
--- start{10.2_transcript.txt} ---
(upbeat music) - [Narrator] Hi, we've talked about the power of using
principle component analysis to do dimensionality reduction. Take a data that is 365 dimensional down to three dimensional. And the question is, why are
we so excited about that? Why can't we just leave data in its original high dimension and just do our machine learning that way? Well, there are some issues
come up when you do that, data doesn't behave in high dimensions the way that your intuition is when you talk about low dimensions. So let's think about a few of those cases. So suppose we have a single gaussian, so something like this, and we are doing it now in end dimensions, I can draw end dimensions,
so I'll just draw two, so this is the second dimension and it's also gaussian in this way, okay? So it's gaussian in both
of these directions. So it's a spherical gaussian. And now we want to ask, what's the distribution of
the distance from the origin? Okay, so here, when we
talk about one dimension it's pretty clear that the distribution of the length would be like
a gaussian, so let's see. So what we see is, that the
length when the dimension is two is indeed very close
to the gaussian, okay? Or to the distribution that is derived from the gaussian when you
take absolute value, okay? So that's what you get in dimension two. But when you go to dimension 10 it starts to behave differently. Right here, the mean is
about 1.2 or something, here the mean is three, here, when we're in dimension
200, the mean is about 14, and when we're in dimension
2000, the mean is about 45. But not only is the mean about 45, but it's very, very, the distribution, the variance
around it is very small, it's kind of something like standard deviation of two or three. Okay so we never see any values here, and we never see any values here. We see values only in a particular place. So even though our intuition is that if you have a gaussian
that looks like something like maybe contours or
like something like this. In high dimension, everything
that is here doesn't matter. The only thing that matters is data that is somewhere around some
kind of variation here so, let me use another color, so there is some distance
where almost all of the vectors are that distance from the center, and that's just because
the dimension is high, it's not because we change
this spherical structure or the gaussian or anything like that, so that's already kind
of surprising, right? So we have, that if you take
a vector generated by gaussian or by any distribution,
any reasonable distribution that is spherical, you get
that there is one distance that is predominant and then distances larger
or smaller than that, are just, never occur, okay? Okay, what's the explanation of this? Why is that so? Well the length of a vector X is the sum of the xi squared, square root. Now because the distribution is gaussian these terms xi are also
independent from each other, okay? That's just the property
of gaussian, okay? Now as the mean of the gaussian is zero, the expected value of
xi is about one, is one no dependence on d, that's the variance of xi, and the variance of xi
squared, is also some constant, also not dependent on d. And therefore when we're adding these independent random variable, we're adding d of them, then we get the length that we get is
square root of cd, okay? So that's the new length. Now the distribution of xi
is concentrated around d, and the distribution of sum of xi squared is concentrated around square root of d. Why is that? Because the length is d and the standard deviation
is square root of cd. So as d increases, cd relative to d becomes smaller and smaller, okay? So if we are right C square
root of d over the mean d, so the variance over d, and that is like C over square root of d. So as the variance increases, as the dimension increases, the relative variance
of the length decreases and that's what creates
this concentration. Okay, so that's two vectors that's a single vector, what's its length? Now what about the distance
between two vectors that are drawn from the same gaussian? So again, what we have is
that in small dimension it behaves like what we would expect, but as we increase the dimension, the distribution becomes
highly concentrated around a particular value. So take any two random vectors from this gaussian distribution, they almost always have the
same distance from each other. So now when we think about things like using distance, like
nearest neighbor and so on, it's clear that we have some trouble. So maybe these strange things happen because we're taking the
data from a single gaussian. Maybe if we take it from two gaussians that are distant from each other, then it'll not happen, okay? So here is what we're going to do. We're going to basically generate, generate two gaussians,
like one here and one here this one at minus five,
and this one at five, and then the other dimensions are always going to be zero, okay? So these are the two
gaussians we're generating and we're going to ask the same question, what is the length of
a typical vector, okay? So what would we expect? We expect most of the vectors
to have length close to five, and as a dimension increases, the length of the
vectors becomes dominated by the coordinates with mean zero. So what do I mean by that? Here we have the dimension two, so we see that that the
mean is indeed five, at dimension 20, the
mean is more like seven, and then now a dimension
200 the mean is 15 and at 2000, the mean is 45, okay? So we see that the distance
between the vectors has very little to do with the distance between the centers of the gaussians. It has more to do with the dimension because the dimension increases the distance between vectors irrespective of where the mean of
where they were generated. Okay, so to compare
with a single gaussian, in dimension 2000, the
distribution are almost the same, concentrated around
square root of 2000 is 45. We cannot use distance
distribution to distinguish between k equal one
and k equal two, right? We cannot say that there is more centers, less centers based on the distribution. Okay, now let's think
about the same distribution but distance between two points. So we have the same two
gaussians at minus five and five and now we're going to take
a single point from here and another point from somewhere, from maybe from here, maybe from here. Clearly if the points
are in different places we expect the distance
between them to be 10, and if they're from the same place we expect it to be close to zero, so let's see what actually happens. So indeed in dimension two, we have distance close to
zero if they're the same and distance close to 10 if
they are from different centers. And similarly here, so
you have these two peaks even though the distances are
now not the correct distances, at least we can distinguish
between the case that the two points are
from the same gaussian or they're from different gaussians. But once we go to 200
and definitely in 2000, all of this goes away, the distance between
the vectors is dominated by all of these other coordinates. And so we cannot really tell if they're from the same center
or from different centers. So K-nearest-neighbors is likely
to fail in high dimensions. Suppose that we have gaussians that are labeled plus one or minus one, and we use K-nearest-neighbor to identify the cluster
which the point came from, in low dimension that works
but not in high dimension. So given any two vector,
the distance between them is about square root of
d, regardless of whether they come from the same
center or from different ones. Okay, so the distance loses
its information power, it doesn't tell us
really where things are. Let's think about one more thing that we could measure is dot products. So when we take dot
products between two vectors that are coming from these two clusters, the result, what we expect
is it should be either minus 25 if they come
from the opposite centers or plus 25 if they come
from the same center. Okay, so plus minus 25 seems like a lot but when we reach high
dimension it all fails, right? So what we have is that, with the dot product in
two dimensions indeed we have it centered with minus 25 or 25. And even at 20 it's a
pretty good separator. And at 200 it's maintains
some power of separation, we see that there are two
peaks in the distribution, but once we reached 2000, then there's absolutely no
information in the dot product, it does not tell us what
is going on with our data. Okay so to summarize,
the 2d and 3d intuitions, break down at high dimensions. The length of the vectors and the distance of the
vectors and the dot product between vectors all
become highly concentrated and independent of the
structure of the data. So they depend really
only on the dimension. The curse of dimensionality, is that most statistical methods break down at high dimensions. A way out, is to find
some high dimensional data has low intrinsic dimension. For example, if you have small
number of PCA eigenvectors to explain a large
fraction of the variants, that's a case in which you have found low intrinsic dimension and now a lot of these
methods work much better. So, that's what I wanted
to talk about today and I'll see you next time.
--- end {10.2_transcript.txt} ---
--- start{9.1_lec.pdf} ---
9.1 Functions
As Vectors


DSC 232R, Class 9: PCA for Weather Data
Temperature Per Day as
a Function


The records of temperatures for
each day can be represented as a
function from 1, 2, 3, … , 365 to
the temperature on that day
(0=Tmin, 1=TMAX, 2=TOBS)


We want to find a way to
approximate these functions
using a set of basis function
How can We Visualize Vectors that are in
Dimension Higher than 3?

All of the Vector Operations are well
defined, including approximating a
function using an othernormal set of
functions
Function Approximation

For simplicity, consider the vectors that are defined by sinusoidal functions.
Define an Orthonormal Set
The dimension of the space is 365 (arbitrary choice: the number of days in a year).
We define some functions based on sin() and cos()


                                                       Const

                                                       Sin
Check that it is an Orthonormal
Basis
   This basis is not complete it does not span the space of all functions. It spans a 9
    dimensional sub-space


   We will now check that this is an orthonormal basis. In other words, the length of
    each vector is 1 and every pair of vectors are orthogonal.
Rewriting the Set of Vectors as a
Matrix
   Combining the vectors as rows in a matrix allows us use very succinct (and very fast)
    matrix multiplications instead of for loops with vector products
Approximating an Arbitrary
Function

Approximations of Increasing
Accuracy

Reconstruction from 1D Projection
Approximations of Increasing
Accuracy


Reconstruction from 1D Projection


                           Residual




                           Residual
Plotting the Approximations


Cont.
Recovering from Noise
Summary



--- end {9.1_lec.pdf} ---
--- start{9.1_notebook.md} ---
Here is the converted content for **9.1_notebook.ipynb**:

```markdown
```python
%pylab inline

import numpy as np
import matplotlib.pyplot as plt
from ipywidgets import interact, interactive, fixed, interact_manual,widgets
import ipywidgets as widgets

print('version of ipwidgets=',widgets.__version__)

from lib.Reconstruction_plots import *
from lib.decomposer import Eigen_decomp
from lib.YearPlotter import YearPlotter

import warnings
warnings.filterwarnings('ignore')
_figsize=[8,6]

```

```python
"""
RETURNS ->

%pylab is deprecated, use %matplotlib inline and import the required libraries.
Populating the interactive namespace from numpy and matplotlib
version of ipwidgets= 8.1.8

/home/scotty/dev/dsc232/lec/week_05/.venv/lib/python3.11/site-packages/IPython/core/magics/pylab.py:166: UserWarning: pylab import has clobbered these variables: ['interactive']
`%matplotlib` prevents importing * from pylab and numpy
  warn("pylab import has clobbered these variables: %s"  % clobbered +
"""

```

## Functions as vectors

### temperature per day as a function

The records of temperatures for each day can be represented as a function from  to the temperature on that day (0=TMin, 1=TMAX, 2=TOBS).

We want to find a way to approximate these functions using a set of basis function.

<img src="Figures/TemperaturesExamples.png" width="600">

* How can we visualize vectors that are in dimension higher than 3?
* One good way to visualize a -dimensional vector is to draw it as a function from  to the reals.

```python
d=4
plt.stem([1,-3,2,0])
grid()

```

```python
"""
RETURNS ->

<Figure size 640x480 with 1 Axes>
"""

```

* All of the vector operations are well defined, including approximating a function using an orthonormal set of functions.

To get an intuition about the working of the PCA, we used an example in the plane, or .
While useful for intuition, this is not the typical case in which we use PCA. Typically we are interested in vectors in a space whose dimension is in the hundreds or more.

How can we depict such vectors? If the coordinates of the vector have a natural order. For example, if the coordinates correspond to a grid of times, then a good representation is to make a plot in which the -axis is the time and the -axis is the value that corresponds to this time.

Later in this class we will consider vectors that correspond to the temperature at a particular location each day of the year. These vectors will be of length 365 (we omit the extra days of leap years) and the PCA analysis will reveal the low dimensional subspace.

### Function approximation

For simplicity Consider the vectors that are defined by sinusoidal functions.

```python
# We define a grid that extends from o to 2*pi
step=2*pi/365
x=arange(0,2*pi,step)
len(x)

```

```python
"""
RETURNS ->

365
"""

```

#### Define an orthonormal set

The dimension of the space is 365 (arbitrary choice: the number of days in a year).

We define some functions based on  and 

```python
c=sqrt(step/(pi))
v=[]
v.append(np.array(cos(0*x))*c/sqrt(2))
v.append(np.array(sin(x))*c)
v.append(np.array(cos(x))*c)
v.append(np.array(sin(2*x))*c)
v.append(np.array(cos(2*x))*c)
v.append(np.array(sin(3*x))*c)
v.append(np.array(cos(3*x))*c)
v.append(np.array(sin(4*x))*c)
v.append(np.array(cos(4*x))*c)

print("v contains %d vectors"%(len(v)))

```

```python
"""
RETURNS ->

v contains 9 vectors
"""

```

```python
U=vstack(v).transpose()
shape(U)

```

```python
"""
RETURNS ->

(365, 9)
"""

```

### Approximating an arbitrary function

We now take an unrelated function 
and see how we can use the basis matrix `U` to approximate it.

```python
f=abs(x-4)

```

```python
eigen_decomp=Eigen_decomp(x,f,np.zeros(len(x)),U)
plotter=recon_plot(eigen_decomp,year_axis=False,interactive=True,figsize=_figsize);
display(plotter.get_Interactive())

```

```python
"""
RETURNS ->

VBox(children=(HBox(children=(FloatSlider(value=0.0, description='c0', max=32.29570559348949, orientation='ver…
"""

```

#### Food for thought

Visually, it is clear that  is getting close to  as  increases. To quantify the improvement, compute
$ | g(i)- f |_2 $ as a function of 

### Recovering from Noise

```python
noise=np.random.normal(size=x.shape)
f1=2*v[1]-4*v[5]
f2=f1+0.3*noise

```

```python
eigen_decomp=Eigen_decomp(x,f2,np.zeros(len(x)),U)
plotter=recon_plot(eigen_decomp,year_axis=False,interactive=True,figsize=_figsize);
display(plotter.get_Interactive())

```

```python
"""
RETURNS ->

VBox(children=(HBox(children=(FloatSlider(value=0.0, description='c0', max=2.031575791925349, orientation='ver…
"""

```

```

```

--- end {9.1_notebook.md} ---
--- start{9.1_transcript.txt} ---
(mellow instrumental music) - [Instructor] Hi. So, we talked about PCA, and I showed you a little bit
about the weather patterns. And now we're going to start putting these two things together, okay? So, how do you do weather patterns PCA? Use functions as vectors. Let me see, let me
explain what that means. Suppose that we have this pattern of temperature as a function
of the day of the year. So, simply put, January
is the coldest month in Massachusetts and July and
August are the hottest months. Okay, little surprise there, but there is much more pattern going on into the measurements of
Tmax and Tmin and T observed. And we want to somehow capture
what this pattern means. So, one way we can think about it is that we have a function
from the day of the year, okay, to the particular temperature,
let's say Tmax, okay? So, to the orange line. So, now we have a function, and now we want to see how we
can represent this function. So, how can we visualize vectors that are higher dimension than three? Well, we can just think
about it as a mapping from the numbers one, two, up to d, in our case up to 365, to a value. So, here is suppose that
we have just four values, and so one, minus three, two, and zero. Then we can represent it as this function that has the value one on zero, the value minus three on one, the value two on two, and the
value zero on three, okay? So, we basically forced
this sequence of numbers to be represented as a function. So, this kind of mapping
gives us something that is well defined and,
in terms of vector space. So, it allows us to take
sum of two functions, take function times some constant, takes even the dot
product of two functions. So, in particular, it lets
us do function approximation. So, we want to approximate
a given function, of measurements let's say, using some kind of basis set of functions, an orthonormal basis of functions. Here we're going to use sinusoids
as our orthonormal basis. And these are the functions that we have. We have nine functions. The first one is simply a constant. And this is sine is the next one, and then we have cosine, sine of two x, cosine of two x, and so on. They look something like this, okay? So, this is here is the constant, and then sine looks like
this, the orange line, and cosine looks like
this, the green line, okay? So, all of these functions, it turns out, form an orthonormal basis. So, you can take each function
and measure its length or its dot product with itself,
and that will give you one. And you can take any two functions, take the dot product and
that will give you zero. So, we're going to check that here. We basically take for every pair, i and j, we take the dot product
and we just print out the dot product just to one decimal point. And what we see is indeed that
if you look at the diagonal, so the elements that are the
vector dot product with itself, you get one, and if you
look at, off the diagonal, you always get zero, okay? So, it is indeed an orthonormal basis. It's not a complete basis because our vectors have length 365. And so, for a complete
basis we need 365 vectors, but we're just going to use these. Okay, so we are going
to use matrix notation because that's easy to do
when you're working in NumPy, and that's simply just stacking the nine vectors that we have. And we get a matrix that is
365 rows by nine columns. Each column is one of the
vectors, one of the sinusoids. And now we want to approximate
an arbitrary function, okay? So, we're taking function
that is not a sinusoid at all, this x minus four absolute value, and we're going to try to approximate it. Okay, so here is our
function, x minus four, and you see that it is very
far from any sinusoids. Nevertheless, you'll see
that we can approximate it with sinusoids better and better as we increase the number of frequencies. Okay, so what are we talking about? We're talking about this
reconstruction matrix, reconstruction formula, sorry, that says that if you want to
approximate the function g using i basis vectors what you do you take the dot product of
the function with itself, not with itself, with the basis function. And then that gives you the coefficient. And the coefficient you
multiply again by the vector, by the unit vector, okay? And this sum is an approximation of the original function f, okay? So, we're going to do that
first just in two dimensions. Oh, one thing I didn't mention is that what we're interested
is that the difference, or the norm, between the
approximation and the function will be as small as possible, okay? So, we're going to look first at a one-dimensional reconstruction. And in this reconstruction we
have points in two dimension, and we have two candidate basis vectors. One is this red line, and the other is this green line. And intuitively the green line is a much worse approximation
for our function. So, how do we see that? So, we're given this function, fx is x minus four absolute value, and we want to approximate
it closer and closer using our sinusoidal basis functions. So, what does that look like? Well, basically the use
of an orthonormal basis is always using this basic formula, which says you take any one of the ui's and you take the dot product
of it with the function. And then you get the coefficient. And then you multiply
this coefficient again by the same basis function, and you sum over the basis functions. Not over all of them, but
over j from zero to i, okay? So, the larger that i is the
better this approximation, gix, is to fx, okay? So, we get closer and closer by projecting and adding the terms of the
orthonormal reconstruction. And the distance that we are
interested in between f and gi is simply the norm of gi minus f, okay? So, specifically just to start, let's think about just reconstructing a two-dimensional vector
with one unit vector, with one basis vector, okay? So, we have this function g one, that is our point in two dimensions, which is a simple thing here and our point in two dimensions. And then we multiply that by
u one to get the coefficient, and then we multiply the
results by u one again to get our approximation g one, okay? And the residual, the error that remains, is the point minus g one, okay? So, now that would be the
part that we would try to minimize using let's
say u two and so on. In this case the dimension is two, so if we have u one and u two we have perfect reconstruction. So, let's see how that actually looks. So, suppose that we're looking
at this point here, okay? And we want to reconstruct it, and we want to reconstruct it considering one of two unit vectors. This is maybe u one, and then another one
here that is u two, okay? Let's start with u two because
it is significantly worse. And so, that kind of explains why the choice of u is important. Here we project the point that we have onto the direction u two
to get the dot product, which is this length, okay? And when we multiply it by u two again we get this point here. So, the residual now is this,
this is the residual, okay? And the residual is pretty large. On the other hand, had we
used this vector, u one, then the reconstruction
would've been this point, and then the residual would
be, have this length, okay? So, you see that if we
choose this vector, u one, then for most of the points that we see the distance between the
point and the projection to the direction u one is
small, and that's why u one would be a preferred basis vector, okay? Let's see how that works when
we are doing the sinusoids. So, what we're going to
show is these approximations where we have g one, g
two, g three, and so on, where the approximation
becomes better and better the residual becomes smaller and smaller. And this is how we see it right now, so let me get rid of that. Okay, so what we have here is worthwhile a little bit of explaining. So, we have nine, one, two, three, four, five, six, nine coefficients for the nine vectors, and the coefficients are
initially all set to zero, okay? So, basically what we're starting with is the function zero, okay? You see down here, the green line, is the function zero, okay? And now as we start to,
as we move these scales we are changing the coefficient for the basis function, okay? So, here we have changed the coefficient of the constant basis function, and we see that now the
green line is much closer to the middle of the
range of the function. And if we add more and more coefficient what we see is that the
result is closer and closer to our function that we're approximating. Now, even if we have
all of the coefficients we see that the approximation
is far from perfect, right? So, if we look here the
approximation is pretty good, but if we look here the
approximation remains not so great. So, it's not a perfect approximation, but it is somewhat surprising how well we're approximating the original function using just these functions
that look like sinusoids, okay? So, this is an example, and this kind of interactive widget we will use it later to do
other kinds of reconstructions and understanding of what
is going on with our data. So, one thing that doing
this kind of reconstruction using sinusoids that is pretty powerful is that it is a good way
to get rid of noise, okay? If you have data that is noisy, and here what we have is
basically normal noise added to this function that is two v one minus four v five, okay? So, let's see what that looks like. It looks like something like this, okay? So, the blue is the original
function without the noise, and added the noise we
get the orange line. So, the question is can
we recover the blue line when we're given only the orange line? So, we can think about
various kinds of averaging, but a better way is to take into account that this function is just a combination of v one and v five. We don't know that these
are the coefficients, but that we can infer from the data. So, here we have the data, let me zero it out again. So, we have that, we have this, okay, so right now we have the
approximation just being zero, and the function that
we see is very noisy. And we can see, we know
how it was constructed so we can guess, but it's very hard to see how to actually extract this underlying, non-noisy structure. So, the way that we're going to do it is we're going to look at the coefficients that we added before,
okay, so it's not this one. It's this one and this one. You see that these two are reconstructing the original data very accurately even though there is a
large amount of noise. And if we look at the other coefficients their effect is actually pretty small. So, we can not only remove the noise, but we can find the coefficients
of the underlying shape. To summarize, functions can be used, be thought of as vectors and vice versa. The Fourier basis is a basis
of orthonormal functions that are made out of sines and cosines. And orthonormal functions
can be used to remove noise added to underlying distribution, and this kind of noise
removal is something that we will use extensively when we go to the weather application. So, I'll see you soon, bye.
--- end {9.1_transcript.txt} ---
--- start{9.2_lec.pdf} ---
dimensionality reduction
decompose_dataframe extracts the series from the row, computers the 𝑘 to decomposition
coefficients and the square norm of the residuals and constructs a new row that is reassembled
into a new dataframe.


For more details, use %load lib/decomposer.py
Let’s do some decompositions!
Join decomposition information with station information
Removing years with little snow

In some locations and in some years, there is almost no snow accumulation. We want to treat
these separately.


To do so we compare the error of using the average to the error of using a zero vector. We keep
only those yearXstation where the mean is a better approximation than the zero Vector.
Plot mean and top eigenvectors

Construct approximation of a time series using the mean and the 𝑘 top-eigen-vectors. First, we
plot the mean and the top 𝑘 eigenvectors.
plot Percentage of variance explained
Exploring the decomposition
Intuitive analysis

• Eig1 is very similar to the Mean --- Indicates heavy/light snow
• If coef_1 is large: snow accumulation is higher.


• Eig2 is a negative January, positive march. Indicates early vs. late season
• If coef_2 is high: snow season is late.


• Eig3 is positive Feb, negative Jan, March -- Indicates a short or long season.
• If Coef_3 is high: Season is short.
Studying the effect of Coefficient 1
Studying the effect of Coefficient 2
Studying the effect of Coefficient 3
Interactive reconstruction
Following is an interactive widget which lets you change the coefficients of the eigenvectors to
see the effect on the approximation. The initial state of the sliders (in the middle) corresponds to
the optimal setting. You can zero a positive coefficient by moving the slider all the way down,
zero a negative coefficient by moving it all the way up.
Studying the distribution of the coefficients
Coeff_1-3 capture most of the variance in the snow-depth distribution.
We can now look at how these coefficients vary from year to year.

--- end {9.2_lec.pdf} ---
--- start{9.2_transcript.txt} ---
(upbeat music) - Hi, we've seen the general theory in power of principle component analysis and now we're going to test the ability of using principle component analysis to do dimensionality reduction, or to represent our data using
a small number of numbers. Okay, so this is dimensionality reduction of PCA the snow depth of Massachusetts. Why are we using snow
depth in Massachusetts? Because that's relatively speaking, a very clean signal that is
relatively easy to analyze and to reason about. Okay, so to remind you, the basic operation
that we're doing here is finding approximations of a vector. And the way that we measure the error from this approximation, of this approximation is the residual. So the original vector is V and the first residual is simply the difference between V and the average of all of our vectors. Okay, so that's the first zeroth residual. The first residual is then
when we take the first residual and we subtract out the projection on the first eigenvector, okay. That gives us the first residual. And we can continue like that. Every time we take the residual, whatever is remaining, and we project it on the next eigenvector
and we get another number v dot U2 in this case,
that is the representation. And the residual is when we subtract from the previous
residual that projection. Okay, and so we go on and generate an approximation like that and we hope that after a few steps we can get a very small residual. So what do we mean by small residual? We simply mean the
square of the norm, okay, or the sum of the squares of the components of the residual. Okay, so the smaller this norm the better the approximation,
the reconstruction. Okay, so a few things to
remember about linear algebra. The zeroth for residual norm is square distance of V from the mean mu. The kth residual norm is
the minimal square distance between V and the point on the hyperspace, hyperplane that is described
by the first k eigenvector. So we say, okay, we can take
any point on this first, on the linear combination of
these first k eigenvectors and we're looking for the
closest point to our point. That's the difference, the distance of that is the residual. So residual norms are non-increasing. The more eigenvector you
add, the smaller they get. And the residual vector rn is always zero because if you have as many eigenvectors as the dimension of the space, you can always represent
the vector precisely. Okay, so we have a code
in decomposed data frame that you can look, but we're
not going to go into it here. And basically that norm
takes a set of vectors from a data frame, and it
takes the statistical analysis of eigenvectors that we have and it computes the residuals
and the reconstruction errors. So let's see what we get. So as I said, snow depth in Massachusetts. This is how we load the
data into our data frame. And the most of the work of
the loading is done here, df2.count simply because
again, it is lazy execution. So we're not really
loading anything when we, before we need to generate the number, here the number of records. Okay, so now we're going
to join this decomposition information with station information. So we want to see for each station what is the longitude, latitude and so on. And so we generate this
combined data frame. And another cleanup thing that we do is that we remove years
with very little snow. So as it turns out, there are parts of Massachusetts where it rarely snows at all. So those basically have
a snow depth of zero throughout the year. And if you include them, they
tend to distort your analysis because zero can best be
approximated just by a single zero. So it's better to think
about it as separate cluster of almost no snow, and then
deal with the rest separately. So we remove years that have little snow. Now let's see what we have in terms of the mean and the top eigenvectors. So top graph here that we see, this one, is it basically shows
us that the deepest snow is more or less at the end of the winter, which is around January, February, March, March you have the end of the winter. And then it goes down to
zero during the summer, there is no snow. No snow from April to, from end of April to
beginning of September. And then in beginning of September there starts to be more snow, okay. So that's what we have and that's the mean. The eigenvectors are more
interesting than that for the most part. So the blue one is eigenvector one, eigenvector one here. That's the blue line here that is very similar
in shape to the total, to the average vector. So what happens when we
add this average vector to this vector? We basically make it bigger or smaller. So what we see is that the biggest, the eigenvector with
the biggest eigenvalue, the one that explains
the most variance is the, is basically the amount of snow overall. If you have more snow, then you'll have positive
coefficient with that. If you have less snow, you'll have negative
coefficient with that, okay. Then the second and the third they look a little bit
different and more interesting, and we will talk about them in
more detail in a little bit. Okay, so the next thing is how much is the explanatory
power of these eigenvectors? So the percentage of variance explained is basically what is the
length of the residual relative to the original variance, relative to r1, which is
the distance from the mean. And what we see is that this graph called the percentage
of variance explained is increasing very rapidly. So it increases from zero, when we have no eigenvectors to about 55% when we have one eigenvector to after six eigenvectors,
we have about 80%. So that means that on the average, if you take the six top eigenvectors and use that as your your dimensionality, as your representation of the vector, that explains 80% of the variance. So the residual is 20%
in terms of its norm. Okay, so let's look at the decomposition in a little bit more detail. So first, let's think about intuitively, what do these vectors
that we saw look like? So the eigenvector one we talked about is very similar to the mean. It indicates heavy or light snow. And if coefficient one is large, snow accumulation is higher. Eigen two is negative in
January and positive in March, it then indicates an early
versus late season, okay. So it basically pushes the
beginning of the snow earlier or later in that year and station. So if coefficient two is
high, snow season is late. And finally eigen three
is positive in February, negative in January and March and that indicates short or long season. So if coefficient three is
high, the season is short. Now why would you believe
me about these vectors? This requires some digging
into the to the files into into the actual reconstructions, and that's what we're going to do. But this is the first kind
of intuitive explanation. And those explanations are important, because that's what connects your analysis to what people can relate to. They people can relate to a
lot of snow or a little snow, a snow season that is early or late, or a snow season that is short or long. That means something in the real world. Okay, so let's look at
first at coefficient one and try to convince
ourselves that that's really the explanation here. So what I'm doing here is I'm taking the data frame generated by the reconstruction software and I'm saying I want the residual to be, the residual after one
to be smaller than 0.1. And I want the coefficient, to sort according to coefficient one. So I'm basically sorting everything from coefficient one being very small to coefficient one being very big. And I ask for cases
where this coefficient, the residual after this
one coefficient is small. So this is the data frame that I get and here is the plot that I get. Okay, so what I get here is a sample of the values of the reconstructions when you have very negative numbers, - 1003, -999, and so on. What you see is that the
target is very close to zero. It's not quite zero, because then we would've filtered it out, but it is very close to zero and much, much smaller than the mean in terms of snow depth, okay. So by subtracting a large
amount of the first coefficient, we basically reduce the
total amount of snow to something that is very close to zero. Okay, so that is the case when the coefficient is very negative. Here is the case when the
coefficient is very positive. What you see now is that the
the signal that we're trying, the purple signal here, is much larger than the average. You see here the average is down here and the signal that we're reconstructing is much, much higher than it. So this is a place where snow depth is much bigger than what is the mean. And indeed, you see that
the first coefficient captures that very well, okay. So this is the first
coefficient being very large compared to the other coefficient. So in absolute value,
this is a very large value and this is also pretty large. And you see that as you
go larger and larger the signal itself is much
bigger than the mean. So this confirms what we were saying that basically this coefficient tells us if there's a lot of snow or
if there's a little snow. What about coefficient number two? We do a similar thing,
but now we are looking at coefficient number two and the residual for
coefficient number two. And now the picture is more interesting. Here is the first case, the coefficient number
two is very negative, the most negative. And what you see is that
the snow starts early, starts like before the 1st of January. Okay, it starts here, and in all of those, if you compare it to the mean, you see that it tends to be
earlier than the mean, okay. So all of these are earlier than the mean. And if we go to the opposite and we say, let's say
the coefficient is large, then we see that the
coefficient is always larger, that the start time of the
snow is later than January. Okay, so that basically tells
us that this coefficient moves the starting time
from very early to late. Okay, finally, let's look
at coefficient number three. Again, we just sort. We asked that the residual
after three is 0.1, smaller than 0.1 and we sort according
to coefficient three. And now what do we get? We get that if this
coefficient is very negative, then the season tends to be long, right? So it tends to basically
have a more flat shape, like this one that basically it is more, it started in January and stays more or less the same and drops, and in these other ones also. And here is the opposite. When the coefficient is very positive, you see that the season
is relatively short. It starts at the end of January and it ends at the end of April, okay. So season is now a short season. So we have basically three parameters: one is the size, the second
is the starting point, and the third is the width or the length of the snow season. So this is a way to explore what our reconstructions mean and see if they agree with our intuition. But here we're basically just looking at this kind of sample of places. Sometimes it is useful to look at a particular place and
see what the coefficient and what the eigenvectors
mean for that place. So we've looked at the
reconstruction of snow depth in Massachusetts and we
looked at it in a bulk way. We looked at individual years and stations that had
large coefficient one or large negative
coefficient one and so on. And we saw that the interpretation that we initially gave makes sense. But now we're going to look
in a little bit different way. Suppose that were just given a station and the measurements that
were of the snow depth and now we want to see what of the properties of the snow depth are captured by the three
top eigen coefficients. Okay, so we're going to take just random station and years and see what we can understand from them using this interactive widget that you have access to in your notebooks. So here is our first example. The example here is
all of the measurements are pretty much zero. So this is a very simple
example, but let's do it anyway and see what that comes out
to be in the coefficient. So these three sliders
represent the coefficients. Initially, the values that
is associated are all zero. So basically that means
that we're not using any of the eigenvectors,
we're just using the mean. And so what you get here is that all of the graphs
are on top of the mean. As you slide the slider, you
move it from zero coefficient to the actual best coefficient according to the orthogonal
decomposition that we use, the closest point using
the Euclidean distance. And here it is, we're
adding or subtracting more and more of this first coefficient until we get to something that has much less snow than average, which is correct for this data. Now, if we add the other coefficients they make non-significant
changes to the, to what we have. So really the main thing
that we get is that if the value of the first
coefficient is negative, let's say around -800, that means that we have
practically no snow. Okay, second one is the
more interesting one. Here it is. Here is the measurement graph
that we see for this case. And we see that there's
much, much more snow than average, than the average, it is the red line here, and that the snow starts, starts pretty late, like the end of January
and ends early April. Okay, so let's see what the
coefficients look like here. If I increase this one,
I get to get, okay, so there's much more than average, but now if I add this one, you
see the shape here changes. So now that the red shape starts later than average and also
ends a little bit later than what we had before. Okay, and finally, when
we add the third one we get a pretty good approximation of how the vector, how the behavior is. And we see that in this case we explained more or less how it started
late and ended early. And that the height is pretty much a good approximation of the data. So we have a good approximation and we know that this is a lot of snow. This one means snow starts late and then this one means
the season is short. Let's see, another one. This case looks pretty simple but similar. But here we see that the
season doesn't start late. And so let's see. Here, we add and we get to the right total volume more or less. And when we add the second
one, it actually is added with a negative coefficient -404, -673. And that basically moves our start to be a little earlier rather than later. And then when we add the last one that's pretty large and that
makes the season shorter. So we get a short season,
but doesn't start early. It ends really early and that's pretty much the approximation. So what you see is that
these three numbers, these three coefficients that's what's called
dimensionality reduction. Knowing these three numbers
gives us the most information about the shape of the snow depth for that station and that year,
and that's the point of it. So I recommend you play around with that, use different years and see
that you can get some intuition about what these coefficients mean. Next, we're going to
study the distribution of these coefficients. That'll be in our next video.
--- end {9.2_transcript.txt} ---
--- start{9.3_lec.pdf} ---
9.3
Visualizing
Coefficients
On a Map
DSC 232R, Class 9: PCA for Weather Data
Visualizing Coefficients on a Map
Visualizing Coefficients on a Map
Compute Spectral Decomposition
Compute the count and average of
coeff_1 for each station
Cont.
Map

   Each circle is centered at a station
   The area of the circle corresponds to the number of years SNWD was recorded at
    the station
   The color fill of the circle corresponds to the value of avg(coeff_1) defined by
    color-bar
Is coef_1 related to elevation?
Is coef_1 related to elevation?
Is coef_1 related to latitude?
Is coef_1 related to latitude?
Is latitude related to elevation?
Summary

   We saw how to use ipyLeaflet to present data on tops of maps
   We saw that in NY state, most of the snow accumulation is in the Adirondacks
   Snow accumulation increase with elevation, but the relationship is weak: locations
    with elevation 400-600 meters have widley varying accumulations of snow

--- end {9.3_lec.pdf} ---
--- start{9.3_transcript.txt} ---
(upbeat music) (transition swooshing) - [Instructor] Hi, we have
looked at the coefficients of the principle components, the three top coefficients
as a function of time, how they change from time
to time, from year to year. Now, we want to look at how
they change from place to place. So we need some kind of mapping
device in order to do that. And there is a very nice mapping tool that you can use in Jupyter Notebooks, it's called ipyLeaflet. And we'll see how it works. So we are going to look at New York and the snow depth in New York. And first, we're going
to do the same steps as we did before. We are going to compute
spectral decomposition for the stations in New York. And that's what we are doing here. There are many cells
here that I'm skipping. If you want to see all of the details, I highly recommend that you
study the notebook by yourself. So now we're going to compute the count and the average of the
coefficient for each station. Okay, so we have these stations. And what we want to know is what's the average
coefficient across time? Because we are not looking at time, we're looking at average for all time. And how many measurements there were because we want to know if
there were a lot of measurements for this place or fewer, okay? And now, we're going to put that on a map. So each circle is going to
be centered on a station. The area of the circle
corresponds to the number of years that that snow depth was measured, and the color corresponds
to the coefficient. So it corresponds to the
coefficients being small or high. So here is our map. And the one nice thing about it is that it's an interactive map. So we can basically zoom
in as much as we want, and we can pan and look
at different parts. And the map underneath it becomes
as detailed as we want it, very much like Google Maps. And so what do we see? If we look a little bit out, we see that in most of New York, especially the south part, there is very little
snow, relatively speaking. But if you go north, there is these mountains
called the Adirondacks, and those mountains get a
significant amount of snow. Some location get really
quite a lot of snow and some just intermediate. So the red one is for very
high values of the coefficient, and then the blue are for low ones, and yellow are intermediate. So we immediately get a sense of how the snow is distributed. There is this mountain area here that doesn't actually
have that many stations, but the stations that you have here record the fact that there is more snow. And indeed this is skiing area. Okay, so that's the ipyLeaflet. And it's a convenient tool
for doing many things. You might want to study it
for your future reference. So now, we can actually ask questions about how do the coefficients relate to properties of the location, okay? So one is to say is coef_1,
which is the amount of snow, does it relate to the height? And we see that when we do a scatterplot, here we have the average of coef_1, and here we have a elevation. There is some relationship. So the places with more snow
tend to be higher elevation, but it's not very strong. And indeed, the elevations
that you have in New York are not very big. And they go up to like 600,
I'm not sure what they are. Feet or meters, must be meters. So in most of the places, all of the elevations get
very little snow, okay? So this is what you see from this scatter. Another way, another thing to ask is, how is coef_1 related to latitude? And you see again, most of the places have zero gain, but there is a tendency with latitude to get more and more
height, more and more snow. There is a lot of snow. And here, there's like negative. This is basically when
the snow is almost zero. Now, that is interesting, but now the question is, which of these are the real influencers, the latitude or the elevation? How are latitude and elevation related? And you see that latitude and elevation are strongly related. So you have that as
the latitude decreases, the elevation increases, okay? This is all of this major part. And so that's actually a
relationship that is stronger than the relationship between the latitude and the amount of snow. So this is the kind of
thing that we can do. We can now study the relationship between the coefficients that we have, and different things that
have to do with the location, or different things that
have to do with time, okay? So these are both the
temporal aspect of our system and the spatial aspect. So to summarize, using ipyLeaflet is a powerful tool that
might be of use to you in other places. We saw that in New York state, most of the snow accumulation
is in the Adirondacks. And snow accumulation
increases with elevation, but the relationship is weak. It's not really a strong elevation. Locations with elevation 400 to 600 meters have widely varying accumulation of snow. 400 and 600 meters is
really not very high places. The altitude is not
really very significant. So that's it. This is the part where we analyze visually relationship to space
and relationship to time. And next, we're going to do
the same kind of analysis, but we're going to do it
in a more quantitative way. I'll see you next time.
--- end {9.3_transcript.txt} ---
--- start{9.4_lec.pdf} ---
9.4
Distribution
of
Coefficients
DSC 232R, Class 9: PCA for Weather Data
Studying the Distribution of the
Coefficients
Coeff_1-3 capture most of the variance in the snow-depth
distribution

We can now look at how these coefficients vary from year to
year

--- end {9.4_lec.pdf} ---
--- start{9.4_transcript.txt} ---
(bright music) (air whooshing) - [Instructor] Hi, so we saw the analysis, we did some detailed analysis
of the three top eigenvectors of snow depth, and we could
see what each of them means. The first one is how much snow. The second one is whether
the season is early or late. And the third one is whether
the season is short or long. And now we want to see how these things, how these descriptors,
the three descriptors that we have change over time, okay? So this is part of the goal
of dimensionality reduction. Instead of having 365 values
that we are trying to compare, we're just comparing three, so it makes the task a little bit easier. So we're going to select
all of these values and we're going to look at some plots. So this is a type of
plot called a box plot and what it says for each year and this is about the first coefficient, what is the mean value
of that coefficient. What's the 25 and 75 percentile? And then what's the range from
the minimum to the maximum? Okay, so we can see that some years have very little snow, very consistently throughout Massachusetts and here very little snow,
here a little bit more. And all of a sudden, 2011 and 2015, we see that there is a lot of snow, okay? And that the variation
is also much bigger. So that's something we can see from looking at this
graph for coefficient one. Now let's look at coefficient two. Now, coefficient two is about whether the season started early or late. So, when it's negative
the season started early and when it's positive, it's late. So, we see again, 2015 was a special year. There was a lot of snow
and it started late, okay? And then we can see other times, whether it was early or
late, it kind of changes. Finally, we can look at the third coefficient and that tells us whether
the season was short or long. So we see that 2015 was
actually another short season. So it was a short season,
but with a lot of snow, okay. Now another thing we
can do is we can look at how much do the residuals to the different eigenvectors explain. So we know that if we have
all of the eigenvectors, they explain everything, but we wanna see how much the first and second and so on. So here is the first, the blue line. And you see that there
are some that it explains, there are some instances where, the residual is very small and some kind of here is in the middle and then some where the
residual is very large. So this is the best part. This is the part where we model the best and here the part where
we model not so well. And then as we go to
the later eigenvectors, so to the orange line and the green line, the red line and the purple line, we see how we explain more
and more of the signal and the residual, the fraction that a lot of the residual is explained
is becoming bigger and bigger. So this is the residual. The residual here, you get smaller residual for
let's say 20% of the data. Another way to look at the same thing is to look at the distribution
of the coefficient. So how do the coefficients vary? So, interestingly, the first coefficient, the one about the volume,
it varies from zero and up because you can't have
less than zero snow. So there's a limit to how low it can go and then it can go up quite a lot. And if you look at the second
and third, they can vary. They have a nominal value that
is in the middle, let's say. No, that's the mean start date of the snow and the mean length of the
year, of the snow season. But it can be lower and it can be higher. So that's what we can look at. We can use our coefficients as descriptors of the years and we can study the
distribution that way. Okay, we'll see you next time.
--- end {9.4_transcript.txt} ---
--- start{9.5_lec.pdf} ---
Analyze whether early or late snow changes more year to year or place to
place.

• We know from previous notebooks that the value of coef_2 corresponds to whether the snow
  season is early or late.
• We want to study whether early/late season is more dependent on the year or the location
• We will use RMS Error to quantify the strength of these dependencies
The Pivot
An operation for creating an XY table from a list.
Estimating the effect of the year vs the effect of the station

To estimate the effect of time vs. location on the second eigenvector coefficient we compute:

• The average row: mean-by-station

• The average column: mean-by-year

We then compute the RMS before and after subtracting either the row or the column vector.
Conclusion Of Analysis

The effect of time is about four times as large as the effect of location.
Iterative reduction
• After removing one component, the other component can have an effect.
• We can use alternating minimization to remove the combined effect of location and time.
Is this conclusion justified?

Check the graph for individual stations
Summary
•   After removing one component, the other component can have an effect.
•   RMS can be used to quantify the effect of different factors (here, time vs. space)
•   The snow season in NY seems to be getting earlier and earlier since 1960.
•   but the high variance places doubt on this conclusion.

--- end {9.5_lec.pdf} ---
--- start{9.5_transcript.txt} ---
(light music) - [Instructor] Hi. So, we used visualization to
understand the relationship between the coefficients and the location and the coefficients in the year. Now we want to ask more
high level question. When we are talking about coefficient two, the coefficient that
corresponds to the season being early or late, is it more affected by a year to year or is it more affected
by location to location? You might have your guess, but we want something quantitative. So, how are we going to do it? We're going to use the RMS approach. We'll talk more about the RMS approach in general in the future,
but just to start, the principal component method
is an RMS minimizing method. It basically finds the representation using the top eigenvector for a point, such that the residual or the
square difference distance between the reconstruction and the point are as small as possible, so that's minimizing
root-mean-square error. Okay. So, we're going to look at
New York and at snow depth. And we're going to extract
longitude and latitude for each station, so we're going to get this table with the name of the station, the year, and then the coefficient. Okay. From this table, we want to
form this list of things. We want to construct a table where one direction would be the location and the other
direction would be the year. So, for this, we use an
operation called pivot, okay? So, that's the pivot operation, pdf.pivot. What we are doing is take
an index one is the year, index two is the station, and the values are the
coefficient two, okay? So, this is how the beginning
of this table looks. All of this is going on in pandas. We're not using Spark for this table. Now we're just checking again how many undefined we have. And so we see that there are many stations
with many, many undefined and that is because
stations came onto New York over the last 150 years. There are very few stations
from 150 years ago, but there are very many stations that are just 10 or 20 years ago, That results in this, many, many of the places of
the entries in this table are actually not defined. Okay? And then, here we're looking at the years. Now like how many stations we have, how many fraction that are undefined, and as a function of
the year and then we see that we used to have
most of them undefined, then it improved until
here and then it became gradually worse and
then it really improved in the last 20 years. Okay, so here is a part of our table and we can see that for these years, we relatively have more entries for the later years. And here, we're looking
at the number of nulls, fraction of years that are undefined, number of stations, and so here, we again see
that for most stations, many of the years are undefined. Okay, so the estimating
the effects of the year versus the effect of the station, okay? So, what we can do is we
can look at the average over all of the years, so the average amount that we have and we can see how well that
average explains that row. It basically is just the
variance of that row. And then we can do the same thing and do based on the columns. And so, we can see how much
would explaining things just based on the year
do and just how much about by the station do And so, what we get is so the total root-mean-square error is 489 and then this root-mean-squared error reduces by only 23 if you look by station, but it reduces by 88 when
you do mean by year, okay? So, if you take years,
the explaining variable, then it explains much more of the variance than if you take the station. So, that gives us reason
to say it's mostly by year, which kind of makes sense
if you think about it because some years, the front comes early in the season, some
years, they come late. It doesn't really depend
so much on the height, on how much snow you have, or it doesn't depend so
much on the location. Okay, so the conclusion of this analysis is the effect of time is about four times as large as the effect of location. Now, we don't have to stop here. We can actually do this
iterated minimization, where after we subtract the
explanatory power of the year, we then go and on the residual, we explain using the station and then we go back and forth. What we get is something like this. So after removing by year, we get to 400. After then, removing
by station, we get 377. And then, we can continue iterating but things don't really, we basically reach a stable minimum, okay? So, that gives us how much we can explain using both the station and the year. So, one thing that might be interesting is to look at how much, if
the explanation is by year, what does it look like? Like is there a trend over the years? And indeed, if we look at the mean as a function of the year, we see that this coefficient
tends to go smaller and smaller which means that the season tends to start earlier and earlier. That's the conclusion from this. And it might be a correct conclusion, but we need to check ourself. So, how would we check ourself? We can just look at how
do the coefficient change for individual stations? And what we see is that
the change is very big. It's on the range of -800 to 400, while the range that we had the decrease is actually very, very much smaller. So, we are are always
suspicious when we have data that has very large variants
and some very small trend or some very small change in the mean. So, it doesn't mean that
it's necessarily incorrect, but it would require a
much more detailed analysis to really say is it
true that over the time, at least in the State of New York, these seasons are becoming
earlier and earlier? So, the problem of
missing data is prevalent and needs to be addressed, so that's one of the
advantages of root-mean-square. It just doesn't care
about the missing values, so that's one nice thing about it. The RMS can be used to quantify the effect of different factor, here, time versus space. And the snow season in New York seems to be getting earlier
and earlier since 1960. It seems to be doing that, but the high variance places
doubt on the conclusion. So, we have something
that has this slight trend but meanwhile, it varies
a lot from year to year and from station to station and we are a little bit uncomfortable making a strong statement. Okay, so thanks and I'll see you next time.
--- end {9.5_transcript.txt} ---
--- start{environ.yaml} ---
../../../../environ.yaml
--- end {environ.yaml} ---
--- start{lib.tgz} ---
 ` ;]wƕIڴ6/S Aee$kSVM%n0 0` OÞ_`Si ٳk3w{~sG֎þI߬ll6ۭ=^e[*d뇞>8"B*rhpbk2pQ$ob>oOqaImd76FoolVH{%s0ϦX1Ú
5ªKjqL	 e(_*N϶<w@ؠ_LF'jE ~I*U3Z	(vX*g3Ip}mىS;N^7ZQ$4^k5۳DeN<o4"bGJ\B,_8tvcba)f,2l' 1~M-4)z9tLCwP7֨;[[ ܍}ˋ6v'5oiM<:v_PS6ubj}<]8S©tB+"OC>*X	Fϩ)[B 	h,J1#j3!vE4a00yr#5yY˓3"L0\B2OHnͳ;aU%SV*z;4xkbK.Yi@@&Y+>gJVպ))s.$0r}wԍ%fv@zڈ&'l&y>b{Ei&CύNg}"Vq&_]9]llɹ/T	+%oCRH+p$A6POO>ůHuL]ωoAHZ%*æ=Lj,P;dS'giw$:pg&-@+Z:"J6efi:I0H'TZ	":8IN,N`Ǻ4ߤskz4n>eV/Yx`dW h~rAVŅF?\@׋;/AYsd[Z
0(4S00+;ZkP+SfRQS{թQ5Цd1℆6ԛFK׾X!{N-oF͖D.LF"7]V[WWf`KVj|FF=SQ$>ZrER	%P(lVE	g~S\GA5mlpeVP{|Ybz CF|ÂR&a垊e BbvЄmgO4R6۴l빽Jj+xiU[Vv0aCZ`"RwT`ϗɯԗETMXvb
[BHc4CK34c 
M6_4dLc":Lr$5[!Mmh+xb;;,oBǮٽ;Y&^q<Z ]^_A	TJ`N"Q9[3f՗.2a-pT9:<8¨jMDd@PEACq6C䄜9;^B :E>%`Nmm&\ B2yL.~Cf!ȣX.]iڭT'gl
BϩIaRWςC	+{YXcI>tH'?<$OLCr}za sS<pPIyhD8h%xȃ; 2ON!l`{^q#2XEHe$"2m|F8Mofi.#e@mt(
5ٱkKzrwk^joЗin;H I6-LR ?0>L<ώ)9 -K6N4RDj@ūa]YjӑjX7bfb	w@]s;ȴlEx&Ywf=VW_b4%AP|Brdh_UywA.3_zA"(<i,]ФE1E*o6=̡ԐvL1j*v\Z$
Ƥ/6UYa[єzb'xXfd@bEҁKz8JU~AtS!B,'7IސASu0:L'񺸴p9%Ղ6 2"phk4-"f@=jbDVxm<W@,&V#)Z
惢*˧#xl4Jjf1sTوț	Fs4Ѝ-LE>+Qb9sB/?)'ϡBPi}T6အ5(29qK.BChw28EG~sw{bڹ6;lm[πoGfO|yz<zح8gÃn7/Dq78@;ikl;rt#pS~}d}?J_wʰghh rp0k+o!pU4g]<38X@#mkпݛ>t|#h\3xxǼ
zNL
D D'u=ty% hG&45 wmH(>ƄyƶpbԀu}ɾ5! )!KuqǚY~7V	GtHn|dA3!Dj~(=f)"s&a\lˢ[G 80o?{D6DLܗ@l^W2G4ux6 *^OL($Mt˨-]#8ͥL0IAd6x8
UϚg(VDR>NFVS2d?0<y{ ИHCRD%JCbSdˇH|vd*c0@Ԧ%7 \r YBP@ȩXA4sQ׽}L^+`Yk8Iiv"elHqqcE"{1iv==0/kyP1V=KPV$?hW!I6\pX=?}Gbg	DWx6U3GK7*g%&ÆiϣS@2v=<Ke#Օj\LIcROavX[alk=CM,+K}Ck^_uٙ4-:d~W)͞dq8H(Q{m@0A;=Σ2a'd*
8%20KW(gá,q:x$q_.|t	13Ҏ1}N*l&J
<(JG t:mbJGV̺wnI/k;¥ӤÒTRtYԖ|_c.B1<Yz:M&@wé#Wg̴vExܰSHP&g
 c_+4#zZ 
V>ɀ@Isڈ,FG?[/	ƅ#Ùy#JF+B I N1֥TTxWtz)̀(k
Uo6iC x q+7=p3Lv) :L:@Mֵ)Ugp<ą# 8F"O
L" cqJl0	x?ďx^jv0imPoln~#ϊK.Ϳ<	4L,]cf>wiqD:I"0w~0%"~nSf".f8J(_XD'!ׇ0rmvBV/8).'~㔥̖:ЬFm]T h6<'Y:'0x
TP8dP,'QDOÚBALO;>)U?>~+sǱ/zKrhKPˋ%X6*贡3N+: h,ey*?%",>`Cjܳ 67+"j`e&Xj)Ia+[ZǲnKRztgp#Jh.hؿ/BvAEE.hؗuɉ8d P]=No#	?RƋH08	Ez7b:XeQ.g\澑E33`pOM{~xDJ)T\CDLSSjr%bJ;xHt4^pg2;e=]?Yݑe]L&ӰU/ȵԷ|YKf>^Ye~౹Lh.ZR#tٹgt;p$OWh&w2df9Ȱl;XT7$+a	F˷s-S, 4sw==x	Cnts63aw8Ћ#cTApd oc npQfgz!cE'#coo8,I; M{hka|o)pc;\ߝ6m xM?my{_J+/r*Nի<*׼tZ\qsV_uB<Fkլ>787J5(^7-ߒ9gd:f9?ǲSU^=[^%J;RXWyYs9Tg꟫a8W(]}pW^Aګ
ԭXEU;ZX~~=<t^=p_to؁=> 9f{ALWkWp6鏸6z av[&u捩kGAcjń7Zߦ͑;oyL(7E4hAB>=.)&ސuE83-o8zjzʳ{'>|}8|p=>9xዏgsvv^F0&=e%Ϯm&yK.G֮D4_rI*E^|[ Sk~^m9gIS:-=}!of1- >!cꟺQ?{=_c;ϯӤPށ(2)~ov81noݼY^\ïHQ.oOTKP mC0ۛ<hM|i]-a
s#_8t>\T@O_-߱b߯tzȟw5q׹{wv IIIie hZ-r,MRTbAfs0HqKR>:R)TCN9r
[T){3;; "YvHEggz٩i巯--$-erfw#YyϡB@;;5C}{r%s?_'ճ{N+3g=VdsF!Ѵ{=nǉ~޲=&@&)y\_ɟ=KyeqsDǽ&#w1r;$R	X]Lc@=u=	<iLvOh5ǟkOT~Γ7VhuK})VV{:;qa'Zkfemc!7m	 -ϛg8~^'[m,m.+ hRޝm
FR8	Y1)rJI.`Vsf7!nGVhYDV|B:xLP9"SL<$"O*HBztRe2A$zBFy"Vٿʿ2_mzvg{>o;#9|nG9n2oy~K	g1qQ{[D	É۪+UKWASln|40&(Y"dTأ*}U+:vP6y+]JSU0kS:yJJnJ-J>Mqr+tlf%LNPFCeM:unVcU9~pFX&8[fBzҹyi	0}wBe`] IkոzoOox2mt< ^zod13}ϺfcB;n(@6VE*xRyf~GrKa]wǥKW`$tf*W)A0Ž>P2 ܯݺ	r^zM6k&D7u%;헦Ok1%%7!Tr9Ƶy xBk1OcńrY=|4wZe%וvU2=	G_
#f6QeYyXrBvڱW3%mуDx؆c5AZw=8='CW.鎋,'c+6ZXW戳wn":{ 0QJkx~񝩇ψo_te@7G (ek٦9./8X_/DB$a<Dg6[OLeմi7h)DPe7L.g2;955dVnhޜ9}R]jM n\Z%浵6o{GER:mF{;{c4-&)wbiMRc;8JGvѮ(Ca3>wҶsOz4+@Gvw0_.[ѪԜoL̒}Ͳj1'Q=C1}Xy`Y]J4fT8ݧ~a:uUYeH]J60@e͝qO*СՊ^UV8%Wp>og"O185Y'	q8cNvM08kx `SKY??o,k73c9o1}3%=    iLVwl"'u	pC";(w)sck5[яLNLNB)HZyA%?!d	T&vz>OHb.B(]zĶoVE껄L7ND\~SZ0.W闻\HO%FfNa*1,Qm|!62Ā&laC3¤Ah"h4hS @3-`/Rjչl_]iiq1v5fκpk:en}>;AY8&2B[,3+`M9hR{A ?b1 [3Ñc4jlL^+>Grhz(@4FʻHJF>pO"=|?
o.'4TDj>,'eNc&qvʦR:P\nbO4Y++^NŇD&9wYM[{veT8LЈ5Vx(U`_-n iHT^{+vⷔ~gggJgf}nOw?~w$Z~۟\",["#c9`pGgF6k6^H4/gJE_ZZ[Vt 5mf!")|j!}QAJUu`K5\b`Y  sdA\a)dW`Dw|-QWԘQuo>$PH_Lq͢ޣ73838ӡN=JԧԍQK*cAj%T~pO%Ŕ?o#ada5U+~s˫+ZT.GU#.WǖSrUUQBnFj8Z!r鱠*xe~LXS8Nvy$|egԳ:lȷ,SSٳyҿnj7}p/+\[%9}?M~`	*Kh8ҫ=;Uɏ4a.o^!!%$q6q=T*wV5_jʞ\=kE͟<]-m$N^u3y|bV-+Dl#(!{BJ9!@.XI0wc	;be QYxk5D+z{EJ$^_m$Y\WQ@Ef嫙^c?fjaxkNbLJAc܏	oլrL[ݱҴ7ݦbfQ`8?Rvk&HMLh7Zw05R9 ul6l-͗6u1cPXB{k5x@H݌A3HNFvtcVڱh͠Ui.BKB7*FohʟX
I+tRF?A]:E wc0{=,.V'ʎs!jwakjtQ^+9S/)%B2I1II7C4tƨ-l>l_[$3ٮ
Y,n$OqY9<ȟuWoEY:Wʳ"WG9f6[m?J+uס.R7[ZBiGPͤGcѽ16  BDlXAp0E,@I@($bi嫔z{Dwt؏-hg.[N/x)A!-
WI+?ɿ-$ʛΏ|3SUoEb܁{il9m1hǲPwYPmޗWb(#tFԮ"f'\uZ0UwA[E5w<=֜H$D5l<T5I3MhUAr%%Ubn8?U7ߣ+Mݫ"?4vcA,hKwO[iVjZc^O"Hg/,^X_Tw<vomVxXUvWZkffuȄ;U[8=^`۔Yc ַ lo?e3	a`$pK~Z6"g\y%^ִBNɫj*v쐦ʁp!g=&ǉI> )'d!w戬"B"^r:wqs$HAĭL5w]%jிjѯu۶/>|=X!XZg&S۬3HpB7Q|M8חÇ_Ye=̩Aٙf??f(e̬%t!#hCs!`	#YM몝CAb%,;\EVWIx,PGy#wPmtKqv}#ˈ~t^	7+.">(_b7T%x24UkW	P]*>`Kvc?&tuѠ!~*Юll{K͂-'[!>z!x&<X(lqBTlav6hCMX43qwqizCʚ@#PC)/Dz]Juw apjaτAuwJ`|jSD0#oj	[@~sqB߂w Y{KN9\a-⌎{,4itk,ҁ?%X/8=dkc鯃Ne=B.VKQGB̨f$Yyh2|jHlG[\Q{beCt2ʖAElI8@0*͘*1ݫ[=_|AOj2Y2mS.7LҼajȴI!<	s^F1I:gR۬	OArm5à~c?{AdtT~gg7/|oZdߺx?lc`땨Zh<aWWTkmT=DK"22!ћCCzB|:VHX8Gx,>|vr|6~5~-~=}04+-z̾(O/DH?r)419؎J;=Ey`q;2r7繼c}gi8^:-6}QYu^xu\~V97
{Ka=#Dύe~Ӧ;Aǁ|lْZE?JO+ 罰Sii{VګU<Q5s^9#Y&DHuVe;.).z໵3`ov>[aef@i
fWC1AԘ%ixW եMPMyz}\ZKZm7jj|ϴ#MSk7Zy۠hYeSӛ@5&VT Uo+ى\އ,#Jar0y#K5:}z0}VI@ZL!e5;56K,VnMG 0,b3`s'Ob֖@OTjG7_?RލyǍ	*E11ﴷ!wv	{u}o	v9뜀=.HNj,A\1耀AXk
uMiV	JմW'UWP*S/%PeC/VMvR֎Mƺ8UFv1׿c%t~+.c sBss_ʪu|dM.v;wxԼBhb֎ss `UKue*wu2쏷j^RrՇN~nLWn˻#Wu݉P(쾣|O
x52VKP[ڕ<-A=Uӛ)!$2F=wىG9x^օȺ0`9O`j5[ A]nJ=nAք|raw%`M$:$lAQT2 - AQGtD10AD A 
ePPQDtPqEeD\p\}(̸0:(;yu޽ZNa
<FCGu8c-)h`;8`MdA	TE&k%.U	
J
.##!AgG2
_:v-HAuFpfh,sUGF?c;UOF-vp ؏O.lȲ8Q-4b"(X҉>>HZe_h\v;)RKSqfp

+hN԰j)I%bVWHES;캰È>UzkC҃GLbq\Lгgj1{c=NV[]R>%3gCŝUE6BGC'?E$DtY{E$:b.>.fK0)·6*3PD8=C CHOy&y`J%4(b	'H%20$ؾ`"&6@jPb
٠{9$yH9@(Y-0Q2>1[c"ja$Ҍ(a%(L
(̎C
xlRPe/'}|
RdۣHUgm=C򀞥-PӲbAB&3b06=Q9EF(`ia(2c>f*&?Wm<b]+B@E]6E%z&i3*MJ_ th	B2)i p	@Lîa`'`\0]
&~F:XD9z<a9fFtH4f`|Tb@q̡`WbTz8`$] т|}^r5!W ǬiơQi#.tZEQe?<hD\PPhR1r,Me-Ӆԫ ;ZVVǫAmjjP [?F\[hOx
ַ"Nda #ELEKWMJݚX <=    NYv"m= "l'bQ'C @,0URոAyhl %U xid$)T7\¼3u}xٙ$,"RHNd0RP ^Q&Qc5|.2 )i(p0Awb`6&RSDӔH'}JsqssC w_b#?]Mć+H	9,B6VJh'o_jݿ{N^O?aTO 1 :kA#	h
P,e
}_3G`mOif5%Et1d bG"\~@Fc6+4(d^5FJa3̣C0
㓰Y( e9Hb$H-jHJuHp@ݨHlS=XLuCgʎ&bGg@ZqE,:.H#G:b/o + JrM@@ĚljIWϯ!hK$f4:Bb# !K'\˥YXʔIt%YgiD`:XБK;@Y ) |5Y~j]#KVe!Z>'{CkZ<tDtq|D*ǽ0CT0n7u^8@w]T e]HUPo
*ͲZ]-|88t`	dvWܥjI=DFc:)*!Y^aTB)cy(}EQҒc"g6mFMjW
mB+XLI&@	{Ӏ31wbCE&GpfikWaO_4'S$BPl*8O~;u;VyW]nP躱sO绺|WWKlV@<0:X!+.з;Nζ~*1&3=\	X;bca1fP\
hSDs)D
$ʧ(*l%?p`?W5#:7R1dQl0KLٱZ<p%)#ay[zl#mUACS:V$@I<F1x7a[PMKQr7`T X\SJY6v$]8x 1JYe*SJ<chT<O=Vqzg'=|*WJcW=(oJ'Y8/*CR*pCbf*aw!$֊	*^vduel!rRR@PNn&:!g'MIlÌ& '[v:e`Y,c<ŕSE'8M:ز٪~MRͮ	F"v"3P=싐8MjvTV!*@_u28r[k,(WkU@TH%Vf@Ϫ,?=?(}'67&
\Հ)NqWWWW7^W,3Chu"׺̺6~FyY>-.2(eF֚'q}Bhn]/:5vRaPO˲뾅ީ\ǩ[o4|o4֭e)R4:Go/ĂD7ZkG<G{w',kާ
uMq4ꉛcg\u]B_)I챖XVTcY^ɔ1nrxߚ!28>;B̉rz})VbO\mYiê~W3_4BXԛ7}nM1iS0uŦGw-ޙ~b[nZ82fSiÛ490'l8>l2L-%1=+_̴]1`-eVn;,g,#-|qUʻŭ
MJ/Y?knHu?O.w0^ge-([wN񃝍ߔɆݘl@
"x{ۿŻYjYx5E灹<j`V:mA׆zvڽ[^fV3rULy[s܀Gno.5gT\􎩶uctxڴַN7o_p%9%4pKeݒ'So?0e䓝bkvXN_krHo<VK5U^rI_TÁ6o[W__o4S,.%*t'/ވ_-p1{|M9 ʺ''\ɡ?]#mǨjrron^𮽏ρDXNFn̸HQ
L;rL`vQ)kߐNoے]hZ|ƶ-h\ =b),\&%}_{vooV3Ԝ֚O3y`%x(qAtŜ
Cxamo-`l-?#veifψ_˙!
ԕi6Ec\?nSkOwaAq%ß)|M=.&?kql:o~`AZ]o,٤Q{$ᆛY>u:_Zb򺟬F$.=<!ύMʦڍ^6a=ۮMm? 0rxy|`mLdt|C1iz&pU4MYIzBcmL+mkz/2Q1o}}qd޹)./5{kC;l97ΑʪT|aBT=G9Z5ojW< ,(vP7Kw*i^nӆJv/Rcy'JL6mGgʬ/^y?8>Iv	ԋC̹sJG"lSgK'r9U119;1B#WΗPJix=^[k߽,szq<tZwm=v6Qq_4]xpSN<cxJO,y,<{GV)ޗz3eV?XRVsKw5$<cz\ԴBPu)J-Wx<os,	9[Cf̵?ɮһT.M/S 11q\HՀ#tάٴy.^2~UE=6GU6o澹ݟR?P_Rjݠ˗M2-x|N;<Z1CiVubd4F5vU%)*^tfY'rf]s#),Xa"7l=?P/p9~L
?Tw{q@҆F?e泜xu,%,QBVQkwv3rg/3ܲڂ5Nẇ]ݨ/jO^Q,X웖q9SKv߾hea\-)3\py}m:3g?O??Ѿ+쳝].ԍ~:ֲa{W_]ry$-pھٚ1	]<)chPw	φ.A{L̖dvxw>su[W+ B#v
8./8=v¹^nuo~XEmhkAA^xǽGC<\.sh9NꎵV0):voϩ7Xp6!W}vF4%=bz*r|dQKDt&L
N-9-3vN?PâRa[5De=Y=Ÿ$j"^f%lјL?6ޤhqQٖBCMvgv~Ι323=9`k׻/{WęVVtǍB42ՠT	*Hc!IUWRj]V۵EzZţ^X-PZ[U9Ln߷aHfy5{3Sʴа;B/rȂ%|zu]ʟZ;f;^o{rw~]|R}E<("yp鲫:!wQޣ+}sއK_q&2?xW5-cfMwҷ3>/5w9'aGwWMg9ozS<<MSj;/bt*K	ӂ3pއn~xI;,3ee{ &Ec%*<S{ۑEsT+:g𩧞ZQ5ƱפX{bsK]}vnE=YeawS3fu_t>T}jW:[1d!dɺߕr~;<6cљo^wNl:{ӝ3YKêu&t{..zĮ]-J֖fn{n:i?na	!S6RmXD-H'8vnщq{m?kJk}o]ف{tJO[>pQktCK:a~'<ʼ+ղ3+~[qOrSK.T_(lZMt'gouA;Ӧsߞ5OF}a5XgTǌ2\^?v~юݜ%ֵL^8&u]/c}\rv{tDG$d>O>_L
lܱ?)uyG_(i^YR*4m^VZSwi?V_i{~(z}uVptY/x*ew_X.wMFvbm\էneK&A!tvvl.lŚ:r̗5CGZ6;Я+>jm¥m&1mZFgU=t5^b|9\|2w.zNv"&hya/OI{ܠ⹖7mHoc3n5gTǖd'>-w!6M2vw
S#d.piO/(۽\n1i;^fo8wϼ͓Gfa8SN[<sWe+CfT3=i?UUnISo?ctWV۹jGW!3zW~9@ԆoL<#WXGw{DEWInH'"dHc[5ypQx͆>Z)V&CLKmvtu@FK՗J#wLI}|0j=f+/nq7'Θp%-V40ڍXDԍ/FߺQz-˦5ߐ:Xr\-+*'=-Ś/:xgk>_1P{Tv,;3wffjT,(W]DU>ѭvqm?r=W_ykz~zV8ekO̿8xwŜ//xwmn=ִWUU#_<'y9?Tv-C|q_5Ьrw,	ysnPcm[v<0'νU[5=Q6XD;Ĺ睛]dcm#zGEM[<(|ҡv)p$nHf}ʠ{Sa;-j,SyJ]Z:ɵso06DϖK[|6# 0a_Q/n_:[	-o|gzl氠>h;h;k`D'v((&;7e4~§n6{~(g3^E79/}^w;޵.m	O֬}9lʻj[i}_|yo~<3:ZUG}[ _ibap}d*A{CBB~GH@QMbl"GҩOǨA(BH @D(,]LTщjȦ pTSQL}T:5xTBTrOe025`I;1@jF-3SXI]҂ME Cɨ3:H(  Pwk,zhn=E8
a$TэI7eYl`jR.;WB3VxnZlZE-=3vS:i'ljdR'2|.GZXPa5Kd-LP`zq	 &W-?(]5Փѐ>XFR4?"&+#-@z%DPle Ch0N+a{D`V,
D*` ^6*1I	Ab FN2Y]v :cD̂o|XP6d!_JB[P<h	Fze&ceZFKtOd$t$i-24H|`*Ӏq3a!Dj%p[2bZ'59$/ာD.Juc]aW*ER+"0zMG({|RUJ*0v7iY+ ,b
IR0(D$wn?2#7,Ġ>
pGJzouwBFwD?lIWDyE~<dhgfFO-sdBCQt )CEfsBV(vu4*
oH߱;Y +NPqT}	 \PHiQ=̖kڸ
.&xtvCQ'<Ʌve&AiiGKGw;it :Iyi[RH$##!IP~+4C!a,J*DTTJ"F9%)5ES>,fL(Xh"!jb8G|YS  $1>nR~;svNY4`H o$aӈ5%+>o9`Yz%1(6?*Z]X?~g9ߖF 4lo;PBu-]2Eب*`vp ҍv96#z҈(&q'ѾL8fII~"cs}rN$۠.d5	ؔhh(._NFwh-q		_3@#G n>?邍!:?af-P({xP\V+Viܘ  a) 03Xt"ĕjჀ0uP̀rAh+m X\f0`B0L7j:DX
%R"LL5FGA{s*2#1,aW$TC!h~ȍ ?p,!$V_`dSA.h" nKgt`
pE*ybXa"F`="ҋA:q]Xyl EyDBb3NI%Pzt7&އȬ±x~\<@Bϲ0CV1̜yÏebQZL1c|[k(q
`lA+"FΖ2_gLo
vxZe2:3A#aQqu$KPY|*,wأ,< 3ؒ`OՓ+,vq PliEzK*eX!XQ0W8BP2=;	"NFő4k ;s톾Ա_=J"u?"(""P *}+?8xDH#WIBӁEsEK8_%k8 Z @~_,8{SD׃/_ߟHTEvSr~u೟i 4ýy(bHw@dXA!Fa>V76	E>DFwy@	eO!P'- "B_9 EI	`I&CC(AL$;XEH&HsF,¡HT(ea5t
mdF߸f36"{	f;-}@^Ftb <ԇ;P	(doÓn8+zgaTRFWQ:E_Q@@[űC	`U([_׀Htp-iQ@2$'EyqR%=-eqI+ʤN;x]Ւg{ޞ|%K!9'%@si?LG3_;(Xw~,ףV=^6{MIyDæiB0B&.rBr$+?bXO3fN_D~!@$NA[PXBy+b<ft=dAESw
(E[,b 
$7Jyx.Ȱ[yHab/$SW"cҠDl28.$o<-RE̘!՘Ť8M}v1nHÑ(F&HNLfv)pRݠmHl0n̥/f,C&/yq@nv/27iA,"p+& VD CEٞ;|ae(0:&js[PZ]yy9B:';27hkX&əU$O"eN˒270D%~MU
BFsnbҗ*MZWph֐;Z3y{vC,WIvl['aE'z'r,fԸ7Y1:PM`0.lN#'(nsN|Ne4(̅mBEi,$y5Q	gcnpfD] WD$*B u2 Pq$Y.69.2WC	ᱸLgd/&h}XU<~!L;[$QzR}X5Upz?y%3D?W Ynb|̝-!åKY"WF"#)x3o|#~K&"߁tޖ4'TrO-qP	l:lACAd&l׻hށh\#3JPB!|2A
'Йfh20EK8Ɣt)Sh OlgA*mJT8IS_Ҥ^\7)	EjHʲ~ )Rڤ4Db @7aЮ9Hc/Ha@9U]F	]IDWst`]x3% so@XVx 10UA徵
pRDNJ3!]l.>hD.U@CKhy5QUB;oUCqiU39 Z㻆=̌Y^7J<(Y:C1s2I?7pR`W SKjZu4JZXE']琓r%9gI<;^,ĭvھ<ӋB.
d)#/Q[vn.lRC5UP% UaW0FwQ208`O >)g(gjWpnhqaRiO(_³!7|tT<zxEj4obň<+wTE#hBTAr85hu6B$) j?T|vjz_ HJS@sEՐ!ї"ITx_Z=)?[ɶj+8>Q&qA5m?,	\M t
_BrcnyrsW8<z4tx?|H"gvC0hX#r0#ߞ<w_m_=?{O/oX\8soo=@fExNpukBI60M7t_=}} 4Rwdol\G`1\"be 2B`^Yfwﾳ˲e02?y]a`(=Ązan`G6_v,Z T40N^0@nP,%pޟ%Cz0U6}5P25ã?RG?|!by>XQA!ط#y8O*T;E*7c<fOBtA/r)XeI7dms.5XO	w2ՠ9bvX`~(  M6}PxۈR)`S=VO,(b- L/'>~bYU,oyǠ3Wh7t*-,NN	ckвd`ȹ8۞t: O[F#\^L,ܵ*&i"ºߞxeoqӹ|!6A2=U,eE%ۮOy@o*2f:MJӑ^u_<nLaPȖn	g՚NM{?8aFo=-49lo
r9*;Ab!H z.u@S>3&K"^QEup(	LNW6Am(g_mҢ(.10g;)	7/b<U\jxrR3»[߃_x(j0V9xwhV"[˾TSrӷ&nx
BzYH>s LJjYgw`,yT9>!u}Eb"Szf9]$&pFao%#:prZkBEDX)ms:`_þ\Q -R0 /M;Kze.b gZw~XzU-DEFm[h׸0 ̃YTVvTΝJM )6+t0Hd\Nz;V!J)Ek	1g`(:ʗt a:iikS,eFѵu:t/Y;(`wdxL\+DXb4,.jsK{"[GI=y4֐9CsDiF_-rd&m;W^UO.jL <sCazfሸA&ıNڂt4
^5;|0
0(qOϾbbz0@k:I<f%n3Ȓ<,h	J$7ڷ(^&[=2u%˾u  %Al\xŪȳp(}>oŧ,X6i+KMܠ=AN04v4I~p-%Kvn<0ZlPR^\QU
Y:0{ʑ]b]buDq/=XFS BUwnd2hի*AK2$7%9[[%e(4IڠԻjC%^w=o-ԩh
BMbw\ӡp=|m%:,	xTyCQ-'+m`PY JcmfS8zJnn٫ib p3.E@wN7ʏx)!/xdfE$*lph&ŋys4`/ @Hҏ٧As<)ãr\=>Oy|e#2n$6Dp><k=ՈZ5{?OpH6 YeUΓyjLB'ַVx>eR>cWiU..a4p`A:$fБĄ7oO	wCE\[AqЩP('!h@s Sļ`ǧO{OOϖ1(qCx:U%1$MTj~MwϧЪPzN8VumZ?A
oH^OWY@LAb-RWMX'.C֥G}'$ qlW3fލ^%?
\L+GV`8wcC3*8^x۵IuyqP4iC=Y_[ JBϸ]%u"-{kHhіk/h(6(2|h; mCw-a(M~b?6a.g2ne)m' &;ӎ]@u/h
ɉս$4Cv3	6suFzu@v06!,b-#o"f	hģ]]e\#$*E|WF	7ِ <ʬlH[gƋ^.f3guLiÔ@2D8RҼáZ0dL."f/y6Go*mk<	jK,5 rWAT!&Qm}v"lk86H_^9:H2nW
3UqbMt1c@o
ѹ\3\j
xg9HUbAV 1Ȑ,sq)df^ɽ,NI	'-)plRde=<<&fIDg"ӍT@h00PAH)ܗHq%>oL#ʽxQR,|n3I[/yR34
-~rjt0jcOܰJl0tJV:;ǡt$bd,;S%7b^(%M=R\RB:/<$FǘGɖioW}^G+&{{hׄ)ѝ!2lh%Y-.RTUj(6Gk,s#DU:B$3+9&m67$d[u^˕S3M ~0+:E
% vݡ$lil1yb	cK?}C!(ޞ=;;S#U>#);磭SdiMl[@0yԉ+lI'O2 eC*Vky@ZCܻdD_7e*i+&+]qb۸>lvy}$ OWv'2eǺdU,cž#x,h<+	[_y,`K}k:ST>᏾911mJITN&7wmդʑ,5KYvgJp8| x7Om5S3wv_x!X0^-q3&	.%4n%K{dTnHT=@I7tΰL㩪H6Ai }CnN ,PڪQr@ͷ)I[K*b1GC4q瘤*o\X /(up^l>ntD>A3LL*?Yx#x,Nl~ |'%{Q1,-cݷ&%V7@[B05<P%+582X˧EKxhKjd>\Sxt+AFdM.RQ!(f85͍<.yƞ
<ī5Qu1Tb8ƾh9ue8 q6]+-HLٶ U}ޠ԰unV![X`M8	Ssdjֈ%c$Cub1$AcVX66
;ܔ d+o%:;'t(<r
rx8V?F;eu5g@~"6%cZMw!;v*gAy|'e阺ݻqJO<,ht)x}L A`pO,́7"*H#ݗF.vW;Y}H:N܁g~rآ*DԈ.ghL蚣}V0u*̛[KmoRQ	aM=ClETb.Y8KNԶvaف!ʌ-Lԕ3hmarJ :S}VJhi:b'[-^O$FY"RF^cSC 4zĵFjZ'+TuW156Mf*vzߌ$ej&jgFՅVI[2h5DWV4/cWFcPp)m1
U*/
QP(@36Qiфl1C`,يʘԐ
*ܭ}s>s>sqQ  
--- end {lib.tgz} ---
--- start{main.py} ---
def main():
    print("Hello from week-05!")


if __name__ == "__main__":
    main()

--- end {main.py} ---
--- start{pyproject.toml} ---
[project]
name = "week-05"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "gdown>=5.2.1",
    "ipykernel>=7.1.0",
    "ipywidgets>=8.1.8",
    "matplotlib>=3.10.8",
    "numpy>=2.4.1",
    "pandas>=2.3.3",
    "pyspark>=4.1.1",
    "pyyaml>=6.0.3",
    "scikit-learn>=1.8.0",
]

--- end {pyproject.toml} ---
--- start{README.md} ---

--- end {README.md} ---
