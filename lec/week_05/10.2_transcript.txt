(upbeat music) - [Narrator] Hi, we've talked about the power of using
principle component analysis to do dimensionality reduction. Take a data that is 365 dimensional down to three dimensional. And the question is, why are
we so excited about that? Why can't we just leave data in its original high dimension and just do our machine learning that way? Well, there are some issues
come up when you do that, data doesn't behave in high dimensions the way that your intuition is when you talk about low dimensions. So let's think about a few of those cases. So suppose we have a single gaussian, so something like this, and we are doing it now in end dimensions, I can draw end dimensions,
so I'll just draw two, so this is the second dimension and it's also gaussian in this way, okay? So it's gaussian in both
of these directions. So it's a spherical gaussian. And now we want to ask, what's the distribution of
the distance from the origin? Okay, so here, when we
talk about one dimension it's pretty clear that the distribution of the length would be like
a gaussian, so let's see. So what we see is, that the
length when the dimension is two is indeed very close
to the gaussian, okay? Or to the distribution that is derived from the gaussian when you
take absolute value, okay? So that's what you get in dimension two. But when you go to dimension 10 it starts to behave differently. Right here, the mean is
about 1.2 or something, here the mean is three, here, when we're in dimension
200, the mean is about 14, and when we're in dimension
2000, the mean is about 45. But not only is the mean about 45, but it's very, very, the distribution, the variance
around it is very small, it's kind of something like standard deviation of two or three. Okay so we never see any values here, and we never see any values here. We see values only in a particular place. So even though our intuition is that if you have a gaussian
that looks like something like maybe contours or
like something like this. In high dimension, everything
that is here doesn't matter. The only thing that matters is data that is somewhere around some
kind of variation here so, let me use another color, so there is some distance
where almost all of the vectors are that distance from the center, and that's just because
the dimension is high, it's not because we change
this spherical structure or the gaussian or anything like that, so that's already kind
of surprising, right? So we have, that if you take
a vector generated by gaussian or by any distribution,
any reasonable distribution that is spherical, you get
that there is one distance that is predominant and then distances larger
or smaller than that, are just, never occur, okay? Okay, what's the explanation of this? Why is that so? Well the length of a vector X is the sum of the xi squared, square root. Now because the distribution is gaussian these terms xi are also
independent from each other, okay? That's just the property
of gaussian, okay? Now as the mean of the gaussian is zero, the expected value of
xi is about one, is one no dependence on d, that's the variance of xi, and the variance of xi
squared, is also some constant, also not dependent on d. And therefore when we're adding these independent random variable, we're adding d of them, then we get the length that we get is
square root of cd, okay? So that's the new length. Now the distribution of xi
is concentrated around d, and the distribution of sum of xi squared is concentrated around square root of d. Why is that? Because the length is d and the standard deviation
is square root of cd. So as d increases, cd relative to d becomes smaller and smaller, okay? So if we are right C square
root of d over the mean d, so the variance over d, and that is like C over square root of d. So as the variance increases, as the dimension increases, the relative variance
of the length decreases and that's what creates
this concentration. Okay, so that's two vectors that's a single vector, what's its length? Now what about the distance
between two vectors that are drawn from the same gaussian? So again, what we have is
that in small dimension it behaves like what we would expect, but as we increase the dimension, the distribution becomes
highly concentrated around a particular value. So take any two random vectors from this gaussian distribution, they almost always have the
same distance from each other. So now when we think about things like using distance, like
nearest neighbor and so on, it's clear that we have some trouble. So maybe these strange things happen because we're taking the
data from a single gaussian. Maybe if we take it from two gaussians that are distant from each other, then it'll not happen, okay? So here is what we're going to do. We're going to basically generate, generate two gaussians,
like one here and one here this one at minus five,
and this one at five, and then the other dimensions are always going to be zero, okay? So these are the two
gaussians we're generating and we're going to ask the same question, what is the length of
a typical vector, okay? So what would we expect? We expect most of the vectors
to have length close to five, and as a dimension increases, the length of the
vectors becomes dominated by the coordinates with mean zero. So what do I mean by that? Here we have the dimension two, so we see that that the
mean is indeed five, at dimension 20, the
mean is more like seven, and then now a dimension
200 the mean is 15 and at 2000, the mean is 45, okay? So we see that the distance
between the vectors has very little to do with the distance between the centers of the gaussians. It has more to do with the dimension because the dimension increases the distance between vectors irrespective of where the mean of
where they were generated. Okay, so to compare
with a single gaussian, in dimension 2000, the
distribution are almost the same, concentrated around
square root of 2000 is 45. We cannot use distance
distribution to distinguish between k equal one
and k equal two, right? We cannot say that there is more centers, less centers based on the distribution. Okay, now let's think
about the same distribution but distance between two points. So we have the same two
gaussians at minus five and five and now we're going to take
a single point from here and another point from somewhere, from maybe from here, maybe from here. Clearly if the points
are in different places we expect the distance
between them to be 10, and if they're from the same place we expect it to be close to zero, so let's see what actually happens. So indeed in dimension two, we have distance close to
zero if they're the same and distance close to 10 if
they are from different centers. And similarly here, so
you have these two peaks even though the distances are
now not the correct distances, at least we can distinguish
between the case that the two points are
from the same gaussian or they're from different gaussians. But once we go to 200
and definitely in 2000, all of this goes away, the distance between
the vectors is dominated by all of these other coordinates. And so we cannot really tell if they're from the same center
or from different centers. So K-nearest-neighbors is likely
to fail in high dimensions. Suppose that we have gaussians that are labeled plus one or minus one, and we use K-nearest-neighbor to identify the cluster
which the point came from, in low dimension that works
but not in high dimension. So given any two vector,
the distance between them is about square root of
d, regardless of whether they come from the same
center or from different ones. Okay, so the distance loses
its information power, it doesn't tell us
really where things are. Let's think about one more thing that we could measure is dot products. So when we take dot
products between two vectors that are coming from these two clusters, the result, what we expect
is it should be either minus 25 if they come
from the opposite centers or plus 25 if they come
from the same center. Okay, so plus minus 25 seems like a lot but when we reach high
dimension it all fails, right? So what we have is that, with the dot product in
two dimensions indeed we have it centered with minus 25 or 25. And even at 20 it's a
pretty good separator. And at 200 it's maintains
some power of separation, we see that there are two
peaks in the distribution, but once we reached 2000, then there's absolutely no
information in the dot product, it does not tell us what
is going on with our data. Okay so to summarize,
the 2d and 3d intuitions, break down at high dimensions. The length of the vectors and the distance of the
vectors and the dot product between vectors all
become highly concentrated and independent of the
structure of the data. So they depend really
only on the dimension. The curse of dimensionality, is that most statistical methods break down at high dimensions. A way out, is to find
some high dimensional data has low intrinsic dimension. For example, if you have small
number of PCA eigenvectors to explain a large
fraction of the variants, that's a case in which you have found low intrinsic dimension and now a lot of these
methods work much better. So, that's what I wanted
to talk about today and I'll see you next time.