--- start{dsc232r_syllabus.pdf} ---
                   DSC 232R: BIG DATA ANALYSIS USING SPARK
                                                 Winter 2026
                                             Course Syllabus


Instructional Team Information
Instructor:: Edwin Solares
Email: esolares@ucsd.edu
Office hours: TBD

Course Description
This course has two main goals: The first is providing an introduction to using large scale data analysis
frameworks (Spark, XGBoost and others). This includes the underlying computer architecture and the
programming abstractions. The second is to combine methods from statistics and machine learning to perform
large scale analysis, identify statistically significant patterns, and visualize statistical summaries.

Course Objectives
At the end of the course, students will be able to:
 ▪ Program Spark using Psyspark
 ▪ Identify computational bottlenecks in big data analysis
 ▪ Apply principal component analysis (PCA) to weather data
 ▪ Understand different machine learning methodologies
 ▪ Study the stability of learning algorithms

Course Format
This is a 10-week course that is asynchronous in that you are not required to attend any sessions during the
week. All your core lectures are pre-recorded for you to view, by Professor Yoav Freund (UCSD professor and
designer of this course). Additional recorded resources will be provided by your current instructor, both ahead
of time and throughout the quarter. There will also be lots of opportunities to connect with your instructional
team, both live over Zoom for office hours and weekly live discussions, and in different asynchronous
discussion (Piazza and Discord).
Curriculum
                  Topic                                                Content
 Engineering Big Data                        ●   I/O limited computation and the memory bottlenecks
                                             ●   Data parallel computation in the cloud
                                             ●   Map-reduce, NFS, Spark
                                             ●   DataFrames and SQL in Spark
 Analyzing Big Data                          ●   Unsupervised Learning: PCA
                                             ●   Analysis of NOAA historical weather data using PCA
                                             ●   Visualizing and understanding PCA eigen-
                                                 decomposition
                                             ●   Mapping weather information using iPyLeaflet
 Classification                              ●   Decision trees, bagging, and random forests
                                             ●   Boosting
 Machine Learning Paradigms                  ●   RMS Methodology
                                             ●   The strange behaviors of high dimensional data
                                             ●   Generative vs. discriminative learning
                                             ●   Learning by elimination
 Quantifying Stability and                   ●   k-fold cross validation
 Overfitting                                 ●   The bootstrap
                                             ●   Margins
                                             ●   Easy and hard examples


Learning Materials

Required: Lecture Videos
Each week includes a series of lecture videos divided into two classes, each covering different topics.
Each class’s lectures are required, and it is recommended you view them in order. All information
needed to complete the class can be found in these lectures.


Resource Library
You can access the course's Resource Library from the home page of this course. The Resource Library
contains all the course lecture videos organized by module, as well as resources to help reinforce
understanding foundational concepts which students need to understand to be successful in the
course.


Assessments, Evaluation, and Grading

Quizzes
There will be 15 quizzes, all offered through Canvas and graded by the course TA. Not every
week/class has a quiz. This means modules will contain up to 1 quiz per week. Quizzes for each week
will become available on Mondays at 12:00 a.m. PST and will be due the following Sunday at 11:59
p.m. PST.
Students will have three chances to take the quiz and the highest of your three scores will be
recorded. Quiz answers will be available at midnight the day after they are due, so no late quizzes will
be accepted.

The quizzes together will represent 10% of the final grade. However, each quiz is graded out of a
different number of points reflecting the content of the quiz.


Assignments
There will be 4 graded assignments. Assignments will be in the form of Jupyter Notebooks /
NBGrader assignments offered through a tool called Vocareum.


Group Project
You will work in groups of up to 5 students to complete an independent project using a public dataset
of your choosing that you will post to a website called Kaggle for review. This will be a competition,
where the top 3 students (voted on by students) will receive extra credit.


Final
A final exam will be offered the last week of the class. This is a timed exam, and you will have 3 hours
to complete it. It will be offered through Vocareum, similarly to your assignments.


Grading

 Activity                            Percent
 Programming Assignments             30%
 Quizzes                             10%
 Group Project                       30%
 Final Exam                          30%


 Grade           Range
 A+              100
 A               95-99
 A-              90-94
 B+              87-89
 B               83-86
 B-              80-82
 C+              77-79
 C               73-76
 C-              70-72
 F               <70

Accessibility
Students requesting accommodations for this course due to a disability must provide
a current Authorization for Accommodation (AFA) letter issued by the UC San Diego Office for
Students with Disabilities (OSD) which is located in University Center 202 behind Center
Hall. Students are required to present their AFA letters to Faculty (please make arrangements to
contact me privately) and to the OSD Liaison in the department in advance so that accommodations
may be arranged. Contact the OSD for further information: https://disabilities.ucsd.edu/.

Religious Accommodation
See: EPC Policies on Religious Accommodation, Final Exams, Midterm Exams

It is the policy of the university to make reasonable efforts to accommodate students having bona fide
religious conflicts with scheduled examinations by providing alternative times or methods to take such
examinations. If a student anticipates that a scheduled examination will occur at a time at which his or
her religious beliefs prohibit participation in the examination, the student must submit to the instructor
a statement describing the nature of the religious conflict and specifying the days and times of
conflict.

For final examinations, the statement must be submitted no later than the end of the second week of
instruction of the quarter.

For all other examinations, the statement must be submitted to the instructor as soon as possible after
a particular examination date is scheduled.

If a conflict with the student’s religious beliefs does exist, the instructor will attempt to provide an
alternative, equitable examination that does not create undue hardship for the instructor or for the
other students in the class.

Accessibility
See: Nondiscrimination Policy Statement

CARE at the Sexual Assault Resource Center
858.534.5793 | sarc@ucsd.edu | https://care.ucsd.edu
Counseling and Psychological Services (CAPS)
858.534.3755 |https://caps.ucsd.edu

Subject to Change Policy
Note that the information contained in the course syllabus, other than the grade and absence policies,
may be – under certain circumstances such as a modification to enhance student learning – subject to
change with reasonable advance notice, as deemed appropriate by the instructor.

--- end {dsc232r_syllabus.pdf} ---
--- {start Week 1 Material} ---
--- start{1.1_slides.pdf} ---
What is Data Science?
Digital Sensors everywhere



                             Data
               processing
                            Analysis
                    Two data analysis tasks
1. Establishing facts:
   1.   The water in City X is safe/unsafe to drink.
   2.   The average daily time spent on a smartphone is Y
   3.   The number of almost accidents per-day on highway 5 is Z
2. Making predictions:
   1.   Selecting ads to maximize clicks.
   2.   Automating/assisting medical diagnostics.
   3.   Choosing candidates for Loans.
    A real life example
highway traffic in California.
Caltrans: PeMS (Performance measurement system)
Car Loop Detectors


                     • For each passing car:
                         • flow : number of cars per
                            unit time.
                         • Occupancy: fraction of time
                            that there is a car above the
                            coil.
                     • ~45,000 individual detectors.
                     • Each detector generates a data
                       packet every 30 seconds.
MAS-DSE Capstone Project, 2016
Kevin Dyer, John Gill III, Conway Wong
Mean and STD of total flow (cars per minute)
Top Eigen-vectors

--- end {1.1_slides.pdf} ---
--- start{1.1_transcript.txt} ---
(light music) - I'd like to talk about
the goal of this course. What is data science? We hear the term used a lot,
but what does it actually mean? So it's a combination of two words, data which most of us understand, and then science, which is not completely
clear what it means. Okay, so let me give you my take on that. So we're surrounded by data. Data is collected on pads in gas stations, using laptops in industry, temperature is gauged, and then the wireless world around us is giving us data all the time. All of this data is sent to
some kind of central computers. And using that data, what we want to do is data analysis. So that is basically data
collection and analysis. But what is data science? So data science I would say
is rational decision making. So how do we make decisions
using data in a rational way that is useful and profitable? So I would divide it into
two kinds of decisions, big decisions and small decisions. What are big decisions? So one decision might be, is the water in a particular
city safe or unsafe to drink? So that's a decision that is important because we would have to do something if the water is unsafe. Another question that is now very relevant is will increasing the federal
interest rate by 75 points, let's say, curb inflation
in the United States? So that's a prediction
that is super important to the whole economy and the Federal Reserve people
have to decide how to do that or how much to increase
the rate at this point. Even more global decision is about moving away from fossil fuel. So we want to say, will increasing prices
of gas to $8 a gallon, will that reduce global
warming by one degree? So that's a benefit of doing that. And then then the cost of doing that. So we would like to know what is the expected gain
from doing such a thing. So these are all very big decisions involving a lot of people and involving big institutions and making those decisions is based on making, on testing hypotheses and verifying that this is
to the best we know the true, the true state of the world. And that is what we predict
will happen based on this. Then there are many small decisions that we are surrounded by
every day, mostly on the web. So for instance, which ad
are you likely to click on? That is an important thing
for a company like Google to basically identify the ad that you are the most likely to click on so that they can have an ad revenue. A similar one is for Netflix. What movie should Netflix recommend to you as the next movie? And a slightly bigger one, more important would be
which loan applications should a particular bank grant? So what is the, of the many applications that they get, which ones should they grant? So that is a bigger decision but it affects maybe
one person or a family. So you see there's gradations of the different kinds of decisions where the big decisions
are really decisions that would be made by
some kind of set of people that deliberate on the issue. And the second set of decisions is more decisions that can be automated and can be made online without user, without human intervention. So let's look at a particular real example of this kind of situation. So we're going to talk
about the highway traffic in California, and there is Caltrans which is the organization that basically is charged with deciding, with collecting data and making decisions about how to improve the transportation in the United States or in California. And this is basically the problem that they're trying to solve, right? So we've all been there. Traffic jams, you can be
stuck there for hours, they take a lot of time out of the day from people and also they, it costs a lot of gas to have these cars basically
idling on the freeway. So how do we get data so that we can make decisions about this? So the way that Caltrans does it is that there are these, there are these loops on the
highway here, pairs of loops. And these basically can measure
when a car goes over them. It can measure how long the car is there, and how many cars pass,
let's say per minute. So this is a more of a schematic
of how this is working, collecting the data, then sending it to a computer, and all of these computers
through the internet collect the data into a central place. So what we have for each passing, for each detector, we have the flow, so the number of cars per unit time, and then we have the occupancy. So what fraction of the time is a car above a particular loop, okay? And there are many such detectors. There are about 45,000
of them in California. And each one of these detectors
generates a data packet that says what happened during that half a
minute every 30 seconds. So a lot of data that
gets collected together. And then we want to do a
data science on that data. So here comes the science part, okay? So there is actually a
science of traffic flow and the science gives us a model of how does traffic go on a highway. And this is the basic graph here. So we have the density of the cars or how many cars per mile there are, and then here is the speed
that the cars move at, okay? So actually the number of cars per minute that pass the detector, okay? So we want a lot of cars
to pass the detector, that would be ideal flow. And so what we have is that
there is this relationship between the density and the flow, okay? So if we have pretty empty highway, then cars can move freely very, very fast. And that's this beginning part. And then as there are more and more cars, the cars have to go slower and at some point you reach a peak. So this is the rate at which
if cars go at this rate, then you get the maximum
number of cars per minute. So if you go beyond that,
beyond that density, if you have the cars closer together, they start to go significantly slower. And so you have less flow, right? You basically, the cars are
overall moving along less. And so you're getting worse and worse flow until you hit the traffic jam where basically cars are standstill and basically are not moving
and everybody's frustrated. So this is the theory of it. And of course the practice
doesn't look exactly like the theory or the model. The practice looks something like this. So here we have one set of measurements and then we here we have
another set of measurements. So it is somewhat reminiscent of here we have the peak around here and once we have this representation we can start to do some
kind of rational decisions like how many cars would, what density of cars can
this highway withstand, beyond that it'll start to
be non-productive, right? So we want to basically be
as much as possible here but without getting into here, right? And definitely we don't want to get down all the way down here. So data science at its
heart is taking the science, what people know about a
particular process or phenomenon. And we take data that we measured and we basically combine the data to basically tell us whether
the data fits the model and what are the parameters of the model that would make this fit good. And then once we have those parameters we can go and start to make decisions. So what decisions are
we going to try to make? Again, the big decisions are things like what
kind of changes we need to make to a highway. So maybe we kind of
change a particular place to have a different kind of intersection or we might just make a smaller change which is to change the
distribution of the lanes, right? So maybe there will be
more lanes in one direction and fewer lanes in another direction. And so each one of these has a cost. If it's a big change like this, it's probably in the
many millions of dollars. If it's just a change of lanes, it's probably just using some paint, it might be much, much cheaper. And the question is, what
would be more effective, right? And so we can use the model
that we measured to say, oh, actually this little
change that doesn't cost a lot will actually be quite beneficial and this change is very expensive and will not really gain
us a lot of improvement in the traffic flow. So these are the big decisions, millions of dollars are involved. Small decisions are something like this. You've probably all experienced this when you get on the highway, there is this light that tells you that you have to wait until it's green and then you go, okay, and why is that important for the flow? Again, because of what I said, that if there are too many cars at the same place in the highway, basically if the density is too high, then everybody suffers. So it's better to block people from getting on the highway when you get on and then
the traffic can flow. Now of course here the
traffic is pretty busy, so you really need to do that. But if the traffic is
actually pretty empty, so there's no, very few cars,
maybe two in the morning, then you don't need to do that, right? So this is many small decisions that are made moment to moment to decide how much does a car
have to wait before it goes, or how many cars should we let to go on the highway per minute? So this is the kind of small decisions that we would basically do based
on the same kind of models. So to start talking about models, one of the basic things
that you would want to do is you just want to know what
is the traffic on average and how much it varies from average. So how much is the standard deviation? So this is this plot here, this is the average and this
is minus one standard deviation and plus one standard deviation. So you see that there's a lot of variation but the overall shape make sense that early in the morning
there's very little traffic, then it builds up until
like eight in the morning and then this traffic goes, becomes generally higher until 6:00 PM and then it goes down. So this is the average, however, any individual place is not likely to be quite
like the average, right? Because there are different
profiles of the traffic. So how do we get to
understand those profiles? So that's a method called
principle component analysis, which we will go over. And basically what this
tells you is it tells you what are the essentially the profiles that are prevalent in the
way that the traffic flows. And so what we have here
is that blue line here is basically the AM traffic jam. And what we have here is we
have the PM traffic jam, okay? And if you just think
about it for a minute, it makes a lot of sense because some directions of the highway are busy in the morning, some directions are busy in the afternoon, this might not be the case always. Some places have traffic jams both in the morning and in the afternoon but it's quite typical for
highways in California. So I showed you an example of what it is, what is data science, and how you make big and
small decisions based on it. So what are the
ingredients of data science that I would like to put in your mind? First one is math. So in order to build these models and relate these models to data, you need some solid foundation in math, mostly linear algebra,
probability and statistics. Then you need to learn about
machine learning algorithms. So there are algorithms that are based on this
probability and statistics and let you basically take data and build flexible models of this data. Then all of that becomes at the end software development, right? If you want to actually do it at scale and maybe packages that are
ready are not good enough for your specific problem, you need to essentially
know how to write software and how to develop a program software that would be useful for many people. And then there is this whole other area which is domain knowledge. So as I said, in the highway case, there is the science of traffic flow. So this is a well-established science that really understands how the main parameters of traffic behave. So because it is so specific
to individual things, it is kind of hard to really teach it in any individual course, right? So it's not so much something
that you would learn here, you will get some examples. It is more something that when you get employed
as a data scientist, you will learn how this particular, the science of that
particular type of company or institution. So there's various different domains and it really takes a long
time to acquire the knowledge of a particular domain in depth, right? So if you really want to be an independent data analysis person or data science person, you need to devote to that area a significant amount of time. And so we will only touch
on particular small examples like this traffic and like the
weather and things like that. So that's more or less what
I would say is data science. This is the introduction
to what you're going to do. And next time we're going
to basically start digging into the details. Okay, so I'll see you next time.
--- end {1.1_transcript.txt} ---
--- start{1.2_slides.pdf} ---
Data Engineering
      and
 Data Models


                   1
Data Science vs. Data Engineering
Data Science                           Data Engineering
• Building models                      • Builds the data and computation
                                         infrastructure used by the data
• Answering business questions           scientist.
• Uses Statistics / Machine learning   • Uses relational databases,
• Expects data to be available and       streaming, cloud computing …
  computation to be fast.
What data engineering we will cover
• Will Cover
   • Hadoop File System
   • Data partitioning and partition balancing
   • Caching and persistence
   • Checkpointing

• Will Not Cover
   • Data Cleaning
   • Spark Server configuration and optimization.
   • Creating scalable pipelines
   • Containerization
         Three Data Models:
        Relations, Tensors and
             Dataframes
Data Scientists and Data Engineering communicate using these data
                              models
       Three Data Models (and languages)
                             name1   name2       …        namen                   name1   name2           …     namen

type        1            …
                             type1
                                 1
                                     type2       …        typen
                                                          …
                                                                                  type1
                                                                                      1
                                                                                          type2           …     typen
                                                                                                                …
                   n                         n                                                        n




                                                                  name1
  …




                                                                          …
                Amn                       Rn                                                   Amn




                                                                  …
                                                                                          Array of values
       1




                                 Set of tuples (rows)




                                                                              1
           Array of values            (typed by column)                                     (typed by column)
  m




                                                                          m
                                                                  namem
                Matrix                 Relation/                                             Dataframe
                                        Table



           Linear                    Relational                                                   ?
           Algebra                    Algebra
                                 type
                                            1        …            n


Matrices




                                   …
                                                     Amn




                                        1
                                                Array of values




                                   m
                                                   Matrix

• A Rectangle of numbers
• All numbers are of the same type
• Can transpose (exchange rows and colummns)
• Can add and multiply.
• Typically small enough to reside in memory of one computer.
                                                        name1      name2           …       namen
                                                         type1 1   type2           …     … typen
                                                                             n

 Tables                                                                  Rn
                                                                Set of tuples (rows)
                                                                     (typed by column)


                                                                           Table

• Row: A tuple – defines an entity
• Column: a named property of the entity: each column has a type
• Schema: a collection of related tables.
• Keys: a special column (1 per table) that uniquely identifies
• Can select a set of rows based on a condition (SQL)
• Can be very large (TB) and reside on disk.
• Disk Data Structures make retrieval much faster than flat files.
Why Use DataBases
• We can do the same in python/C/Matlab
• But only if they fit in memory
• A Database can span many disks on many computers.
• Disk Data Structures to make retrieval much faster than flat files.
Dataframes
• A blend of ideas from relations and matrices
   • Originally defined in the S language, which led to R
   • Ordered, named rows and columns.
   • But only columns have types.
• Beware: there is not a standard definition of dataframes
    • Some define dataframes as relations with an ordering key. No transpose!
• Two popular flavors: Pandas DataFrames and Spark DataFrames.
Summary
• Data Science: Data analytics on large complex data.
• Data Engineering: making the analytics scale on very large data.
• Matrices are the basic data structure for linear algebra, Neural
  Networks.
• Tables are the basic data structure for relational databases.
• DataFrames are a compromise between tables and matrices.

--- end {1.2_slides.pdf} ---
--- start{1.2_transcript.txt} ---
(bright music) - Okay, I'd like to tell you
some about the difference between data science and data engineering. So, data science, which is the
main subject of this course, is about building models. So, you get data and you're
trying to build a model that would represent this data and capture the important aspect of it. And that would be used to
answer some scientific question or some business question. So, you as a data scientist,
are going to present it to people that are not data scientists but are domain experts. What you use is your tools are tools from statistics and machine learning. Those are the basic tools that you use. And what you expect is that
when you have the data, it's available to you
in an easy to use format and that the software for analyzing it will run quickly on that data. You will do your part to
make the software efficient, but the basic infrastructure is usually not part of
your responsibility. On the other hand, you
have the data engineers that are becoming more and more important. And their job is to actually
create the infrastructure on which you can run your software and that stores your data. They uses their basic tools, things like relational
databases, streaming, cloud computing, things of that type. And we will cover only
a little bit about it just so that you can
talk with a data engineer in a meaningful way, but a
course on data engineering truly would be a separate
course from this. So, what we will cover is
the Hadoop File System. That's a general way of having a distributed file
system that is redundant. Data partitioning and partition balancing, which is part of what you do in Spark in order to make use of this
distributed data system. Caching and persistence, and checkpointing. So, all of these are things that interface between you and the
underlying infrastructure. But we will not cover the
details of the infrastructure. So, things like data cleaning, Spark Server configuration
and optimization. So, you need detailed understanding about how the Spark System is organized so that you can optimize
it for a particular need. And creating scalable pipelines, which is when you take
your code for data analysis and bring it into production, so that it can run on the
business side in a regular way by people that are not data
scientists or data engineers. And containerization is a methodology for packaging software and data, so that you can run it, you
can move it like a container, and you can move it
from one infrastructure to another infrastructure without worrying about
the internal details. Okay, so the way that the communication between the data scientist
and the data engineer works is that we have data models. So, we have standard ways to store and communicate and process data. And those basically, this is the language by which we communicate
with the data engineers to make data available for
us in the relevant way. So, we're going to talk
about three data models. The first one is the matrix. And the matrix is just an array of values, so it's something that
you're familiar with if you ever used NumPy or MATLAB. The relation or table
which is data structure used in relational databases, which tends to be very
efficient if you're storing it very large amounts of data on disk. And then, we have an kind
of a combination of the two, which is called DataFrames and is not exactly clear and
well-established data model, but it is becoming more and
more popular in various setups. And so, it kind of takes
some of the properties of the matrix and some of the properties of relations and tables. So, to start with matrices. So, what is a matrix? Matrix is basically a
rectangle of numbers, okay? And all the numbers have the same type, so they are like all floats or all in 16 or something like that. And you can do various
operations on a matrix. You can transpose it, which is
one of the basic operations. You exchange the rows for
columns and columns for rows. And you can perform algebraic operations, you can add matrices,
you can multiply matrices to generate new matrices. So, it's really a mathematical object that allows you to do a lot
of different linear algebra. And typically, this kind of dataset is stored in the memory
of one computer, okay? This is the pre-big data setup where you take all of the
data that you want to analyze, you put it in the memory of one computer and you process it. In big data, you usually cannot do that. You cannot put all of
the data in one machine, so that's where the matrices
reach problem point. Tables are in a way similar, they're also a rectangle of values. However, in a table, a row is basically an entity. It's like one of your customers or one of the cars you
have for the company. And the columns are
properties of that, okay? And so, the columns, all of the column has
to be of the same type, it can be a string, it can be an integer, it can be something else, but they all have to be the same type. But across different columns, they can have different types, okay? So, a schema is basically a definition of a set of tables and how
they're related to each other. And that schema is basically the way that a person using relational databases would describe the whole
collection of data. So, it's multiple tables, and they sometimes share columns or sometimes they have keys
to each other and so on. So, keys is a special column that is basically, you can think about it as an identifier for a
row, the name of the row, the ID of the row, so that
this ID can be mentioned in other tables and point
to that particular row. And there is a language that goes with this relational database. The most common language is SQL. And the SQL allows you to describe what part of the data you want and the machinery underneath
it will get you that data. So, SQL is a kind of programming language, but it's programming
language that is declarative, so it basically just
says, here is what I want, not how to get it. And the main power of relational databases is that the data that is
stored in a relational database can be very, very large, okay? The amount of time that it takes to get to a particular record
or a set of records is small. You don't have to read
all of the big file, all of the table into
memory, you can do things just with the keys and
pointers and indices to get to the relevant parts. So, why do we use databases? In principal, we can do the
same in Python or C or MATLAB, but only if they fit in memory, okay? Only if the data fits in memory does it make sense to do
it in these languages. And the database can span many computers, many disks on many computers, so it can basically store
much larger amount of data. And reading the data into the machine is much faster than if
you read a flat file. Finally, let's talk about DataFrames. So, DataFrames is a blend. There are a blend of ideas from tables and ideas from matrices. And originally, it was
defined by statisticians in the language S and
then in the language R. And what you have is a table
with named rows and columns, but only columns have types, okay? So, the columns have types, and so the types are
preserved along the column. And a little bit of the problem with this is that if you are
really a database person, DataFrames are not completely consistent according to the tables that you'd have. So, two popular implementations of DataFrames for data science. One is pandas, you might
be familiar with already. And the other is inside Spark, there is also DataFrames and those are DataFrames that
are intended to be on disk, so they're intended to
make use of the fact that you can store things on disk and then access them efficiently. So, to summarize, data
science is data analytics using large complex data that
doesn't fit in one computer. Data engineering is the
making of the infrastructure that makes it possible to do data science. Matrices are the basic data
structure for linear algebra, Neural Networks, and tables are the basic data structure for relational databases. And DataFrames are
essentially a compromise. Okay, so that's a little bit of a view about what is data engineering and how it relates to data science and what are the data models that are used for this communication between data engineering and data science. See you next time.
--- end {1.2_transcript.txt} ---
--- start{1.3_notebook.txt} ---
%pylab inline
from time import time
%pylab is deprecated, use %matplotlib inline and import the required libraries.
Populating the interactive namespace from numpy and matplotlib
Numpy vs. Native Python
This notebook demonstrates the large difference in speed between native python code and numpy.

We start with a very simple task of taking the product of two square matrices. In numpy this done using the function .dot()

n=100
A=ones([n,n])
B=copy(A)
C=copy(A)
%%time
C=A.dot(B)
CPU times: user 22.4 ms, sys: 1.45 ms, total: 23.9 ms
Wall time: 13.2 ms
matrix product in native python
To perform the same operation in python we need a three level nested loop. Instead of 1ms, the operation takes 500ms

%%time
for i in range(A.shape[0]):
    for j in range(A.shape[1]):
        s=0
        for k in range(A.shape[1]):
            s+=A[i,k]*B[k,j]
        C[i,j]=s  
t2=time()
CPU times: user 746 ms, sys: 65.5 ms, total: 812 ms
Wall time: 519 ms
def test_mat_prod_numpy(n=100):
    A=ones([n,n])
    B=copy(A)
    t0=time()
    C=A.dot(B)
    t1=time()
    return t1-t0

def test_mat_prod_python(n=100):
    A=ones([n,n])
    B=copy(A)
    C=copy(A)

    t1=time()
    for i in range(A.shape[0]):
        for j in range(A.shape[1]):
            s=0
            for k in range(A.shape[1]):
                s+=A[i,k]*B[k,j]
            C[i,j]=s  
    t2=time()
    return t2-t1
repeats=10
tn=[[] for i in range(repeats)]
tp=[]
sizes=list(range(1,10))+list(range(10,100,10))+[200,300]
for n in sizes:
    print('\r',n,end='')
    for i in range(repeats):
        tn[i].append(test_mat_prod_numpy(n))
    tp.append(test_mat_prod_python(n))
 300
figure(figsize=[15,10])
for i in range(repeats):
    loglog(sizes,tn[i],label='numpy %d'%i);

loglog(sizes,tp,label='python');
xlabel('matrix size (1 dimension)')
ylabel('seconds')
title('comparing speed of matrix product in numpy and python')
legend()
grid()

Observe

For matrix sizes up to 10x10 there is no significant advantage to numpy over native python
For matrix size of 300x300 numpy is about 10,000 times faster than native python
For sizes up to 10x10 the running time of numpy fluctuates between $10^{-4}$ and $10^{-6}$ of a second.
As the size increases from 10x10 to 300x300 the relative fluctuations decreases.

Inverting a matrix
a = np.random.normal(size=[1000,1000])
a.shape
(1000, 1000)
%%time
ainv = np.linalg.inv(a) 
CPU times: user 199 ms, sys: 12.5 ms, total: 212 ms
Wall time: 67.9 ms
Lets check that ainv is really the inverse of a
The dot product should give us the unit matrix.

C=ainv.dot(a)
C
array([[ 1.00000000e+00, -1.30562228e-13, -7.07212067e-14, ...,
        -1.16129328e-13,  2.65787392e-13,  6.62525590e-14],
       [-1.72750703e-13,  1.00000000e+00,  2.75446332e-13, ...,
         9.90318938e-14,  4.99600361e-14,  1.32116540e-13],
       [ 1.41220369e-13, -3.99680289e-14,  1.00000000e+00, ...,
        -1.33004718e-13,  1.80522264e-13, -3.07365244e-13],
       ...,
       [ 2.33146835e-15, -7.54951657e-15,  2.32452946e-15, ...,
         1.00000000e+00,  9.36750677e-15, -1.14491749e-15],
       [-6.75015599e-14,  1.04805054e-13,  3.33066907e-15, ...,
         1.05249143e-13,  1.00000000e+00,  4.91828800e-14],
       [ 3.01980663e-14, -4.70734562e-14, -5.99520433e-15, ...,
        -5.50670620e-14,  1.32116540e-14,  1.00000000e+00]])
C[C<0.001]=0
C
array([[1., 0., 0., ..., 0., 0., 0.],
       [0., 1., 0., ..., 0., 0., 0.],
       [0., 0., 1., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 1., 0., 0.],
       [0., 0., 0., ..., 0., 1., 0.],
       [0., 0., 0., ..., 0., 0., 1.]])
Linear regression
A = np.array([[1, 2, 1],
              [1,1,2],
              [2,1,1],
              [1,1,1]])
b = np.array([[4,3,5,4],[1,2,3,4]]).T # transpose to align dimensions

x, residuals, rank, s = np.linalg.lstsq(A,b,rcond=None)

print(x) # columns of x are solutions corresponding to columns of b
#[[ 2.05263158  1.63157895]
# [ 1.05263158 -0.36842105]
# [ 0.05263158  0.63157895]]
print(residuals) # also one for each column in b
#[ 0.84210526  5.26315789]
Numpy and memory size
Numpy is very efficient if the dataset fits in memory. Otherwise it becomes very slow.

Comparing numpy to matlab
Matlab, like numpy, is very fast for matrix arithmetic. The reason is simple: they are both based on highly optimized libraries for linear algebra.

For example LAPACK, which was release in 1992 and is written in FORTRAN !

--- end {1.3_notebook.txt} ---
--- start{1.3_slides.pdf} ---
Speeding up data processing
    • Fast libraries vs. pure Python
    • Throughput vs. latency
Fast libraries
• Linear Algebra is the computational engine of data analysis, Neural
  Networks.
• Can be computed in pure python
• But: about 1000 times slower than using numpy
• Numpy Speed similar to matlab speed.
• Both use similar highly optimized libraries
• Example: LAPACK, library released in 1992,
   • written in FORTRAN!
Matlab and Numpy: fast linear algebra
Numpy vs pure python
From: 4_NumpyVsPurePython.ipynb, A,B are 100X100
 Big data starts when
your data cannot fit in
 the memory of one
      computer.
Throughput vs latency
• Throughput: the number of bytes processed per second
   • Can benefit from parallelism
   • Can benefit from processing blocks of data.
   • Important for data science
• Latency: The time it takes to process a single byte from input to
  output.
   • Does not benefit from parallelism
   • Does not benefit from processing blocks of data.
   • Important for gaming, robotics, self-driving cars.
 Queues: an example of Latency vs. throughput
• Latency: time between joining the
  line and leaving the store.
• Throughput: Number of people
  coming out of the store per minute.
• Minimal latency: Cashier work
• The customer cares about latency.
• The store cares about throughput
  (dollars per minute) and to some
  degree latency (customer happier).
Latency and throughput for big data
•
Summary / Increasing throughput for Big
Data

• The bottleneck is usually moving of data, not computation.
• Hardware: better to have many cheap/slow/unreliable computers
  than a few expensive/fast/reliable computing.
• Map-Reduce / Hadoop / Spark = Methods for organizing computation
  on large, unreliable computer clusters.

--- end {1.3_slides.pdf} ---
--- start{1.3_transcript.txt} ---
(light music) (air whooshing) - Let's talk a little bit about the principles or tricks that you can use to make code run faster, specifically code that
is data analysis code. So suppose you have some program that you wrote in Python. And when you run it on a thousand data points, it takes two seconds to run and you're happy with that. But now, you want to run the same program on something like 1 billion points. So you don't want to wait for a million times longer than for the thousand points. And so how would you accelerate the code? And so how would you accelerate the code? So I'm going to talk
about two basic tricks. One is called using an efficient library or vectorizing and the other is the difference based on the difference between
latency and throughput. So regarding fast libraries, suppose we have a program that is doing regression or is training a deep neural network. The engine behind it, underneath it is linear algebra. And we can code this linear algebra in pure Python and that would work but that would be much, much slower than if we use a call to the NumPy library, okay? If the NumPy library has this operation that we want to do in linear algebra, then it would typically do it about a thousand times
faster than pure Python. So NumPy gives you the speed that you expect from code maybe in C++ or from Matlab. And the reason that it gives you the same speed as Matlab is that they both use similar libraries is that they both use similar libraries of optimized code and such a library, for instance, is LAPACK. That is a code base that has been released for the first time in 1992, 30 years ago, but it's still one of the most optimized code for specific computer architectures. So this code is available free of charge and basically is written in Fortran and it's constantly being optimized. So here is an example of what you might want to do. So suppose you want to multiply two matrices A and B and that basically means that you want to take each row here and multiply it by the column here. Take the dot product with a column, okay? And that would give you one element of the result matrix, okay? So you need to do that m times p times and each one of these
operations is m times. So you have the same operations of taking product and adding, but that you have to do it many, many times, okay? And fundamentally, this is the operation that you're doing but the question is, can you do these operations
really efficiently in the computer memory? So here is a little bit of code that is available to you in this notebook. And there are two matrices, A and B that are about a hundred by a hundred each. And you just want to
take the multiplication. So if you just do it with NumPy, that's the dot product operation. It takes about... 370 or 400 microsecond. So about half a millisecond, okay? And if you do it in native Python, then you basically need to write these loops that would perform the summation to generate the final matrix. So these embedded loops in Python are done not in the most efficient way. And therefore, the time that it takes to do that here is 65.5 or 519 millisecond, okay? So about half a second which is about a thousand times slower than the NumPy method. Okay, now, for the second part, big data basically starts when your data cannot fit into the memory of one computer. So suppose we still want to do something like the linear algebra but we cannot fit the whole data into the computer. And then basically, what is the bottleneck is bringing the data from disk to the computer. And that brings us to throughput versus a latency. So what are these two things? I'll first define them and then give you some examples. So throughput is the number of bytes that you process per second. So you process maybe
one gigabyte per second and that can benefit from parallelism because if your machine can do one gigabyte per second, then a hundred machine can maybe do 100 gigabyte per second. And it benefits also from processing big chunks of data rather than doing the data one by one. And this is the kind of thing that we really care about in data science, the throughput. How much data we can process per second? Latency is a different thing. It's basically the time that it takes from an input to an output. So if we give the computer an input, what output it generates? And that doesn't benefit from parallelism because if you have another computer, it doesn't make the computation any faster and the latency any smaller and it doesn't benefit from processing big blocks of data. But it is important in other things. It's important for anything
that is interactive. If you give some command to the computer, you're playing a computer game, you want the reaction
to be very, very fast like fraction of a second, right? And similarly, if you're
doing data analysis and you're hitting a key and you want to get a nice graph, you hope that this graph can be given to you as a matter of fraction of a second. So let's think about latency and throughput in the context of a store where people are going to the cashier. They join the line, a queue to one of the cashiers and then they wait in line until they get to the front and then the cashier sums their purchases and then the cashier sums their purchases and they pay and then they leave, okay? So in this context, latency is the time between the moment you joined the queue and the moment that you
exit the store, okay? So how long did you need to spend in processing that? And remember, the part that you're doing in the queue is part of it. That's part of how of the time that it takes you to process your purchases. Throughput on the other hand, is a different quantity. It's a quantity that says, if I look at the exit from the store, how many people are leaving per minute? So latency here has a minimum possible which is that there's no queue and you just have the cashier process your purchases. That's the minimum that you can expect. And in general, the customer really cares about the latency. You care about how long will it take you to wait in line. The store mostly cares about the throughput which basically means how many people are processed. But of course, they also want to keep the latency reasonable so people don't just leave in frustration. And because if they don't
have enough cashiers, the lines will just grow and grow and grow and grow, right? So they don't want that. They want basically the lines to be stable so that people basically can expect to leave the store in a reasonable amount of time. So what is latency and throughput for big data? Suppose we have one terabyte on disk. The data is basically a sequence of floating point numbers. And what we want to do is just sum the numbers and sum the square of the numbers. That's something that we
will actually want to do. And the question is, how fast can we do it? So it turns out that in a typical computer, the amount of time that it would take you to read the information from the disk and put it in memory is dominating the amount of time to do the actual computation. So basically, the CPU
is just sitting there waiting for data. So if you have 200 megabytes per second that you can get, then a gigabyte will
take you five seconds. And if one terabyte is 5,000 seconds, that will take about 1.4 hours, okay? So that's a long time to do this simple operation. But now, suppose that you
have a hundred machines, they're all exactly like
the original machines but each one of them has a separate disk with part of the data. And then you basically just have all of these machines compute these sums in parallel. And then at the end, you just combine the sums, okay? So this is a perfect throughput increasing operation. And you can do now the one terabyte in 50 seconds instead of 1.4 hours. Okay, so MapReduce, which we're going to talk about and Spark are ways of organizing
this kind of computation. So basically in a way that is transparent to you. It will basically compute the sum in parallel way on all of these machines. Okay, so to summarize, how do we increase
throughput for big data? The bottleneck is usually moving of data, not the computation itself. In terms of hardware, it's better to have many cheap, slow and unreliable computers than to have a few super fast and expensive and reliable computers. So the question is, how do you organize such a cluster of less reliable computers? And MapReduce and Hadoop and Spark are basically methods or methodologies for organizing a computation in these kind of large, unreliable computer clusters, okay? So that's it for this little video.
--- end {1.3_transcript.txt} ---
--- start{2.1_slides.pdf} ---
  Latency, Throughput
And the memory Hierarchy
Some numbers
•
 Example of Latency vs. throughput
• Latency: time between joining the
  line and leaving the cashier.
• Throughput: Number of people
  coming out of the store per minute.
• Minimal latency: Time at Cashier.
• Customer cares about latency.
• Store cares mostly about throughput
  (dollars per minute) and to some
  degree latency (customer happier).
Definitions
•
            Wholesale vs. retail
Wholesale
                                          (water cache)
Costco                                         Retail



                Buying water in
                costco and driving the
                truck to it’s stand:     Walking to the truck
                2 hours                  and buying a water
                10 cents / bottle        bottle: ½ minute
                                         1$ / bottle
Different latencies for different units
•
                                                                                                  Latency = ½ sec
Sequential execution                       ½ sec
                                                                                                  Throughput = 2 byte/sec
CPU 1      byte            byte            byte             byte            byte



Parallel execution                          ½ sec

                                                                                                        Latency = ½ sec
CPU 1       byte            byte                byte         byte                byte

CPU 2             byte            byte             byte            byte             byte                Throughput = 8 byte/sec

CPU 3        byte               byte             byte            byte             byte

CPU 4                    byte            byte             byte            byte             byte
Reading data blocks from disk
• The latency for reading a random block from a spinning Disk is 10ms.
• Why? Because of the mechanics of moving the reading head.
• Instead of reading one byte, read a whole block!
• If all of the data in the block is useful:
   • Latency: 10ms (100Byte/sec for random access)   byte   byte   byte   byte   byte
                                                     byte   byte   byte   byte   byte
   • Throughput: 100MB/sec (for sequential access)
                                                     byte   byte   byte   byte   byte

• Caching: a method for achieving low latency by predicting access.
When do we need low latency?
• When interacting with our cell phone or laptop.
• When playing games online: https://tetris.com/play-tetris
• When checking in to a flight.
• Anything interactive – working on a jupyter notebook.
• Running javascript inside a browser
• Latency = reaction time < 100ms
When do we need high throughput
• When we have a large amount of data to process (50TB),
   • we care about throughput – number of seconds to process 1TB
   • we don’t care about latency = processing one record,
• Example: transfer 50TB of data to the cloud.
   • Regular home line: upload speed 6Mbps = 6/8 MBps -> 771 days ~ 2 years
   • Fast University line: upload speed 100Mbps = 100/8 MBps -> 46 days
   • Dedicated fiber: upload speed 10Gbps = 10/8 GBps -> 11 hours
   • Dedicated fiber costs 10-100K$ per year.
The fastest and cheapest option: Fedex
                        • Latency: 24 hour
                        • Throughput using 50TB snowball:
                            50TB / 24 hours.
                        • Cost ~ 200$
Summary for part 1
• When providing an interactive experience, we care more about
  latency than throughput.
• When processing TB, we care more about throughput than about
  latency.
• Transmitting TB through the internet is slow and expensive.
• Sending a physical disk through Fedex is cheap and high bandwidth.

--- end {2.1_slides.pdf} ---
--- start{2.1_transcript.txt} ---
(gentle upbeat music) (air whooshing) - Hi. Today we're going to talk in some detail about latency throughput and how they relate to
the memory hierarchy. So, we talked about latency
and throughput before using this example. So, suppose that you have people that are buying some product in a store waiting to check out, okay? So there is, there's various queues that people are waiting
in in order to check out. And part of the time necessarily, they will use, they will wait on queue, and part of the time they will wait for the cashier to do her work. So, the latency is the time
between joining the line and leaving the cashier, okay? So, that's the total
time for both of these. And throughput is a different measure. It measures, if you look
at the store as a whole, how many people are leaving
the store per minute, okay? So, that's the throughput. The minimal latency is the time that you spend in the cashier, right? So, that time cannot be reduced further if you have some number of items that will take some amount of time. And the customer cares about latency. So the customer, from the
customer's point of view, waiting in line is a waste of time, right? They'd rather just go to the cashier and pay for their stuff and go. From the store, most of the
interest is in the throughput. So, the store cares about
how many people can be served or can pay in a minute, let's say. And they care also about
latency, but to a smaller degree because latency just
means that the customers are happier or not as happy. But the main thing for the store itself is just how many people
go through and pay. So let's define, let's
give some definitions. So in general, not in a store,
but in a general system, latency is the total
time to process one unit from start to end, okay? From start to finish. All of that time. Throughput is the number of units that you can process in one minute. Right, so they seem very closely related and one is tempted to ask, "Is throughput just one over latency? If I can process 10 things per minute, does that mean that I wait for each, the latency for each
element is six seconds?" Right? And that's not really the case. And we will see why. Okay, so let's imagine the
following system that is, the story is that we have
some guy that has a food truck and they need to have water to sell to people in the food truck. So, what they do is they
go in the morning to Costco and they purchase a
large number of bottles. Let's say, each case has
some 32 bottles in it, and they buy two cases, okay? So to do that, to buy the bottles and bring them to their truck and drive the truck to
where it should stand, that takes a significant amount of time. However, it is worthwhile
because each bottle is pretty cheap in this setup, right? You have 10 cents per bottle
when you buy the water in bulk. Later on, this food truck
with the water bottles in it is serving customers. And now, let's look at the
point of view of the customer. From the point of view of the customer if they want to buy a water
bottle, it will maybe take them about half a minute to pay
for a water bottle and get it. And for that, they're willing to pay a dollar per bottle, right? Rather than the 10 cents. So, they're getting very low latency and the truck is supporting
this low latency. So, you can think about the water, this water that is stored
is water that is stored close to the customer or a cache where we cache our water bottles so that they're ready
for customers to buy. So, if we now look at the units, the different units have
different latencies. So, if the unit is a
package of 32 water bottles, this big package that we buy in Costco, then the latency is pretty long, right? It takes about two hours
to get these water bottles from Costco to the place
where we wanna sell them. So, the throughput in this
case is the number of packages sold in one Costco per hour, okay? So that can be, we can
view that as a throughput and maybe buy 20, maybe people overall buy 20 packages of 32 bottles. So in this case, the throughput is much higher than the latency, right? The latency is about one over the latency. The latency is about two hours. But in one hour, Costco sells 20 packets. So, you have high level of parallelism and the throughput is much,
much higher than the latency. On the other hand, if we
look at the single bottle, then the latency is the time that it takes from requesting a bottle to
having the bottle in your hand which is maybe about half a minute. And the throughput is
really the number of people that come and wanna buy
a water bottle, okay? So, maybe that's 10 per hour. That's a reasonable number. So in this case, the throughput
is much, much smaller than one over the latency. The latency is pretty fast,
you get it in half a minute, but the number of people
that come and buy them, that's separated by 10 by 6
minutes, 10 times an hour. And so it's actually the
throughput is much lower than one over the latency. So, you see that things can be much lower. The throughput can be much
lower than the latency if the serving time is
just a fraction of the time between customers and
it can be much higher if the serving time,
which is here two hours is much larger than the time
between selling packets. If Costco sells 20 packets,
of these big packets per hour, then every about three
minutes, they sell a packet. But so again, this three
minutes is much shorter than the time that it actually takes to process this water bottles. Okay, so this is maybe a
nice story, but in fact, it's highly related to how
things are done in a computer. What things are done in bulk. What things are done in small units. So suppose that we read
data from disk, okay? And the data is stored in blocks like this but the data that is the part that we care about is just these two bytes, okay? So, the latency of reading the block can be as high as 10 millisecond, and that's because you need
to move the disk reading head to a different location and
that takes mechanical movement, and it takes at least 10 millisecond. But instead of reading one byte, we're going to, once we
are in the right place, we're going to read the whole block, okay? It's not going to cost us much more to read the whole block
versus to read just two bytes. So, the question then become, "Is all of the data in the block useful?" Right? If it was just these two elements, then it's not very useful, right? We basically have waited
this 10 millisecond and now we got only two bytes. So the throughput here is, so the latency is 10 millisecond
which is very, very slow. If we had one byte for each
read, then we would have about a hundred bites
per second throughput which is extremely low. In fact, the throughput is about a hundred megabyte per second. And that is because once
you're in the right place you read actually the whole block and the blocks are organized in cylinders. And so these cylinders, you're basically, you can read the whole cylinder
without moving the head. So, that gives you the speed
of 100 megabyte per second. Okay? So, here we see something similar to what we saw with the water bottles. We see that by buying in bulk, by moving things in bulk, we can gain but we need to make sure that we actually make use of
this bulk in an efficient way. So, now I'm going to
tell you about caching which is basically the general method for achieving low latency
by predicting access. So, when do we need low latency? When we have an interaction like interacting with
our cell phone or laptop. Or when we play games online. Or when we check for a flight. We don't want to wait
there for three minutes to see when exactly is the flight. Or anything interactive, anything in which we're basically typing
or touching the screen. And we want a reaction from the computer. So, latency in this context, the required latency is
at most 100 millisecond. So, a 10th of a second is what we perceive as pretty much instantaneous reaction. When do we need high throughput? That's in a very different
kind of situation. When we have a large
amount of data to process. Let's say 50 terabyte. We care about throughput, the number of seconds
to process one terabyte. We don't care about latency,
processing one record. It doesn't matter to us. If taking, processing one
terabyte takes let's say a minute, we do not care if the
first byte of this terabyte takes 10 seconds to process, right? That's all kind of inside. It doesn't matter to us. So, here's an example. Let's say that we transfer 50
terabyte data to the cloud. So, on a regular home
line the upload speed would be six megabyte per
second, megabit per second. And so, that's about six to
eight megabyte per second. 6/8, 6 over 8 megabyte per second. And that if you just calculate
how long it will take you to move this one terabyte, it
will take you about 771 days, which is about two years. So, it's not really a practical solution. So, suppose you have a faster line. In the university here, we have lines of typically a hundred megabit per second and that would reduce the
time significantly to 46 days. 46 days is still kind of a long time. For a dedicated fiber, you
can get pretty good speed. You can get maybe the whole
moving of the data is 11 hours. However, having dedicated
fiber is a big investment. If the type that I'm talking about here, it can cost 10 to $100,000 per year, okay? So, you don't do it for like one terabyte that you want to do. You want to use it if you
constantly need to move terabytes from one point to another. So, it turns out that there is actually a cheap and fast option, and that is to send
the data through FedEx. And unless you think that I'm just giving some kind of funny anecdote,
no, this is actually a service that is provided by AWS
which is called Snowball, in which you get a disk in the mail and you put into it up
to 50 terabyte of data and then you put it back
in the mail and send it. So, here's a situation where the latency, the time that it take to send
something is like 24 hours, but the throughput is massive, right? It basically is 50 terabyte in 24 hours. Even our fiber line could not do that. So, to summarize this part. When providing interactive experience, we care more about
latency than throughput. When processing terabytes of data, we care more about throughput
than about latency. Transmitting a terabyte
through the internet is a slow and expensive thing. And sending it physically
on a truck using FedEx is often much more effective.
--- end {2.1_transcript.txt} ---
--- start{2.2_slides.pdf} ---
2: Storage Latency
CPU
             Storage
         A

C=   *




                       B
Latencies                                           Storage Types
1.   Read A           Latency 1                   • Main Memory (RAM)




                                  Total Latency
2.   Read B           Latency 2
               TIME
3.   C=A*B            Latency 3

4.   Write C          Latency 4
                                                  • Spinning disk

With big data, most of the latency
is memory latency (1,2,4), not
computation (3)                                   • Remote computer
Summary for part 2
• The major source of latency in data analysis is reading and writing to
  storage
• Different types of storage offer different latency, capacity and price.
• Big data analytics revolves around methods for organizing storage and
  computation in ways that maximize speed while minimizing cost.
• Next, storage locality.
3: Caching
Latency, size and price of computer memory
       Given a budget, we need to trade off
   $10: Fast & Small           $10: Slow & Large
Cache: The basic idea
                                   Slow & Large
                          Memory
           Fast & Small   0   1    2   3   4   5   6   7   8   9
               Cache      10 11 12 13 14 15 16 17 18 19
                12 67     20 21 22 23 24 25 26 27 28 29

    cpu         50 51     30 31 32 33 34 35 36 37 38 39
                52 53     40 41 42 43 44 45 46 47 48 49
                32 33     50 51 52 53 54 55 56 57 58 59
                          60 61 62 63 64 65 66 67 68 69
                          70 71 72 73 74 75 76 77 78 79
Cache Hit
                                Memory
                                0   1    2   3   4   5   6   7   8   9
                        Cache   10 11 12 13 14 15 16 17 18 19
            Cache Hit   12 67   20 21 22 23 24 25 26 27 28 29

    cpu                 50 51   30 31 32 33 34 35 36 37 38 39
                        52 53   40 41 42 43 44 45 46 47 48 49
                        32 33   50 51 52 53 54 55 56 57 58 59
                                60 61 62 63 64 65 66 67 68 69
                                70 71 72 73 74 75 76 77 78 79
Cache Miss
                     Memory
                     0   1    2   3   4   5   6   7   8   9
             Cache   10 11 12 13 14 15 16 17 18 19
             12 67   20 21 22 23 24 25 26 27 28 29

    cpu      50 51   30 31 32 33 34 35 36 37 38 39
             52 53   40 41 42 43 44 45 46 47 48 49
             32 33   50 51 52 53 54 55 56 57 58 59
                     60 61 62 63 64 65 66 67 68 69
                     70 71 72 73 74 75 76 77 78 79
Cache Miss Service: 1) Choose byte to drop
                        Memory
                        0   1    2   3   4   5   6   7   8   9
               Cache    10 11 12 13 14 15 16 17 18 19
               12 67    20 21 22 23 24 25 26 27 28 29

    cpu        50 51    30 31 32 33 34 35 36 37 38 39
               52 53    40 41 42 43 44 45 46 47 48 49
               32 33    50 51 52 53 54 55 56 57 58 59
                        60 61 62 63 64 65 66 67 68 69
                        70 71 72 73 74 75 76 77 78 79
Cache Miss Service: 2) write back
                        Memory
                        0   1    2   3   4   5   6   7   8   9
               Cache    10 11 12 13 14 15 16 17 18 19
               12 67    20 21 22 23 24 25 26 27 28 29

    cpu        50 51    30 31 32 33 34 35 36 37 38 39
               52 53    40 41 42 43 44 45 46 47 48 49
               32 33    50 51 52 53 54 55 56 57 58 59
                        60 61 62 63 64 65 66 67 68 69
                        70 71 72 73 74 75 76 77 78 79
Cache Miss Service: 3) Read In
                        Memory
                        0   1    2   3   4   5   6   7   8   9
               Cache    10 11 12 13 14 15 16 17 18 19
               12       20 21 22 23 24 25 26 27 28 29

    cpu        50 51    30 31 32 33 34 35 36 37 38 39
               52 53    40 41 42 43 44 45 46 47 48 49
               32 33    50 51 52 53 54 55 56 57 58 59
                        60 61 62 63 64 65 66 67 68 69
                        70 71 72 73 74 75 76 77 78 79
Summary of part 3
• Cache is much faster than main memory
• Cache hit = the needed value is already in the cache
• Cache miss = the needed value is not in the cache – needs to be
  brought in from memory
• If there is no space in cache:
   • need to make space
   • If dirty, need to write value back to memory first.
• Cache miss – latency is much bigger than cache miss.
4: Locality of storage access
Access Locality
• The cache is effective If most accesses are hits.
   • Cache Hit Rate is high.
• Cache effectiveness depends on patterns (statistics) of memory
  access.
• Temporal Locality: Multiple accesses to same address within a short
  time period
• Spatial Locality: Multiple accesses to near-by addresses within a
  short time period
Temporal Locality
•
Spatial locality
•
Linked List


      Page 1           Page 2        Page3       Page4
  6            3   2            5            4    1




  Traversal of 6 elements touches 4 pages
array


   Page 1           Page 2                   Page3   Page4
                      1      2   3   4   5    6




 Traversal of 6 elements touches 2 pages
Summary of Part 4
• Caching Is effective when memory access is local
• Temporal locality: accessing the same location many times in a short
  period of time.
• Spatial locality: accessing close-by locations many times in a short
  period of time.
• Hardware and compilers have a symbiotic relationship: success if
  compiler generates machine code that has good locality.
Word Count
• Task: given a (large) text
• Count the number of times each word
• Output (word,count) sorted in decreasing order by Count.
Unsorted word count / poor locality


Dict={}               Suppose
For word in list:        len(list)=1,000,000
   if word in Dict:      len(Dict) = 100,000
                      Access to list: spatially local
      Dict[word]+=1
                      Access to Dict: random
   else:
       Dict[word]=1
sorted word count / good locality


                      Suppose
Dict={}
                         len(list)=1,000,000
Sort(list)               len(Dict) = 100,000
For word in list:     Access to list: spatially local
   if word in Dict:   Access to Dict: Spatially local
      Dict[word]+=1
   else:              But what about the sort step?
       Dict[word]=1   Sorting can be done in time O(n)
                      Efficient in distributed setup
Summary
• Improved memory locality reduces run-time
• Why?
  • Because computer memory is organized in pages.
  • And caching retrieves a page at a time.

--- end {2.2_slides.pdf} ---
--- start{2.2_transcript.txt} ---
(bright music) - Okay, so let's now see
how these ideas of latency, throughput and caching can be used in the context of an
actual computer system. So, computer system, as you might know, is made out of a CPU, the
central processing unit, and storage, okay? And the main thing that we
need to know about storage at this point is that it's basically a long sequence of addresses going from maybe zero to 999, okay? So, something like that. And what we want to do is
do a very simple operation which is multiply A and B and put the result in a variable called C. So, A and B are in memory,
and C will be in the memory. Okay. So, here is A, and here is B. And we read A into the memory. We read B from the memory into the CPU. We take the product to calculate C, and then we take C and we write it back into another location in the memory, okay? So, that's the operation. Now let's think about the
latencies that are involved, okay? So, we're going to read A, then read B, then multiply A and B and get the results C,
and then write C, okay? So, these operations happen in order and let's see what are their latencies? So, I just wrote here,
Latency 1, Latency 2, Latency 3, Latency 4, okay? One cannot start before
the previous ended. So, what we find out when we
look at big data especially is that most of the latency is in these reading from the
memory and writing to memory. And this part is relatively small. Okay. So, why does writing and reading
from memory take so long? Well, it's not a uniform thing. Some systems can write
and read very quickly, some of them much more slowly. That's based on essentially
the technology used for the memory, so storage type. So, you have main memory. You have let's say, a spinning disk, and you have maybe a remote computer. So, when you are using Google Drive or something like that, the memory, the disk that you're using is some other place and
that that is going to be the longest latency, okay? So, these go from low latency. So, this is low latency, medium latency and long latency. Throughput is another matter. Okay, so to summarize, the major source of
latency in data analysis is reading and writing to storage. So, you write some big
statistical analysis, most of the time spent is reading data and then writing the results. Different types of storage offer different latencies, capacity, so how much they can store, and price. And big data analytics
revolves around methods for organizing the storage
and the computation in a way that maximizes speed
while minimizes the cost. And when I say here maximizing the speed, I mean throughput, okay? So, how much data can
we process per minute? So, now let's see, in
terms of storage locality, how we can organize the
computer at different levels in order to give you low
latency and high throughput. So, we're going to talk about caching. So, here, we're just looking again, into the latency, size and
price of computer memory. And so, we need to balance these things. So we can buy, let's say for $10, either a fast and small memory or we can buy a slow
but large memory, okay? So, small and fast would be something that would be, for instance,
inside the CPU itself. Inside the CPU, it has some memory sometimes called registers
or register bank, and those are much, much faster than the RAM or the main memory. Okay, and now the question is how do we get to use
the fast and small one most of the time so that it hides the latency of the slow and large one? So, here is the basic idea. What you do is you put in between the CPU and the large and slow memory, you put something that is called a cache. Very similar to the cache that we had with the water bottles being
in the food truck, okay? So, what we have is that there are some memory addresses here, this memory is physically
inside the CPU, for instance, and then what we have is basically copies of elements from the memory, we have them stored close by, okay? So, these are close by and this
is close by and so on, okay? The other elements, they are just not available
in the cache, okay? So, the cache is just a
kind of a little collection, a small collection of
data from the main memory. So, we have that 12 is 12, 67 to 67. Okay, so how does that help us? The CPU is executing a program. The program as you write it is not aware at all of cache
or the memory hierarchy. It's not being made available to you because if it was made available to you, it would make programming so much harder. So, you just think about the big memory when you write the program. And what you hope is, what
you try to design outside is to make sure that most of the time what you need is in the cache, okay? So, that is what is called a cache hit. So, suppose we want to process element 67, then the cache will actually
intercept this request and say, "Oh, I have 67 right here. You don't need to go all
the way to Costco." Okay? So, you can just get this 67 from here and that's going to be much, much faster, maybe a hundred times faster. A cache miss, on the other hand, is where you pay for doing this trick. And that is suppose that
the CPU that your program wants to read location 47 and location 47 is not in the cache, so you have to get it
actually from memory. But wait, this is not
even the first problem. The first problem is to make
some space in the cache, right? So, the cache is usually completely full with things from the past and you need to remove
something in order to make space for the cache, for the new element. So, what you do is you
choose which bite to drop. And let's say that you
choose this bite, okay? You chose 67 to drop, okay? So, if you choose 67, now you're
faced with another problem, which is that maybe the CPU actually change that 67
to, let's say, 71, okay? So, now this is 71, but this is still 67. So before you remove this,
before you empty this place, you have to write back the 71 into the location of
that cell in main memory. And so, this is what is done here, okay? And now you have this
empty space that you made and now you can put this
space into this space 47. And now, 47 can be read by the CPU, okay? So, what you see is that
you have a situation where if you have most of
the time you hit the cache, you can have very low latency, but when you miss the cache, then you have much bigger
latency than you would've had, even if just going to the memory, right? You need to basically potentially
write something to memory and then read something from memory and then you can continue, okay? So, what intuitively you want to happen is you want to somehow try to make sure that most of the time the
cache hits are what happens. Okay, so to summarize this part, cache is much faster than main memory. A cache hit happens when the needed value is already in the cache, and the cache miss, it's
when it's not in the cache. If there is no space in the cache, which is usually the case,
then you need to make space. And if it's dirty, so
meaning that if this location in the cache has been actually changed, then you need to actually write
that part back into memory. So, cache miss, the latency is much bigger than cache miss. Okay, this is a mistake. So, cache miss, the latency
is much bigger than cache hit. All right, so we talked about caching and how caching can potentially help the computer run faster by reducing the access to slow memory. Now we're going to talk about locality, which is the kind of property
that you have in a program that allows caching to work well. So, it's not something
that is a requirement, it's more of a general
heuristic, if you will, that basically tells you,
okay, if a program does this and then it will do this
and it will do this. And so, I can use that pattern to predict which elements will be used very frequently and
keep them in the cache. Okay, so access locality. The cache is effective if
most of the accesses are hit. And we call that cache hit rate is high. So cache hit rate is
something that is used a lot in the lingo for this kind of thing. And the cache effectiveness
depends on patterns or statistics of memory access, right? So, you wanna capture something
that happens in programs in terms of memory access, even if people are not completely aware that that's what they're doing. So, we have two types of access. One is called temporal
locality and that's basically when we say there is one
variable, one location in memory that we're using very, very frequently. Like for instance, if we have a for loop, then the index of the for loop has high temporal locality. You basically access i to increment it like every cycle of the for loop. So, this i should be in cache. Then, the second one is spatial locality, which is multiple accesses
to nearby addresses, not to the same address,
but to address close to it in a short time period. So, let me give you examples. So, here is temporal locality. Suppose the task is you
want to compute the function f theta x on many xs, x1, x2, up to xn. Theta is a parameter vector, okay? S, it's, let's say, the weights
in the neural network, okay? So, those are things that do not change or do not change very quickly. But in this case, they don't change at all because we're talking about using, applying the neural network
rather than learning, okay? So, the parameter theta are needed at each iteration
of the computation. And so, if theta fits in cache, then you can just put
theta inside the cache and every time that you need theta, it will be just there waiting for you. If theta does not fit in cache, then you're going to have
the program all of a sudden, fall off a cliff because what will happen is that every loop, it can't
get to all of theta at once, so it needs to read some
into the cache, use it, then read another part
into the cache and use it. So, it's kind of a critical phenomena, if you want your theta
to fit in the cache, if it's slightly beyond the cache, then you pay a very big price. Okay, so temporal locality
is repeated access to the same memory location. What is spatial locality? The spatial locality
is, here is an example. Suppose that we have,
again, this long sequence of elements, x1, x2 up to xn and we're looking at the square difference between xi and xi plus one, okay? And we wanna sum all of those. You can think about this, about storing the x1 to xn. Before we talked about the parameters, now the data itself, we're storing it. So, if you use a linked list,
then you get poor locality. And if you get an indexed array,
then you get good locality. So, that might be a little
bit much to understand, so let's go and look at
picture explaining that. So, here is linked list. So, what does that mean? It means that the element
one points to element two, element two to element three, element three to element four, four to five and five to six, okay? And what we see is that
these are the pages, these are the units at which
the cache operates, okay? So, the cache operates
by reading or writing a bunch of locations, not
just one location, okay? And so, what do we see? We see that as we go
through this sequence, we're basically touching
On each one of these pages maybe multiple time. So, if we traverse them,
then we hit all four pages. And suppose that our cache
itself can hold just two pages, then we will need to write pages out, read pages in and so on. It will cost us a long time, it will make the latency significant. Here is, on the other hand,
organizing the data in an array. So, in an array, all of the
data is just sequential, one memory location after the other, okay? So, that's the way that it is. There's no pointers. It's just based on calculating the individual location of each element. And so in this case, if
we go through the elements to make our computation, we just need to hit page one and page two. And if we have two pages in cache, that means that we have
one cache miss here, then another cache miss
here, and we're done, okay? So, that would work much faster. And so, that basically
tells you that linked lists for very large amounts of
data is a very bad idea because it basically makes the computer go back and forth and back and forth to find the relevant data. So, to summarize. Caching is effective when
memory access is local. We can have temporal locality, accessing the same location many times in a short period of time. Or we can have spatial locality, accessing close by locations many times in a short period of time, okay? So, we go close by locations
instead of the same location. The hardware and compilers
have a symbiotic relationship in this regard. Basically, the hardware is designed so that the compiler will generate, so that it basically has
the compiler work well, work fast and have low cache miss. And on the other hand, the
writers of the compilers are trying to make themselves work well for the most
popular hardware, okay? So, both of these things are
kind of evolving over time and they have a symbiotic relationship.
--- end {2.2_transcript.txt} ---
--- start{2.3_slides.pdf} ---
5: The memory Hierarchy
The Memory Hierarchy
• Real systems have a several levels storage types:
   • Top of hierarchy: Small and fast storage close to CPU
   • Bottom of Hierarchy: Large and slow storage further from CPU
• Caching is used to transfer data between neighboring levels of the
  hierarchy.
• To the programmer / compiler does not need to know
   • The hardware provides an abstraction : memory looks like like a single large
     array.
• Performance depends on locality of program memory access.
The Memory Hierarchy
                Computer clusters
           extend the memory hierarchy
• A data processing cluster is
  simply many computers linked
  through an ethernet connection.
• Storage is shared
• Locality: Data to reside on the
  computer this will use it.




                                    Et
                                      he
• “Caching” is replaced by




                                        rn
                                          et
  “Shuffling”
• Abstraction is spark RDD.
  Sizes and latencies in a typical memory hierarchy.

               CPU           L1    L2       L3      Main   Disk       Local
               (Registers)   Cache Cache    Cache   Memory Storage    Area
                                                                      Network

Size (bytes)   1KB           64KB   256KB   4MB     4-16GB   4-16TB   16TB –12
                                                                      10PB orders of
                                                                              magnitude

Latency        300ps         1ns    5ns     20ns    100ns    2-10ms   2-10ms6
                                                                              orders of
                                                                              magnitude
Block size     64B           64B    64B     64B     32KB     64KB     1.5-64KB
Summary of part 5
• Memory Hierarchy: combining storage banks with different latencies.
• Clusters: multiple computers, connected by ethernet, that share their
  storage.

--- end {2.3_slides.pdf} ---
--- start{2.3_transcript.txt} ---
(gentle music) - So we now come to combine
the things that we talked about caching and locality and so on and see how it works in the
context of the computer. So the memory hierarchy is such that real systems have several
levels of storage types. At the top of the hierarchy, you have small and fast storage
that is close to the CPU. At the bottom of the hierarchy you have things like disk that are large, but slow and are further from the CPU. Caching is used to transfer data between neighboring
levels of the hierarchy. So to the programmer and the compiler they don't need to know
how this caching works. The whole caching hierarchy
works as an abstraction and that makes the programmer think as if there is just very
large and fast storage. But the performance actually depends on locality of the program's memory access as we talked about them. So here is an example of
computer architecture. Here you have the CPU, and inside the CPU there is some memory, very fast memory for storing
commands instructions here or data. And that is the fastest
part of the whole hierarchy. The next level is an L2 cache that is faster than general memory, but slower than these registers. And then you have the main memory and finally you have disk. Okay? So what you see is as you go
from the fast and very small, 32 to 256 kilobyte, you go through the hierarchy, you get larger and larger storage, but at the price of being
more and more slow, okay? So here it is, nanoseconds, and here it is, milliseconds. At the next level we have multiple computers and they're all connected
through an ethernet cable. And so they form one compute cluster. And what we talk about locality
and caching in this case, when we have the storage, the memory storage, or the disk storage we share it across the computers. And the locality is when the data resides on the computer that needs to process it. So the price is paid when some data that resides
here needs to be processed by some computer that is here, then you need to pass ring
through the ethernet cable, which is relatively slow. And caching, what we talked about before about caching is now replaced with
something called shuffling. And the abstraction that we will get to when we talk about spark is the spark RDD. So the spark RDD will
serve as an abstraction that makes us think that
all of this is one computer and we don't need to worry
about where things are stored. Here's some summary,
probably a little out of date about the different sizes that you can get and their latency from 300 picosecond to 2 to 10 millisecond, and their total size and the block size. So as you remember, for
sequential locality, we use blocks in the cache so
that we can get the advantage of a whole block of data
that we read at once. So we see that in terms of size, there is twelve orders of magnitude from the very small to the very big and six order of magnitude
in terms of speed. Okay, so to summarize this part the memory hierarchy is
combining storage banks with different latencies and clusters are multiple
computers connected by ethernet that share their storage.
--- end {2.3_transcript.txt} ---
--- start{2.4.1_notebook.txt} ---
Memory locality, Rows vs. Columns
The effect of row vs column major layout
The way you traverse a 2D array effects speed.

numpy arrays are, by default, organized in a row-major order.
a=array([range(1,31)]).reshape([3,10])

 1  2  3  4  5  6  7  8  9 10
11 12 13 14 15 16 17 18 19 20
21 22 23 24 25 26 27 28 29 30

a[i,j] and a[i,j+1] are placed in consecutive places in memory.
a[i,j] and a[i+1,j] are 10 memory locations apart.
This implies that scanning the array row by row is more local than scanning column by column.
locality implies speed.
%pylab inline
from time import time

# create an n by n array
n=1000
a=ones([n,n])
Populating the interactive namespace from numpy and matplotlib
%%time
# Scan column by column
s=0;
for i in range(n): s+=sum(a[:,i])
CPU times: user 16 ms, sys: 0 ns, total: 16 ms
Wall time: 14.2 ms
%%time
## Scan row by row
s=0;
for i in range(n): s+=sum(a[i,:])
CPU times: user 12 ms, sys: 4 ms, total: 16 ms
Wall time: 11.2 ms
Some experiments with row vs column scanning
We want to see how the run time of these two code snippets varies as n, the size of the array, is changed.

def sample_run_times(T,k=10):
    """ compare the time to sum an array row by row vs column by column
        T: the sizes of the matrix, [10**e for e in T]
        k: the number of repetitions of each experiment
    """
    all_times=[]
    for e in T:
        n=int(10**e)
        #print('\r',n)
        a=np.ones([n,n])
        times=[]

        for i in range(k):
            t0=time()
            s=0;
            for i in range(n):
                s+=sum(a[:,i])
            t1=time()
            s=0;
            for i in range(n):
                s+=sum(a[i,:])
            t2=time()
            times.append({'row minor':t1-t0,'row major':t2-t1})
        all_times.append({'n':n,'times':times})
    return all_times
#example run
sample_run_times([1,2],k=1)
[{'n': 10,
  'times': [{'row minor': 5.4836273193359375e-05,
    'row major': 3.600120544433594e-05}]},
 {'n': 100,
  'times': [{'row minor': 0.0004298686981201172,
    'row major': 0.0003581047058105469}]}]
Plot the ratio between run times as function of n
Here we have small steps between consecutive values of n and only one measurement for each (k=1)

all_times=sample_run_times(np.arange(1.5,3.01,0.001),k=1)

n_list=[a['n'] for a in all_times]
ratios=[a['times'][0]['row minor']/a['times'][0]['row major'] for a in all_times]

figure(figsize=(15,10))
plot(n_list,ratios)
grid()
xlabel('size of matrix')
ylabel('ratio or running times')
title('time ratio as a function of size of array');

Conclusions
Traversing a numpy array column by column takes more than row by row.
The effect increasese proportionally to the number of elements in the array (square of the number of rows or columns).
Run time has large fluctuations.
See you next time.
Next, we want to quantify the random fluctuations
and see what is their source

k=100
all_times=sample_run_times(np.arange(1,3.001,0.01),k=k)
_n=[]
_row_major_mean=[]
_row_major_std=[]
_row_major_std=[]
_row_minor_mean=[]
_row_minor_std=[]
_row_minor_min=[]
_row_minor_max=[]
_row_major_min=[]
_row_major_max=[]

for times in all_times:
    _n.append(times['n'])
    row_major=[a['row major'] for a in times['times']]
    row_minor=[a['row minor'] for a in times['times']]
    _row_major_mean.append(np.mean(row_major))
    _row_major_std.append(np.std(row_major))
    _row_major_min.append(np.min(row_major))
    _row_major_max.append(np.max(row_major))

    _row_minor_mean.append(np.mean(row_minor))
    _row_minor_std.append(np.std(row_minor))
    _row_minor_min.append(np.min(row_minor))
    _row_minor_max.append(np.max(row_minor))

_row_major_mean=np.array(_row_major_mean)
_row_major_std=np.array(_row_major_std)
_row_minor_mean=np.array(_row_minor_mean)
_row_minor_std=np.array(_row_minor_std)
figure(figsize=(20,13))
plot(_n,_row_major_mean,'o',label='row major mean')
plot(_n,_row_major_mean-_row_major_std,'x',label='row major mean-std')
plot(_n,_row_major_mean+_row_major_std,'x',label='row major mean+std')
plot(_n,_row_major_min,label='row major min among %d'%k)
plot(_n,_row_major_max,label='row major max among %d'%k)
plot(_n,_row_minor_mean,'o',label='row minor mean')
plot(_n,_row_minor_mean-_row_minor_std,'x',label='row minor mean-std')
plot(_n,_row_minor_mean+_row_minor_std,'x',label='row minor mean+std')
plot(_n,_row_minor_min,label='row minor min among %d'%k)
plot(_n,_row_minor_max,label='row minor max among %d'%k)
xlabel('size of matrix')
ylabel('running time')
legend()
grid()

Summary
Scan by column is slower than scan by row and the difference increases with the size.
scan by row increases linearly and has very little random fluctuations.
Scan by column increases linearly with one constant until about n=430 and then increases with a higher constant.
Scan by column has large fluctatuations around the mean.

--- end {2.4.1_notebook.txt} ---
--- start{2.4.2_notebook.txt} ---
measuring memory latency
In this notebook we will investigate the distribution of latency times for different size arrays.

Note: It is impossible to run 10GB files on workbench, so all 10GB experiments are omitted here
Goal 1: Measure the effects of caching in the wild
Goal 2: Undestand how to study long-tail distributions.
Import modules
%pylab inline
from numpy import *
Populating the interactive namespace from numpy and matplotlib
import time
from matplotlib.backends.backend_pdf import PdfPages

from os.path import isfile,isdir
from os import mkdir
import os
Set log directory
This script generates large files. We put these files in a separate directory so it is easier to delete them later.

run this cell only once
## Remember the path for home and  log directories
home_base,=!pwd
log_root=home_base+'/logs'
if not isdir(log_root):
    mkdir(log_root)
from lib.measureRandomAccess import measureRandomAccess
from lib.PlotTime import PlotTime
from lib.create_file import create_file,tee
defining memory latency
Latency is the time difference between the time at which the CPU is issuing a read or write command and, the time the command is complete.

This time is very short if the element is already in the L1 Cache,
and is very long if the element is in external memory (disk or SSD).
setting parameters
We test access to elements arrays whose sizes are:
m_legend=['Zero','1KB','1MB','1GB','10GB']
Arrays are stored in memory or on disk
We perform 100,000 read/write ops to random locations in the array.
We analyze the distribution of the latencies as a function of the size of the array.
# include 10GB here
# m_list=[0]+[int(10**i) for i in [3,6,9,10]]
# m_legend=['Zero','1KB','1MB','1GB','10GB']

m_list=[0]+[int(10**i) for i in [3,6,9]]
m_legend=['Zero','1KB','1MB','1GB']
L=len(m_list)
k=100000 # number of pokes
print('m_list=',m_list)
m_list= [0, 1000, 1000000, 1000000000]
Set working directory
This script generates large files. We put these files in a separate directory so it is easier to delete them later.

TimeStamp=str(int(time.time()))
log_dir=log_root+'/'+TimeStamp
mkdir(log_dir)
%cd $log_dir
stat=open('stats.txt','w')

def tee(line):
    print(line)
    stat.write(line+'\n')
/home/ccc_v1_t_TS0f_161889/work/asn88885_10/asn88886_1/work/Section1-Spark-Basics/0.MemoryLatency/logs/1569977822
_mean=zeros([2,L])   #0: using disk, 1: using memory
_std=zeros([2,L])
_block_no=zeros([L])
_block_size=zeros([L])
T=zeros([2,L,k])
# %load /Users/yoavfreund/academic.papers/Courses/BigDataAnalytics/BigData_spring2016/CSE255-DSE230-2018/Sections/Section1-Spark-Basics/0.MemoryLatency/lib/create_file.py
import time

stat=open('stats.txt','w')

def tee(line):
    print(line)
    stat.write(line+'\n')
    
def create_file(n,m,filename='DataBlock'):
    """Create a scratch file of a given size

    :param n: size of block
    :param m: number of blocks
    :param filename: desired filename
    :returns: time to allocate block of size n, time to write a file of size m*n
    :rtype: tuple

    """
    t1=time.time()
    A=bytearray(n)
    t2=time.time()
    file=open(filename,'wb')
    for i in range(m):
        file.write(A)
        if i % 100 == 0:
            print('\r',i,",", end=' ')
    file.close()
    t3=time.time()
    tee('\r              \ncreating %d byte block: %f sec, writing %d blocks %f sec' % (n,t2-t1,m,t3-t2))
    return (t2-t1,t3-t2)
Random_pokes=[]
Min_Block_size=1000000
for m_i in range(len(m_list)):
    
    m=m_list[m_i]
    blocks=int(m/Min_Block_size)
    if blocks==0:
        _block_size[m_i]=1
        _block_no[m_i]=m
    else:
        _block_size[m_i]=Min_Block_size
        _block_no[m_i]=blocks
    (t_mem,t_disk) = create_file(int(_block_size[m_i]),int(_block_no[m_i]),filename='BlockData'+str(m))

    (_mean[0,m_i],_std[0,m_i],T[0,m_i]) = measureRandomAccess(m,filename='BlockData'+str(m),k=k)
    T[0,m_i]=sorted(T[0,m_i])
    tee('\rFile pokes _mean='+str(_mean[0,m_i])+', file _std='+str(_std[0,m_i]))

    (_mean[1,m_i],_std[1,m_i],T[1,m_i]) = measureRandomAccess(m,filename='',k=k)
    T[1,m_i]=sorted(T[1,m_i])
    tee('\rMemory pokes _mean='+str(_mean[1,m_i])+', Memory _std='+str(_std[1,m_i]))
    
    Random_pokes.append({'m_i':m_i,
                        'm':m,
                        'memory__mean': _mean[1,m_i],
                        'memory__std': _std[1,m_i],
                        'memory_largest': T[1,m_i][-1000:],
                        'file__mean': _mean[0,m_i],
                        'file__std': _std[0,m_i],
                        'file_largest': T[0,m_i][-1000:]                
                })
print('='*50)
              
creating 1 byte block: 0.000001 sec, writing 0 blocks 0.001522 sec
File pokes _mean=1.4178752899169922e-07, file _std=1.735372681852446e-07
Memory pokes _mean=1.3778924942016603e-07, Memory _std=2.0693948659186357e-07
              
creating 1 byte block: 0.000002 sec, writing 1000 blocks 0.007607 sec
File pokes _mean=1.167142391204834e-05, file _std=4.742214808370476e-06
Memory pokes _mean=1.736617088317871e-07, Memory _std=1.577743855945899e-07
              
creating 1000000 byte block: 0.000071 sec, writing 1 blocks 0.022258 sec
File pokes _mean=1.3782503604888917e-05, file _std=7.029326266297049e-06
Memory pokes _mean=2.594304084777832e-07, Memory _std=1.7600767824608167e-07
              
creating 1000000 byte block: 0.000046 sec, writing 1000 blocks 13.825604 sec
File pokes _mean=1.5203940868377686e-05, file _std=7.625527918272001e-06
Memory pokes _mean=3.649687767028809e-07, Memory _std=2.1319633784894616e-07
==================================================
fields=['m', 'memory__mean', 'memory__std','file__mean','file__std']
print('| block size | mem mean  | mem std | disk mean | disk std |')
print('| :--------- | :----------- | :--- | :-------- | :------- |')
for R in Random_pokes:
    tup=tuple([R[f] for f in fields])
    print('| %d | %6.3g | %6.3g |  %6.3g | %6.3g |'%tup)
| block size | mem mean  | mem std | disk mean | disk std |
| :--------- | :----------- | :--- | :-------- | :------- |
| 0 | 1.38e-07 | 2.07e-07 |  1.42e-07 | 1.74e-07 |
| 1000 | 1.74e-07 | 1.58e-07 |  1.17e-05 | 4.74e-06 |
| 1000000 | 2.59e-07 | 1.76e-07 |  1.38e-05 | 7.03e-06 |
| 1000000000 | 3.65e-07 | 2.13e-07 |  1.52e-05 | 7.63e-06 |
Mean and std of latency for random access

Note that zero is not really zero.
system time is accurate to micro-second, not nano-second.
SSD random access latency is 
Digging deeper
The mean and std are the first statistics to look at.
but the distribution might have more to tell us.
First, lets try histograms
m_i=3
Disk_Mem=1
print('Disk Block of size %2.1g GB'%(m_list[m_i]/1e9))
print('\r latency mean='+str(_mean[1,m_i])+',  std='+str(_std[1,m_i]))

_mean_t=_mean[Disk_Mem,m_i]
_std_t=_std[Disk_Mem,m_i]
_normal=random.normal(loc=_mean_t,scale=_std_t,size=T.shape[2])
tmp=T[Disk_Mem,m_i]
print(' Fraction of zeros=',sum(tmp==0)/len(tmp))
figure(figsize=(10,4))
subplot(121)
thr=1e-6
hist([tmp[tmp<thr],_normal[_normal<thr]],bins=20);
#ylim([0,20000])
xlim([-thr/10,thr])
title('time < %3.2g ms'%(thr*1e3))
xticks(rotation=45)
grid()
subplot(122)
hist([tmp[tmp>=thr],_normal[_normal>=thr]],bins=40);
xlim([thr,thr*10])
#ylim([0,20])
title('time >= %3.2g ms'%(thr*1e3))
xticks(rotation=45);
grid();
Disk Block of size  1 GB

 latency mean=3.649687767028809e-07,  std=2.1319633784894616e-07
 Fraction of zeros= 0.00252

CDF instead of histogram
Choosing ranges and bin-numbers for histograms can be hard.

$$CDF(a) = Pr(T \leq a) \hdots (T=\text{latency})$$

Plotting a CDF does not require choosing bins.
We are interested in larger latencies, so we use instead

$$1-CDF(a) = 1 - Pr(T \leq a ) = Pr(T > a)$$

figure(figsize=(14,7))
subplot(121)
grid()
PlotTime(sort(_normal),_mean_t,_std_t,Color=['y','b','k','r'],LS=['-','-','-','--'],LogLog=False)
title('normal distribution')
xlabel('delay (sec)',fontsize=18)
xlim([-thr/10,thr])
ylabel('1-CDF',fontsize=18)
tick_params(axis='both', which='major', labelsize=16,rotation=45)
tick_params(axis='both', which='minor', labelsize=12,rotation=45)

#print('%d Memory Blocks of size %d bytes'%(m_list[m_i],n))
#print('\rMemory pokes _mean='+str(_mean[1,m_i])+', Memory _std='+str(_std[1,m_i]))
subplot(122)
grid()
PlotTime(sort(tmp),_mean_t,_std_t,Color=['y','b','k','r'],LS=['-','-','-','--'],LogLog=False)
title('distribution of 1GB memory access latencies')
xlabel('delay (sec)',fontsize=18)
xlim([-thr/10,thr])
#ylim([0,0.001])
#ylabel('1-CDF',fontsize=18)
tick_params(axis='both', which='major', labelsize=16,rotation=45)
tick_params(axis='both', which='minor', labelsize=12,rotation=45)

CDF + loglog plots
figure(figsize=(12,6))
subplot(121)
grid()
PlotTime(sort(_normal),_mean_t,_std_t,Color=['y','b','k','r'],LS=['-','-','-','--'])
title('normal distribution / loglog scaling')
xlabel('delay (sec)',fontsize=18)
xlim([1e-7,1000*thr])
ylabel('1-CDF',fontsize=18)
tick_params(axis='both', which='major', labelsize=16)
tick_params(axis='both', which='minor', labelsize=12)

#print('%d Memory Blocks of size %d bytes'%(m_list[m_i],n))
#print('\rMemory pokes _mean='+str(_mean[1,m_i])+', Memory _std='+str(_std[1,m_i]))
subplot(122)
grid()
PlotTime(sort(tmp),_mean_t,_std_t,Color=['y','b','k','r'],LS=['-','-','-','--'])
title('distribution of mem access delays / loglog scaling')
xlabel('delay (sec)',fontsize=18)
xlim([1e-7,1000*thr])
#ylabel('1-CDF',fontsize=18)
tick_params(axis='both', which='major', labelsize=16)
tick_params(axis='both', which='minor', labelsize=12)

Characterize random access to storage
pp = PdfPages('MemoryFigure.pdf')
figure(figsize=(6,4))

Colors='bgrcmyk'  # The colors for the plot
LineStyles=['-',':']
Legends=['F','M']

fig = matplotlib.pyplot.gcf()
fig.set_size_inches(18.5,10.5)

for m_i in range(len(m_list)):
    Color=Colors[m_i % len(Colors)]
    for Type in [0,1]:
        
        PlotTime(sort(T[Type,m_i]),_mean[Type,m_i],_std[Type,m_i],\
             Color=Color,LS=LineStyles[Type],Legend=m_legend[m_i]+' '+Legends[Type],\
             m_i=m_i)

grid()
legend(fontsize=18)
xlabel('delay (sec)',fontsize=18)
ylabel('1-CDF',fontsize=18)
tick_params(axis='both', which='major', labelsize=16)
tick_params(axis='both', which='minor', labelsize=12)
pp.savefig()
pp.close()

Characterize sequential access
Random access degrades rapidly with the size of the block.
Sequential access is much faster.
We already saw that writing 10GB to disk sequentially takes 8.9sec, or less than 1 second for a gigbyte.
Writing a 1TB disk at this rate would take ~1000 seconds or about 16 minutes.
import time
Consec=[]
Line='### Consecutive Memory writes:'
print(Line); stat.write(Line+'\n')
n=1000
r=np.array(list(range(n)))
Header="""
|   size (MB) | Average time per byte |
| ---------: | --------------: | """
tee(Header)
# for m in [1,1000,1000000,10000000]: # include 10GB experiment
for m in [1,1000,1000000]:
    t1=time.time()
    A=np.repeat(r,m)
    t2=time.time()
    Consec.append((n,m,float(n*m)/1000000,(t2-t1)/float(n*m)))
    tee("| %6.3f | %4.2g |" % (float(n*m)/1000000,(t2-t1)/float(n*m)))
A=[];r=[]
stat.close()
### Consecutive Memory writes:

|   size (MB) | Average time per byte |
| ---------: | --------------: | 
|  0.001 | 0.00036 |
|  1.000 | 6.3e-09 |
| 1000.000 | 6.9e-09 |

We are measuring bandwidth rather than latency:
We say that it take 8.9sec to write 10GB to SSD, we are NOT saying that to write one byte to SSD it take $8.9 \text{x} 10^{-10}$ second to write a single byte.
This is because many write operations are occuring in parallel.
Byte-rate for writing large blocks is about (100MB/sec)
Byte-rate for writing large SSD blocks is about (1GB/sec)
Comparison:
Memory: Sequential access: 100M/sec, random access: $10^{-9}$ sec for 10KB,$10^{-6}$ to $10^{-3}$ for 10GB
SSD: Sequential access: $10^{-5}$ to $10^{-3}$ sec for 10KB, $10^{-4}$ to $10^{-1}$ for 10GB

Collecting System Description
In this section you will find commands that list properties of the hardware on which this notebook is running.

Specify which OS you are using
Uncomment the line corresponding to your OS. Comment all of the rest.

# brand_name = "brand: Macbook"
brand_name = "brand: Linux"
#brand_name = "brand: Windows"
For Mac users
The next cell needs to be run only by Mac OS users. If run on other OS platforms, it will throw error.

if brand_name== "brand: Macbook":
    # To get all available information use !sysctl -a
    os_info = !sysctl kernel.osrelease kernel.osrevision kernel.ostype kernel.osversion
    cpu_info = !sysctl machdep.cpu.brand_string machdep.cpu.cache.L2_associativity machdep.cpu.cache.linesize machdep.cpu.cache.size machdep.cpu.core_count
    cache_info = !sysctl kern.procname hw.memsize hw.cpufamily hw.activecpu hw.cachelinesize hw.cpufrequency hw.l1dcachesize hw.l1icachesize hw.l2cachesize hw.l3cachesize hw.cputype 
For Linux OS users
if brand_name == "brand: Linux":
    os_info = !sysctl kernel.ostype kernel.osrelease 
    os_version = !lsb_release -r
    memory_size = !cat /proc/meminfo | grep 'MemTotal'
    os_info += os_version + memory_size

    cache_L1i = !lscpu | grep 'L1i'
    cache_L1d = !lscpu | grep 'L1d'
    cache_L2 = !lscpu | grep 'L2'
    cache_L3 = !lscpu | grep 'L3'
    cache_info = cache_L1i + cache_L1d + cache_L2 + cache_L3

    cpu_type = !lscpu | grep 'CPU family'
    cpu_brand = !cat /proc/cpuinfo | grep -m 1 'model name'
    cpu_frequency = !lscpu | grep 'CPU MHz'
    cpu_core_count = !lscpu | grep 'CPU(s)'
    cpu_info = cpu_type + cpu_brand + cpu_frequency + cpu_core_count
For Windows users
if brand_name =="brand: Windows":
    os_release  = !ver
    os_type     = !WMIC CPU get  SystemCreationClassName
    memory      = !WMIC ComputerSystem get TotalPhysicalMemory
    os_info     = os_release + os_type

    cpu_core_count  = !WMIC CPU get NumberOfCores
    cpu_speed       = !WMIC CPU get CurrentClockSpeed
    cpu_model_name  = !WMIC CPU get name
    cpu_info        = cpu_core_count + cpu_speed + cpu_model_name

    l2cachesize = !WMIC CPU get L2CacheSize
    l3cachesize = !WMIC CPU get L3CacheSize
    cache_info  = l2cachesize + l3cachesize
# Print collected information
description=[brand_name] + os_info + cache_info + cpu_info
print("Main Harware Parameters:\n")
print('\n'.join(description))
Main Harware Parameters:

brand: Linux
kernel.ostype = Linux
kernel.osrelease = 4.4.0-139-generic
/bin/bash: lsb_release: command not found
MemTotal:       32127736 kB
L1i cache:             32K
L1d cache:             32K
L2 cache:              1024K
L3 cache:              33792K
CPU family:            6
model name	: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz
CPU MHz:               2500.000
CPU(s):                8
On-line CPU(s) list:   0-7
NUMA node0 CPU(s):     0-7
Summary of Macbook Pro hardware parameters
Intel four cores
Clock Rate: 2.50GHz (0.4ns per clock cycle)

# Writing all necesarry information int oa pickle file.
import pickle
with open(home_base+'/memory_report.pkl','wb') as pickle_file:
    pickle.dump({'description':description,
                'Consec':Consec,
                'Random_pokes':Random_pokes},
               pickle_file)
Observations
making measurements in the wild allows you to measure the performance of your hardware with your software.
Measuring in the wild you discover unexpected glitches:
timer resolution is $1\mu$s
once every ~10,000 of a zero-time poke there is a $10^{-5}$ delay. Maybe a context switch?
Latencies typically have long tails - Use loglog graphs.
Memory latency varies from $10^{-9}$ sec to $10^{-6}$ sec depending on access pattern.
SSD latency for random access varies from $10^{-5}$ sec to $10^{-1}$ sec.

When reading or writing large blocks, we care about throughput or byte-rate not latency
Typical throughputs: Memory: 100MB/sec SSD: 1GB/sec Disk: (fill in)

Impact on Big Data Analytics
Clock rate is stuck at around 3GHz, and is likely to be stuck there for the forseeable future.
Faster computers / disks / networks are expensive
focus on data access: ** The main bottleneck on big data computation is moving data around, **NOT calculation.
The cost-effective solution is often a cluster of many cheap computers, each with many cores and break up the data so that each computer has a small fraction of the data.
Data-Centers and the "Cloud"
I invite you to use this notebook on your computer to get a better understanding of its memory access latency.
If you are interest in way to make more accurate measurements of latency, try notebook 3.
See you next time.

--- end {2.4.2_notebook.txt} ---
--- start{2.4.3_notebook.txt} ---
measuring memory latency
The purpose of this notebook is to overcome a problem int the notebook 2_measuring_performance_of_memory_hierarchy.ipynb.

The problem is that the time() function is only accurate up to $10^{-7}$ of a second. So any operations that take a shorter time do not register as taking any time.

To overcome the problem we perform many random pokes in sequence and measure the time it takes to complete all of the pokes.

As we ware interested in times shorter than $10^{-7}$ we restrict our attention to the main memory, rather than to files.

Import modules
%pylab inline
from numpy import *
Populating the interactive namespace from numpy and matplotlib
import time
from matplotlib.backends.backend_pdf import PdfPages

from os.path import isfile,isdir
from os import mkdir
import os
from lib.measureRandomAccess import measureRandomAccess
from lib.PlotTime import PlotTime
setting parameters
We test access to elements arrays whose sizes are:
1MB, 10MB, 100MB, 1000MB (=1GB)
Arrays are stored in memory or on disk on disk
We perform 1 million read/write ops to random locations in the array.
We analyze the distribution of the latencies.
n=100 # size of single block (1MB)
m_list=[1,10,100,1000,10000] # size of file in blocks
k=100000;  # number of repeats
L=len(m_list)
print('n=%d, k=%d, m_list='%(n,k),m_list)
n=100, k=100000, m_list= [1, 10, 100, 1000, 10000]
Set working directory
This script generates large files. We put these files in a separate directory so it is easier to delete them later.

log_root='./logs'
if not isdir(log_root): mkdir(log_root)
TimeStamp=str(int(time.time()))
log_dir=log_root+'/'+TimeStamp
mkdir(log_dir)
%cd $log_dir
stat=open('stats.txt','w')

def tee(line):
    print(line)
    stat.write(line+'\n')
/home/ccc_v1_t_TS0f_161889/work/asn88885_10/asn88886_1/work/Section1-Spark-Basics/0.MemoryLatency/logs/1569977826
_mean=zeros([2,L])   #0: using disk, 1: using memory
_std=zeros([2,L])
Tmem=[]
TFile=[]
import numpy as np
from numpy.random import rand
import time

def measureRandomAccessMemBlocks(sz,k=1000,batch=100):
    """Measure the distribution of random accesses in computer memory.

    :param sz: size of memory block.
    :param k: number of times that the experiment is repeated.
    :param batch: The number of locations poked in a single experiment (multiple pokes performed using numpy, rather than python loop)
    :returns: (_mean,std,T):
              _mean = the mean of T
              _std = the std of T
              T = a list the contains the times of all k experiments
    :rtype: tuple

    """
    # Prepare buffer.
    A=np.zeros(sz,dtype=np.int8)

    # Read and write k*batch times from/to buffer.
    sum=0; sum2=0
    T=np.zeros(k)
    for i in range(k):
        if (i%100==0): print('\r',i, end=' ')
        loc=np.int32(rand(batch)*sz)
        t=time.time()
        x=A[loc]
        A[loc]=loc
        d=(time.time()-t)/batch
        T[i]=d
        sum += d
        sum2 += d*d
    _mean=sum/k; var=(sum2/k)-_mean**2; _std=np.sqrt(var)
    return (_mean,_std,T)
m_list=[10,1000,10000,100000,1000000,10000000,100000000,1000000000]
m_legend=['10B','1KB','10KB','100KB','1MB','10MB','100MB','1GB']
Random_pokes=[]

L=len(m_list)
_mean=zeros([L])   #0: using disk, 1: using memory
_std=zeros([L])
TMem=[0]*L

for m_i in range(L):
    m=m_list[m_i]
    print('Memory array %d Bytes'%m)
    out = measureRandomAccessMemBlocks(m,k=1000,batch=1000)
    (_mean[m_i],_std[m_i],TMem[m_i]) = out
    TMem[m_i].sort()
    tee('\rMemory pokes _mean='+str(_mean[m_i])+', Memory _std='+str(_std[m_i]))

    Random_pokes.append({'m_i':m_i,
                        'm':m,
                        'memory__mean': _mean[m_i],
                        'memory__std': _std[m_i],
                        'memory_largest': TMem[m_i][-100:],
                })
Memory array 10 Bytes
Memory pokes _mean=2.2172689437866045e-08, Memory _std=1.2749012371199187e-07
Memory array 1000 Bytes
Memory pokes _mean=1.76773071289062e-08, Memory _std=1.4492373426138178e-08
Memory array 10000 Bytes
Memory pokes _mean=2.1302700042724702e-08, Memory _std=2.3344059100144298e-08
Memory array 100000 Bytes
Memory pokes _mean=1.4214038848876832e-08, Memory _std=2.143190224073267e-08
Memory array 1000000 Bytes
Memory pokes _mean=1.654481887817381e-08, Memory _std=3.1749179760893e-08
Memory array 10000000 Bytes
Memory pokes _mean=3.1414508819579996e-08, Memory _std=1.0611376225861106e-07
Memory array 100000000 Bytes
Memory pokes _mean=1.2456393241882282e-07, Memory _std=3.8794427160679063e-07
Memory array 1000000000 Bytes
 0
Characterize random access to storage
pp = PdfPages('MemoryBlockFigure.pdf')
figure(figsize=(6,4))

Colors='bgrcmyk'  # The colors for the plot
LineStyles=['-']

fig = matplotlib.pyplot.gcf()
fig.set_size_inches(18.5,10.5)

for m_i in range(len(m_list)):
    Color=Colors[m_i % len(Colors)]
    PlotTime(TMem[m_i],_mean[m_i],_std[m_i],\
             Color=Color,LS='-',Legend=m_legend[m_i],\
             m_i=m_i)

grid()
legend(fontsize=18)
xlabel('delay (sec)',fontsize=18)
ylabel('1-CDF',fontsize=18)
tick_params(axis='both', which='major', labelsize=16)
tick_params(axis='both', which='minor', labelsize=12)
pp.savefig()
pp.close()
Conclusions
We see that for this laptop (an apple powerbook) the latency of random pokes is close to $10^{-8}$ for blocks of size up to 1 MB. Beyond that, for sizes of 10MB, 100MB and 1GB, the delay is significantly larger.

This makes sense because the size of the L3 cache in this machine is about 6MB.

--- end {2.4.3_notebook.txt} ---
--- start{2.4_slides.pdf} ---
6: Heavy Tail Distributions
       Caching effect on sequential computation

    Cache hit                   •   Cache hit: 1ns
                                •   Cache miss 100ns
                                •   Cache miss occurs 1% of the time
  Cache miss                    •   Sequential execution


Sequential execution


                         Time
    Caching effect on parallel computation
Parallel execution

      Time
Distribution of access latencies
•
Heavy tails are hard to visualize
• Both the normal density and the probability look very close to zero for latency=100
• Problem 1:
    • The distribution of the latencies is a point-mass-function (PMF)
    • The distribution defined by the normal is a density (PDF)
    • The two are incomparable.
Using log scale to compare small probabilities
•




                                                 Heavy tail

                                      Light/exponential tail
Summary: heavy tail distribution
•
A heavy tails example
• Example: A program’s run time is
   • 1 second with probability 99.9%
   • 300 sec (5 minutes) with probability 0.1%
   • Mean=1.3sec, std=9.5sec
• You run the program in parallel on 1000 dataset, 1000 computers
• You wait until all of the runs finish.
• The laggard problem: the slowest run determines the overall running
  time. More than half the time we need to wait 300 seconds.
The latency of the task is not normal
•
The reason:
• With probability 99.9%, the run finishes in 0.5 second
• With probability 0.1%, the run finish in 1000 seconds
• When you have 1000 computers, there is a large probability that one
  of the machines will take 1000 seconds. Causing the fast machines to
  waste 999.5 seconds.
What about just one machine?
• Using just one machine.
• Run the jobs sequentially
• We are interested in the sum, not the max




• Very rarely more than 6500 sec
• Mean is 1500 Sec (it was about 1000 for 1000 machines!)
• The distribution has exponential (= light tails)
Problem in HW
• Estimate the fraction of 1’s in a large RDD
• Distribution of a single sample: 1 with prob p, 0 with probability 1-p
• Distribution of number of 1’s: binomial distribution
• Binomial approaches normal for number of samples ~>100
• We can bound the probability of a the tail using the normal
  distribution.

--- end {2.4_slides.pdf} ---
--- start{2.4_transcript.txt} ---
(upbeat music) (air whooshing) - We're now going to talk
about heavy tail distributions. Heavy tail distributions
are not usually something that you learn when you learn basic probability and statistics, but in the context of computers,
they're very important. So let's think about the caching effect on sequential computation. So let's say that these markers
here are when we have a hit. So it's a very short
response, like one nanosecond. And here we have a cache miss. This is time. And here we have a cache miss, and that takes 100 nanoseconds, so much, much longer
than the cache misses. And luckily cache misses
occur only 1% of the time. Okay, so 1% of the time we
have to pay 100 nanosecond, but most of the time
we need one nanosecond, 100 nanosecond versus one nanosecond. So here is the time, again,
that we have running this way. And what we have is we
have many, many hits, many hits, hits, hits, hits,
hits, and then we have a miss, and then we have another bunch of hits and then a miss, and continuing with hits. So we have just two
misses in this sequence and it definitely affects
the amount of time that it takes us to execute, but it is balanced well with
the rest of the computation. So the expected time per access
is two nanosecond, right? So one nanosecond in one case, 100 nanosecond in the other case. If you average them out,
you get two nanosecond. And the time to complete the whole process is about 2 times n, n is the number of accesses that we have, plus minus 6 times square
root nanosecond times sigma. Okay, so this is just
a regular calculation of the mean and standard
deviation of the sequence. And so we have that the standard
deviation is 10 nanosecond. So relative to the whole
length of what we have, it is not very big. So if n is 1 million, then what we get is a total
time is about 2 million for the average 2 nanosecond, and then plus 60,000, plus minus 60,000, which
is one standard deviation. So 60,000 relative to 2
million is very small, and we are pretty happy
with what we have working. On the other hand, suppose that we have parallel computation. So we have our many
computers and we're trying to essentially do the
same kind of computation, but now distributed across the computers. Okay? What you immediately see
is that, in this case, when you have a miss,
it creates a big effect on the parallel processing, right? Because most of the time, we
can finish what we are doing in let's say 10 nanosecond but sometimes it takes us
more than 100 nanosecond. And the problem is that we have to wait for everybody to finish before we can say that
the processing is done. So expected time per
access is 2 nanosecond. The time to complete n accesses is 2n plus 6 square root of n time sigma. Suppose that we have a 100 CPUs, we then have about n equal 1000 per core. And so what we get is 1000 plus minus 380. Okay, so 380 is now much
bigger relative to the 1000. But is this really right? Is this the right computation? It still seems reasonably short. You know, we can tolerate that. But it isn't right actually, because the computation that we did here, this computation depends on the sum having a normal distribution, or approximately a normal
distribution, but it doesn't. It has one element that
has probability 99% that has length 1, and then
1% that have length 100. So it's not really a
very good approximation by the normal distribution. All right, so in this case, the normal distribution is
just not a good approximation. And this is a situation where we call it, distribution where the extreme
values are much more likely than the normal, are called
heavy tail distribution. So the tail of the distribution, if we think about it as here is one part and here is another part, so maybe the normal approximation would be something like this. But it's a very bad approximation because we have an element
that is, let's say, at 100 that has some significant probability. So how can we see this long
tail distribution in practice? So suppose that we have
this setting as we were, that we have 1 nanosecond
occurs 99% of the time and 100 nanosecond 1% of the time, the expected latency is indeed
1.99 or about 2 nanosecond and the standard deviation is 10. However, if you now draw this
1 nanosecond here for the mean plus minus 10, for the standard deviation of the normal distribution, you see that the probability of the 100 is much, much smaller than 1%. So this is kind of hard
to see in this plot because this distribution
that we're trying to model, here, this distribution that
is like 99% here and 1% here, is a point-mass distribution. It's not a density distribution, so it's hard to compare. So instead, what we want to do is we want to compare the
cumulative distribution function. Okay, so here we have 1 minus the cumulative
distribution function, and we have the 1 minus for
the normal distribution, and then we have the one for the two-point mass distribution. Okay, so at least now we are
working in the same space. We're talking about probabilities. Okay. But it's still hard to
see the difference, right? So are these two really
different from each other, right? It's not clear from this plot. So in order to see that, we need to use log of the probabilities. So we need to basically
be much more sensitive when we're close to zero, and this is how we do that. Okay, so now we have 1 minus
CDF using the semi-log plot. Okay, so the probability is logarithmic, and the latency is, again, just linear. And what we have is,
we have the point here and the point here for the
point-mass distribution, and then we have this for
the normal distribution. And now it is very, very clear
that the probability of this, which is about 1 out of 100, is much, much bigger than
the probability of the tail of the normal distribution, which is about 10 to the minus
20 or something like that. Right? It goes way, way below. So now we can see that there is really
a very big difference. And this point here, there's just 1%, we get 100 nanosecond is an outlier relative to the normal distribution. So this is the light exponential tail. So this is what we call the light tailed. And this is the heavy tailed. So to summarize, a distribution has heavy tails if the probability of outliers is much, much higher than
the probability that you get if you assume the normal distribution. So you can calculate the mean
and the standard deviation. It's just that the standard
deviation doesn't give you a lot of information about
what is the distribution. Even if the distribution is heavy tailed, you can't use the estimate
that says the value is about the expected value plus minus k times the standard deviation, which is a very common estimate to use. And heavy tail distributions
are very common in the memory hierarchy because most of the time you get hits and then some of the times you get misses and some smaller fraction
of the time you get a miss followed by a miss, followed by a miss, so several levels of the hierarchy. And because a miss is a rare
but expensive operation, you get this problem. Okay, so I'll see you
in the next video. Bye.
--- end {2.4_transcript.txt} ---
--- start{week_01_guide.pdf} ---
      DSC 232R: Big Data Analytics Using Spark
                    Winter 2026
                     Week 1


                               January 8, 2026


1       Topic: Introduction to Big Data Using Spark
1.1     What is Data Science
1.1.1    Lecture Content
    • Definition: Data Science is defined as rational decision making using
      data to make decisions that are useful and profitable. It combines ”Data”
      (collection from sensors, logs, etc.) and ”Science” (hypothesis testing and
      verification).
    • Types of Decisions:
         – Big Decisions: Strategic, high-stakes decisions involving delibera-
           tion by people.
             ∗ Examples: Is city water safe?, Federal interest rate hikes, Infras-
               tructure changes (adding lanes).
         – Small Decisions: Tactical, automated, high-volume decisions made
           without human intervention.
             ∗ Examples: Ad selection (Google), Movie recommendations (Net-
               flix), Loan approvals, Ramp metering (traffic lights on highway
               on-ramps).
    • Ingredients of Data Science:

         1. Math: Linear Algebra, Probability, Statistics.
         2. Machine Learning: Algorithms to build flexible models.
         3. Software Development: Implementing at scale.
         4. Domain Knowledge: Understanding the specific science of the
            problem (e.g., Traffic Flow theory).



                                        1
   • Case Study: Caltrans PeMS (Performance Measurement Sys-
     tem):
        – Data Collection: 45,000 magnetic loop detectors in CA highways.
        – Measurements:
              ∗ Flow: Number of cars per unit time.
              ∗ Occupancy: Fraction of time a car is over the loop.
        – Traffic Theory (The Fundamental Diagram): Relationship be-
          tween Density (cars/mile) and Flow.
              ∗ Low Density → High Speed, increasing Flow.
              ∗ Peak Flow → Optimal capacity.
              ∗ High Density (Traffic Jam) → Low Speed, decreasing Flow.
        – Analysis Techniques: Uses PCA (Principal Component Analysis)
          to identify traffic profiles (e.g., AM vs PM peaks).

1.2     Data Engineering and Data Science
1.2.1   Lecture Content
   • Roles:
        – Data Scientist: Builds models, answers business questions, uses
          Stats/ML. Expects data availability and fast computation.
        – Data Engineer: Builds the infrastructure (databases, streaming,
          cloud, pipelines) that supports the data scientist.
   • Course Scope (Data Engineering Topics):
        – Covered: Hadoop File System (HDFS), Data Partitioning, Caching/Per-
          sistence, Checkpointing.
        – Not Covered: Data Cleaning, Spark Server Optimization, Con-
          tainerization.
   • Data Models (The Interface):
        1. Matrix (Linear Algebra):
            – Rectangle of numbers (all same type).
            – Supports transposition, addition, multiplication.
            – Limitation: Typically must fit in the memory of one computer.
        2. Relation/Table (Relational Databases):
            – Rows = Tuples (Entities), Columns = Properties (Attributes).
            – Columns have types, but types can differ across columns.
            – Scale: Can span many disks/computers; uses indices for fast
              retrieval without loading everything into memory.


                                       2
        3. DataFrame (The Hybrid):
            – Blend of Matrix and Table concepts (popularized by R/S/Pan-
              das).
            – Ordered, named rows and columns.
            – Spark DataFrames: Designed to reside on disk and support
              distributed processing (unlike standard in-memory matrices).

1.3     Speeding Up Data Processing
1.3.1   Lecture Content
   • Two Primary Optimization Methods:
        1. Fast Libraries (Vectorization): Replacing explicit Python loops
           with calls to optimized libraries (e.g., NumPy).
        2. Throughput vs.      Latency: Optimizing for volume rather than
           individual speed.
   • Throughput vs. Latency Definitions:
        – Latency: Time to process a single item from start to finish.
            ∗ Analogy: Time spent waiting in a grocery line.
            ∗ Parallelism: Does NOT improve latency (adding 100 cashiers
              doesn’t make your checkout faster).
            ∗ Critical for: Gaming, High-Frequency Trading.
        – Throughput: Amount of data processed per unit time (e.g., bytes/sec).
            ∗ Analogy: Total customers exiting the store per hour.
            ∗ Parallelism: DOES improve throughput (100 cashiers process
              100x more people).
            ∗ Critical for: Data Science and Big Data processing.
   • The Big Data Bottleneck:
        – Bottleneck is usually Disk I/O (Moving data Disk → Memory), not
          CPU speed.
        – Example: Processing 1TB of data.
            ∗ Single Machine (200MB/s): ∼1.4 hours.
            ∗ 100 Machines (Parallel): ∼50 seconds.
        – Solution: MapReduce/Spark organize this parallel processing on
          unreliable clusters.




                                      3
1.3.2   Juypter Notebook Content: Numpy vs Pure Python
   • The Experiment: Matrix Multiplication (A × B) of size 100 × 100.
   • Results:

        – NumPy: ∼0.4 ms (uses optimized C/Fortran libraries).
        – Pure Python: ∼500 ms (uses nested loops).
        – Speedup: NumPy is ∼1000x faster for this size.
   • Under the Hood:

        – NumPy relies on LAPACK (Linear Algebra PACKage).
        – Written in Fortran (released 1992).
        – Highly optimized for vector/matrix operations.
   • Scaling Behavior:

        – Small Matrices (< 10 × 10): No significant advantage (overhead
          dominates).
        – Large Matrices (300 × 300): NumPy is ∼10,000x faster.




                                     4
2       Topic: Memory Hierarchy
2.1     Latency Throughput and Memory Hierarchy
2.1.1    Lecture Content
    • Definitions:
         – Latency: The total time to process one single unit from start to
           finish. (Analogy: Time waiting in line + checkout).
         – Throughput: The number of units processed per unit of time.
           (Analogy: Customers exiting the store per hour).
         – Key Insight: Throughput is not necessarily 1 Latency.
             ∗ Example: A store with no lines has low latency (fast checkout),
               but if nobody visits, throughput is near zero.

    • Analogy: Costco (Wholesale) vs. Retail:
         – Wholesale (Batch Processing): High Latency (2 hours to drive
           truck), but massive Throughput (transferring 1000s of bottles). This
           is the Big Data approach.
         – Retail (Random Access): Low Latency (30 seconds to grab a
           bottle), but low Throughput (one bottle at a time). This is the
           Interactive approach.
    • Hard Disk Mechanics:
         – Seek Time (Latency): ∼10ms to physically move the read head.
         – Sequential Read (Throughput): ∼100 MB/s once the head is in
           position.
         – The Trap of Random Access: Reading 1 byte randomly requires
           the full 10ms seek time.
             ∗ Result: Throughput drops to ∼100 Bytes/sec.
             ∗ Solution: Always read in blocks (Sequential Access) to amortize
               the seek cost.
    • Data Transfer at Scale (AWS Snowball):

         – Transferring 50TB over a 100Mbps university line takes ∼46 days.
         – Transferring 50TB via AWS Snowball (physical shipping via FedEx)
           takes ∼24 hours.
         – Lesson: For massive data, physical transport (high latency) offers
           superior throughput compared to network transfer.




                                       5
2.2     Storage Latency
2.2.1   Lecture Content
   • The Basic Operation (C = A × B):

        – Requires 4 distinct steps, each adding to total latency:
           1. Read A from Storage (High Latency).
           2. Read B from Storage (High Latency).
           3. Compute A × B in CPU (Low Latency).
           4. Write C to Storage (High Latency).
   • The Bottleneck:

        – In Big Data analysis, the majority of execution time is spent on
          Steps 1, 2, and 4 (Storage I/O).
        – The actual computation (Step 3) is negligible compared to data move-
          ment.
   • Storage Hierarchy:

        – Different storage types offer different trade-offs between Latency, Ca-
          pacity, and Price.
        – Low Latency: Main Memory (RAM).
        – Medium Latency: Spinning Disk.
        – High Latency: Remote Computer / Cloud Storage.

   • Goal of Big Data Systems: Organize storage and computation to max-
     imize Throughput (processing speed) while minimizing Cost.

2.3     Memory Hierarchy
2.3.1   Lecture Content
2.3.2   Lecture Content
   • The Hierarchy Levels:

        1. CPU Registers: Fastest (∼ 300 ps), Smallest (∼ 1 KB).
        2. L1/L2/L3 Cache: Fast (∼ 1 − 20 ns), Small (∼ 64 KB - 4 MB).
        3. Main Memory (RAM): Moderate (∼ 100 ns), Moderate Size (∼
           16 GB).
        4. Disk Storage: Slow (∼ 10 ms), Huge Size (∼ 16 TB). Note: 6
           orders of magnitude slower than CPU!
        5. Network (Cluster): Slowest, Massive Scale (10+ PB).
   • The Abstraction:


                                       6
        – Hardware presents memory as a single, large, flat array.
        – Performance relies on Locality: The hardware automatically moves
          frequently accessed data to the faster levels.
   • Cluster Computing (Spark Context):

        – Extends the hierarchy via Ethernet.
        – Locality Rule: Compute data on the node where it resides to avoid
          network latency.
        – Shuffling: The cluster equivalent of moving data between levels (ex-
          pensive).
        – Spark RDD: The abstraction that makes a cluster look like a single
          computer’s memory.

2.4     Heavy Tail Distributions
2.4.1   Lecture Content
   • The Problem: Memory is 1-Dimensional (linear addresses), but Matrices
     are 2-Dimensional. We must flatten 2D data into 1D storage.
   • Two Standards:

        1. Row-Major Order: Consecutive elements of a row are stored to-
           gether.
            – Used by: C, C++, Python, NumPy.
            – Traversal: Fast to iterate row-by-row (inner loop on columns).
        2. Column-Major Order: Consecutive elements of a column are
           stored together.
            – Used by: Fortran, MATLAB, R, Spark (often, due to Scala/JVM).
            – Traversal: Fast to iterate column-by-column (inner loop on rows).
   • Performance Impact: Traversing a Row-Major array in Column order
     (or vice versa) breaks Spatial Locality.
        – The CPU fetches a cache line, uses 1 value, and discards the rest.
        – Result: Massive increase in Cache Misses and execution time.

2.4.2   Juypter Notebook Content: Row vs Col Major
   • The Experiment: Summing elements of a 10k × 10k NumPy matrix.

   • Results:
        – Row Traversal (Good Locality): ∼8 seconds.
        – Column Traversal (Bad Locality): ∼100-200 seconds.


                                      7
        – Factor: 10x - 20x slowdown purely due to memory access patterns.
   • Key Takeaway: Always match your iteration order to the language’s
     storage layout. For NumPy, iterate over rows first.

2.4.3   Juypter Notebook Content: Measuring Performance of Mem-
        ory Hierarchy
   • The Experiment: Accessing random indices in arrays of increasing size
     (1KB to 1GB).
   • The Staircase Graph:
        – Step 1 (L1 Cache): Sizes < 32KB. Latency ∼ 0.5 ns.
        – Step 2 (L2 Cache): Sizes 32KB - 256KB. Latency ∼ 2 − 5 ns.
        – Step 3 (L3 Cache): Sizes 256KB - 6MB. Latency ∼ 10 ns.
        – Step 4 (Main Memory): Sizes > 10MB. Latency ∼ 100 ns.
   • Conclusion: You can physically ”see” the cache sizes of a computer by
     measuring access latency spikes.

2.4.4   Juypter Notebook Content: A More Accurate Measure of
        Memory Poke Latency
   • The Measurement Problem: A naive loop ‘for i in range(n): x = A[i]‘
     measures both the memory access AND the loop overhead (Python logic,
     index increment).
   • The Solution (Loop Unrolling):

        – Perform many accesses inside a single loop iteration.
        – Code: ‘sum += A[i] + A[i+1] + ... + A[i+9]‘
   • Effect: Amortizes the loop overhead across many operations, bringing
     the measured time closer to the true hardware latency (approx 1 ns for
     L1 vs 3-4 ns naive).




                                      8
3     Exam Traps: Danger Zone
    • The Definition Trap: Data Science is not just ”analyzing data”; the
      professor explicitly defines it as rational decision making (Big vs. Small
      decisions).
    • The Traffic Graph Trap: Higher density does not always mean higher
      flow. After the ”critical density” peak, increasing density decreases flow
      (Traffic Jam).
    • The ”Matrix” Trap: Matrices are homogeneous (numbers only) and
      must fit in one computer’s memory. If the data is 50TB or has mixed
      types (strings/ints), it’s a Table or DataFrame, not a Matrix.
    • The Parallelism Limit: Parallelism improves Throughput (total vol-
      ume), but it rarely improves Latency (time for one unit). Adding 100
      computers won’t make a single network request 100x faster.
    • The ”Dirty” Penalty: A cache miss is bad. A cache miss on a dirty
      block is worse because you pay double the latency: 1) Write the old dirty
      data to RAM, 2) Read the new data from RAM.

    • Language Defaults: NumPy is Row-Major. Iterating column-by-
      column kills performance (10-20x slower) because it breaks spatial locality.
    • The Bottleneck Reality: In Big Data, the bottleneck is almost always
      Disk I/O (moving data), not CPU speed. ”Faster Math” doesn’t help if
      the CPU is waiting for data.




                                        9
4     Practice Quiz
Question 1
The Spatial Locality Check
Which operation benefits most from Spatial Locality?
a Updating a single global sum variable 1,000 times.
b Reading every element of a 10,000-element integer array.

c Traversing a binary tree with 10,000 nodes allocated randomly in memory.
d Calculating a factorial using recursion.

Answer: B
Brief Explanation
    • Spatial Locality refers to accessing memory addresses that are contigu-
      ous (next to each other).
    • An array is stored contiguously in memory. Loading the first element likely
      loads the next 64 bytes (Cache Line) automatically, making subsequent
      reads nearly instant.

    • Option A is Temporal Locality (same address). Option C has poor locality
      (random pointers).

Question 2
Traffic Flow Theory
According to the Fundamental Diagram of Traffic, what happens when density
exceeds the critical peak?
a Flow remains constant at maximum capacity.

b Flow increases linearly with density.
c Flow decreases as velocity drops significantly.
d Throughput increases, but Latency decreases.

Answer: C
Brief Explanation
    • Beyond the peak density, cars are too close to move fast. The drop in
      speed outweighs the increase in density, causing a traffic jam where flow
      (throughput) drops.


                                       10
Question 3
Data Models
Which Data Model is characterized by having named columns, heterogeneous
types (different types for different columns), and is designed to scale across
many disks?
a Matrix
b Relation (Table)

c Vector
d Tensor

Answer: B
Brief Explanation
   • Matrices are homogeneous and memory-bound. Tables (Relations) allow
     different types per column and use indices to scale to massive sizes on
     disk.

Question 4
Latency vs. Throughput
You need to transfer 50TB of data. Option A (Fiber Optic) takes 11 hours.
Option B (AWS Snowball/FedEx) takes 24 hours. Which statement is true?
a Option A has higher Latency and higher Throughput.

b Option B has higher Latency, but could have higher Throughput if data size
  increases to 100TB.
c Option A is always preferred because Latency is lower.
d Throughput is identical because the data size is the same.

Answer: B
Brief Explanation
   • Physical transport (Snowball) has a fixed high latency (24h travel time)
     but massive bandwidth. If you scaled to 100TB, Fiber would take 22
     hours, while the truck still takes 24 hours, making the throughput com-
     parison dependent on volume.




                                      11
Question 5
4.0.1   NumPy Speed
Why is NumPy matrix multiplication (∼1ms) vastly faster than a pure Python
implementation (∼500ms) for a 100 × 100 matrix?
a NumPy uses GPU acceleration automatically.
b NumPy uses a specialized C++ compiler at runtime.
c NumPy calls optimized Fortran libraries (LAPACK).

d NumPy avoids memory storage by streaming data.

Answer: C
Brief Explanation
   • NumPy relies on BLAS/LAPACK libraries, originally written in Fortran
     (1992), which are highly optimized for vector operations.

Question 6
Cache Miss Penalty
In the context of the Memory Hierarchy, what is a ”Dirty Eviction”?
a Removing data that has been corrupted.
b Removing data that was read but never used.
c Evicting a cache block that has been modified, requiring a write-back to RAM.

d Clearing the cache to prevent security leaks.

Answer: C
Brief Explanation
   • If the CPU modified a value in the cache (”dirty”), that value must be
     saved to main memory before the cache slot can be reused. This doubles
     the latency penalty.




                                      12
Question 7
Row vs. Column Major
You are iterating through a standard NumPy array using a nested loop. To
maximize performance, which index should be in the inner loop?
a The Row index (iterate top-down).
b The Column index (iterate left-right).
c It does not matter for NumPy.

d It depends on the size of the array.

Answer: B
Brief Explanation
   • NumPy is Row-Major. Elements in the same row are stored next to
     each other. Iterating along the column index (moving left-to-right across
     a row) accesses contiguous memory, maximizing spatial locality.

Question 8
Hierarchy Scale
Approximately how much slower is a Random Disk Access compared to a Main
Memory (RAM) Access?
a 10x
b 100x

c 1,000x
d 100,000x (5 orders of magnitude)

Answer: D
Brief Explanation
   • RAM access is ∼100 nanoseconds. Disk seek is ∼10 milliseconds. 10ms =
     10, 000, 000ns. The difference is roughly 5-6 orders of magnitude (105 ).




                                         13
Question 9
Big Data Latency Chain
When calculating C = A × B on a dataset larger than memory, which step
dominates the execution time?
a Computing the product in the ALU.
b Reading A and B from Disk.
c Allocating memory variables.

d The operating system scheduler.

Answer: B
Brief Explanation
   • In Big Data, the bottleneck is moving data from slow storage (Disk) to
     fast memory. The CPU spends most of its time waiting for data (I/O
     Bound).

Question 10
Data Structures
Why does a Linked List generally exhibit poor performance compared to an
Array for large sequential scans?
a Linked Lists have O(N 2 ) access time.
b Linked Lists use more memory for integers.

c Linked List nodes are scattered in memory, causing frequent Cache Misses.
d Arrays are always cached in L1 by default.

Answer: C
Brief Explanation
   • Linked list nodes are allocated dynamically and linked via pointers, scat-
     tering them across memory pages. Traversing them requires loading many
     different pages, whereas an Array loads many elements in a single page
     (Spatial Locality).




                                      14

--- end {week_01_guide.pdf} ---
--- start{week_01_guide.tex} ---
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Code Snippet
% LaTeX Template
% Version 1.0 (14/2/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Velimir Gayevskiy (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{article}

%----------------------------------------------------------------------------------------
\usepackage{enumerate}
\usepackage{listings} % Required for inserting code snippets
\usepackage[usenames,dvipsnames]{color} % Required for specifying custom colors and referring to colors by name

\definecolor{DarkGreen}{rgb}{0.0,0.4,0.0} % Comment color
\definecolor{highlight}{RGB}{255,251,204} % Code highlight color

\lstdefinestyle{Style1}{ % Define a style for your code snippet, multiple definitions can be made if, for example, you wish to insert multiple code snippets using different programming languages into one document
language=Python, % Detects keywords, comments, strings, functions, etc for the language specified
backgroundcolor=\color{highlight}, % Set the background color for the snippet - useful for highlighting
basicstyle=\footnotesize\ttfamily, % The default font size and style of the code
breakatwhitespace=false, % If true, only allows line breaks at white space
breaklines=true, % Automatic line breaking (prevents code from protruding outside the box)
captionpos=b, % Sets the caption position: b for bottom; t for top
commentstyle=\usefont{T1}{pcr}{m}{sl}\color{DarkGreen}, % Style of comments within the code - dark green courier font
deletekeywords={}, % If you want to delete any keywords from the current language separate them by commas
%escapeinside={\%}, % This allows you to escape to LaTeX using the character in the bracket
firstnumber=1, % Line numbers begin at line 1
frame=single, % Frame around the code box, value can be: none, leftline, topline, bottomline, lines, single, shadowbox
frameround=tttt, % Rounds the corners of the frame for the top left, top right, bottom left and bottom right positions
keywordstyle=\color{Blue}\bf, % Functions are bold and blue
morekeywords={}, % Add any functions no included by default here separated by commas
numbers=left, % Location of line numbers, can take the values of: none, left, right
numbersep=10pt, % Distance of line numbers from the code box
numberstyle=\tiny\color{Gray}, % Style used for line numbers
rulecolor=\color{black}, % Frame border color
showstringspaces=false, % Don't put marks in string spaces
showtabs=false, % Display tabs in the code as lines
stepnumber=5, % The step distance between line numbers, i.e. how often will lines be numbered
stringstyle=\color{Purple}, % Strings are purple
tabsize=4, % Number of spaces per tab in the code
}

% set title
\title{DSC 232R: Big Data Analytics Using Spark \\ Winter 2026 \\ Week 1} 
%----------------------------------------------------------------------------------------

\begin{document}

\maketitle

%----------------------------------------------------------------------------------------
% section 1 
%----------------------------------------------------------------------------------------

\section{Topic: Introduction to Big Data Using Spark}

%----------------------------------------------------------------------------------------
% section 1.1
%----------------------------------------------------------------------------------------
\subsection{What is Data Science}
\subsubsection{Lecture Content}
\begin{itemize}
    \item \textbf{Definition:} Data Science is defined as \textit{rational decision making} using data to make decisions that are useful and profitable. It combines "Data" (collection from sensors, logs, etc.) and "Science" (hypothesis testing and verification).
    \item \textbf{Types of Decisions:}
    \begin{itemize}
        \item \textbf{Big Decisions:} Strategic, high-stakes decisions involving deliberation by people.
        \begin{itemize}
            \item \textit{Examples:} Is city water safe?, Federal interest rate hikes, Infrastructure changes (adding lanes).
        \end{itemize}
        \item \textbf{Small Decisions:} Tactical, automated, high-volume decisions made without human intervention.
        \begin{itemize}
            \item \textit{Examples:} Ad selection (Google), Movie recommendations (Netflix), Loan approvals, Ramp metering (traffic lights on highway on-ramps).
        \end{itemize}
    \end{itemize}
    \item \textbf{Ingredients of Data Science:}
    \begin{enumerate}
        \item \textbf{Math:} Linear Algebra, Probability, Statistics.
        \item \textbf{Machine Learning:} Algorithms to build flexible models.
        \item \textbf{Software Development:} Implementing at scale.
        \item \textbf{Domain Knowledge:} Understanding the specific science of the problem (e.g., Traffic Flow theory).
    \end{enumerate}
    \item \textbf{Case Study: Caltrans PeMS (Performance Measurement System):}
    \begin{itemize}
        \item \textbf{Data Collection:} ~45,000 magnetic loop detectors in CA highways.
        \item \textbf{Measurements:}
        \begin{itemize}
            \item \textit{Flow:} Number of cars per unit time.
            \item \textit{Occupancy:} Fraction of time a car is over the loop.
        \end{itemize}
        \item \textbf{Traffic Theory (The Fundamental Diagram):} Relationship between \textit{Density} (cars/mile) and \textit{Flow}.
        \begin{itemize}
            \item Low Density $\rightarrow$ High Speed, increasing Flow.
            \item Peak Flow $\rightarrow$ Optimal capacity.
            \item High Density (Traffic Jam) $\rightarrow$ Low Speed, decreasing Flow.
        \end{itemize}
        \item \textbf{Analysis Techniques:} Uses PCA (Principal Component Analysis) to identify traffic profiles (e.g., AM vs PM peaks).
    \end{itemize}
\end{itemize}
%----------------------------------------------------------------------------------------
% section 1.2 
%----------------------------------------------------------------------------------------
\subsection{Data Engineering and Data Science}
\subsubsection{Lecture Content}
\begin{itemize}
    \item \textbf{Roles:}
    \begin{itemize}
        \item \textbf{Data Scientist:} Builds models, answers business questions, uses Stats/ML. Expects data availability and fast computation.
        \item \textbf{Data Engineer:} Builds the infrastructure (databases, streaming, cloud, pipelines) that supports the data scientist.
    \end{itemize}
    \item \textbf{Course Scope (Data Engineering Topics):}
    \begin{itemize}
        \item \textbf{Covered:} Hadoop File System (HDFS), Data Partitioning, Caching/Persistence, Checkpointing.
        \item \textbf{Not Covered:} Data Cleaning, Spark Server Optimization, Containerization.
    \end{itemize}
    \item \textbf{Data Models (The Interface):}
    \begin{enumerate}
        \item \textbf{Matrix (Linear Algebra):}
        \begin{itemize}
            \item Rectangle of numbers (all same type).
            \item Supports transposition, addition, multiplication.
            \item \textit{Limitation:} Typically must fit in the memory of \textbf{one} computer.
        \end{itemize}
        \item \textbf{Relation/Table (Relational Databases):}
        \begin{itemize}
            \item Rows = Tuples (Entities), Columns = Properties (Attributes).
            \item Columns have types, but types can differ across columns.
            \item \textit{Scale:} Can span many disks/computers; uses indices for fast retrieval without loading everything into memory.
        \end{itemize}
        \item \textbf{DataFrame (The Hybrid):}
        \begin{itemize}
            \item Blend of Matrix and Table concepts (popularized by R/S/Pandas).
            \item Ordered, named rows and columns.
            \item \textbf{Spark DataFrames:} Designed to reside on disk and support distributed processing (unlike standard in-memory matrices).
        \end{itemize}
    \end{enumerate}
\end{itemize}

%----------------------------------------------------------------------------------------
% section 1.3
%----------------------------------------------------------------------------------------
\subsection{Speeding Up Data Processing}
\subsubsection{Lecture Content}
\begin{itemize}
    \item \textbf{Two Primary Optimization Methods:}
    \begin{enumerate}
        \item \textbf{Fast Libraries (Vectorization):} Replacing explicit Python loops with calls to optimized libraries (e.g., NumPy).
        \item \textbf{Throughput vs. Latency:} Optimizing for volume rather than individual speed.
    \end{enumerate}
    \item \textbf{Throughput vs. Latency Definitions:}
    \begin{itemize}
        \item \textbf{Latency:} Time to process a single item from start to finish.
        \begin{itemize}
            \item \textit{Analogy:} Time spent waiting in a grocery line.
            \item \textit{Parallelism:} Does NOT improve latency (adding 100 cashiers doesn't make \textit{your} checkout faster).
            \item \textit{Critical for:} Gaming, High-Frequency Trading.
        \end{itemize}
        \item \textbf{Throughput:} Amount of data processed per unit time (e.g., bytes/sec).
        \begin{itemize}
            \item \textit{Analogy:} Total customers exiting the store per hour.
            \item \textit{Parallelism:} DOES improve throughput (100 cashiers process 100x more people).
            \item \textit{Critical for:} Data Science and Big Data processing.
        \end{itemize}
    \end{itemize}
    \item \textbf{The Big Data Bottleneck:}
    \begin{itemize}
        \item Bottleneck is usually \textbf{Disk I/O} (Moving data Disk $\to$ Memory), not CPU speed.
        \item \textit{Example:} Processing 1TB of data.
        \begin{itemize}
            \item Single Machine (200MB/s): $\sim$1.4 hours.
            \item 100 Machines (Parallel): $\sim$50 seconds.
        \end{itemize}
        \item \textbf{Solution:} MapReduce/Spark organize this parallel processing on unreliable clusters.
    \end{itemize}
\end{itemize}

\subsubsection{Juypter Notebook Content: Numpy vs Pure Python}
\begin{itemize}
    \item \textbf{The Experiment:} Matrix Multiplication ($A \times B$) of size $100 \times 100$.
    \item \textbf{Results:}
    \begin{itemize}
        \item \textbf{NumPy:} $\sim$0.4 ms (uses optimized C/Fortran libraries).
        \item \textbf{Pure Python:} $\sim$500 ms (uses nested loops).
        \item \textbf{Speedup:} NumPy is $\sim$1000x faster for this size.
    \end{itemize}
    \item \textbf{Under the Hood:}
    \begin{itemize}
        \item NumPy relies on \textbf{LAPACK} (Linear Algebra PACKage).
        \item Written in \textbf{Fortran} (released 1992).
        \item Highly optimized for vector/matrix operations.
    \end{itemize}
    \item \textbf{Scaling Behavior:}
    \begin{itemize}
        \item \textbf{Small Matrices ($< 10 \times 10$):} No significant advantage (overhead dominates).
        \item \textbf{Large Matrices ($300 \times 300$):} NumPy is $\sim$10,000x faster.
    \end{itemize}
\end{itemize}

\newpage
%----------------------------------------------------------------------------------------
% section 2
%----------------------------------------------------------------------------------------
\section{Topic: Memory Hierarchy}

%----------------------------------------------------------------------------------------
% section 2.1 
%----------------------------------------------------------------------------------------
\subsection{Latency Throughput and Memory Hierarchy}
\subsubsection{Lecture Content}
\begin{itemize}
    \item \textbf{Definitions:}
    \begin{itemize}
        \item \textbf{Latency:} The total time to process one single unit from start to finish. (Analogy: Time waiting in line + checkout).
        \item \textbf{Throughput:} The number of units processed per unit of time. (Analogy: Customers exiting the store per hour).
        \item \textbf{Key Insight:} Throughput is \textbf{not} necessarily 1 Latency.
        \begin{itemize}
            \item \textit{Example:} A store with no lines has low latency (fast checkout), but if nobody visits, throughput is near zero.
        \end{itemize}
    \end{itemize}
    \item \textbf{Analogy: Costco (Wholesale) vs. Retail:}
    \begin{itemize}
        \item \textbf{Wholesale (Batch Processing):} High Latency (2 hours to drive truck), but massive Throughput (transferring 1000s of bottles). This is the Big Data approach.
        \item \textbf{Retail (Random Access):} Low Latency (30 seconds to grab a bottle), but low Throughput (one bottle at a time). This is the Interactive approach.
    \end{itemize}
    \item \textbf{Hard Disk Mechanics:}
    \begin{itemize}
        \item \textbf{Seek Time (Latency):} $\sim$10ms to physically move the read head.
        \item \textbf{Sequential Read (Throughput):} $\sim$100 MB/s once the head is in position.
        \item \textbf{The Trap of Random Access:} Reading 1 byte randomly requires the full 10ms seek time.
        \begin{itemize}
            \item Result: Throughput drops to $\sim$100 Bytes/sec.
            \item Solution: Always read in \textbf{blocks} (Sequential Access) to amortize the seek cost.
        \end{itemize}
    \end{itemize}
    \item \textbf{Data Transfer at Scale (AWS Snowball):}
    \begin{itemize}
        \item Transferring 50TB over a 100Mbps university line takes $\sim$46 days.
        \item Transferring 50TB via \textbf{AWS Snowball} (physical shipping via FedEx) takes $\sim$24 hours.
        \item \textbf{Lesson:} For massive data, physical transport (high latency) offers superior throughput compared to network transfer.
    \end{itemize}
\end{itemize}

%----------------------------------------------------------------------------------------
% section 2.2 
%----------------------------------------------------------------------------------------
\subsection{Storage Latency}
\subsubsection{Lecture Content}
\begin{itemize}
    \item \textbf{The Basic Operation ($C = A \times B$):}
    \begin{itemize}
        \item Requires 4 distinct steps, each adding to total latency:
        \begin{enumerate}
            \item Read $A$ from Storage (High Latency).
            \item Read $B$ from Storage (High Latency).
            \item Compute $A \times B$ in CPU (Low Latency).
            \item Write $C$ to Storage (High Latency).
        \end{enumerate}
    \end{itemize}
    \item \textbf{The Bottleneck:}
    \begin{itemize}
        \item In Big Data analysis, the majority of execution time is spent on \textbf{Steps 1, 2, and 4} (Storage I/O).
        \item The actual computation (Step 3) is negligible compared to data movement.
    \end{itemize}
    \item \textbf{Storage Hierarchy:}
    \begin{itemize}
        \item Different storage types offer different trade-offs between Latency, Capacity, and Price.
        \item \textbf{Low Latency:} Main Memory (RAM).
        \item \textbf{Medium Latency:} Spinning Disk.
        \item \textbf{High Latency:} Remote Computer / Cloud Storage.
    \end{itemize}
    \item \textbf{Goal of Big Data Systems:} Organize storage and computation to maximize Throughput (processing speed) while minimizing Cost.
\end{itemize}

%----------------------------------------------------------------------------------------
% section 2.3 
%----------------------------------------------------------------------------------------
\subsection{Memory Hierarchy}
\subsubsection{Lecture Content}
\subsubsection{Lecture Content}
\begin{itemize}
    \item \textbf{The Hierarchy Levels:}
    \begin{enumerate}
        \item \textbf{CPU Registers:} Fastest ($\sim300$ ps), Smallest ($\sim1$ KB).
        \item \textbf{L1/L2/L3 Cache:} Fast ($\sim1-20$ ns), Small ($\sim64$ KB - $4$ MB).
        \item \textbf{Main Memory (RAM):} Moderate ($\sim100$ ns), Moderate Size ($\sim16$ GB).
        \item \textbf{Disk Storage:} Slow ($\sim10$ ms), Huge Size ($\sim16$ TB). \textit{Note: 6 orders of magnitude slower than CPU!}
        \item \textbf{Network (Cluster):} Slowest, Massive Scale ($10+$ PB).
    \end{enumerate}
    \item \textbf{The Abstraction:}
    \begin{itemize}
        \item Hardware presents memory as a single, large, flat array.
        \item Performance relies on \textbf{Locality}: The hardware automatically moves frequently accessed data to the faster levels.
    \end{itemize}
    \item \textbf{Cluster Computing (Spark Context):}
    \begin{itemize}
        \item Extends the hierarchy via \textbf{Ethernet}.
        \item \textbf{Locality Rule:} Compute data on the node where it resides to avoid network latency.
        \item \textbf{Shuffling:} The cluster equivalent of moving data between levels (expensive).
        \item \textbf{Spark RDD:} The abstraction that makes a cluster look like a single computer's memory.
    \end{itemize}
\end{itemize}
%----------------------------------------------------------------------------------------
% section 2.4 
%----------------------------------------------------------------------------------------
\subsection{Heavy Tail Distributions}
\subsubsection{Lecture Content}
\begin{itemize}
    \item \textbf{The Problem:} Memory is 1-Dimensional (linear addresses), but Matrices are 2-Dimensional. We must flatten 2D data into 1D storage.
    \item \textbf{Two Standards:}
    \begin{enumerate}
        \item \textbf{Row-Major Order:} Consecutive elements of a \textbf{row} are stored together.
        \begin{itemize}
            \item \textit{Used by:} C, C++, Python, \textbf{NumPy}.
            \item \textit{Traversal:} Fast to iterate row-by-row (inner loop on columns).
        \end{itemize}
        \item \textbf{Column-Major Order:} Consecutive elements of a \textbf{column} are stored together.
        \begin{itemize}
            \item \textit{Used by:} Fortran, MATLAB, R, Spark (often, due to Scala/JVM).
            \item \textit{Traversal:} Fast to iterate column-by-column (inner loop on rows).
        \end{itemize}
    \end{enumerate}
    \item \textbf{Performance Impact:} Traversing a Row-Major array in Column order (or vice versa) breaks \textbf{Spatial Locality}.
    \begin{itemize}
        \item The CPU fetches a cache line, uses 1 value, and discards the rest.
        \item Result: Massive increase in Cache Misses and execution time.
    \end{itemize}
\end{itemize}

\subsubsection{Juypter Notebook Content: Row vs Col Major}
\begin{itemize}
    \item \textbf{The Experiment:} Summing elements of a $10k \times 10k$ NumPy matrix.
    \item \textbf{Results:}
    \begin{itemize}
        \item \textbf{Row Traversal (Good Locality):} $\sim$8 seconds.
        \item \textbf{Column Traversal (Bad Locality):} $\sim$100-200 seconds.
        \item \textbf{Factor:} 10x - 20x slowdown purely due to memory access patterns.
    \end{itemize}
    \item \textbf{Key Takeaway:} Always match your iteration order to the language's storage layout. For NumPy, iterate over rows first.
\end{itemize}

\subsubsection{Juypter Notebook Content: Measuring Performance of Memory Hierarchy}
\begin{itemize}
    \item \textbf{The Experiment:} Accessing random indices in arrays of increasing size ($1$KB to $1$GB).
    \item \textbf{The Staircase Graph:}
    \begin{itemize}
        \item \textbf{Step 1 (L1 Cache):} Sizes $< 32$KB. Latency $\sim 0.5$ ns.
        \item \textbf{Step 2 (L2 Cache):} Sizes $32$KB - $256$KB. Latency $\sim 2-5$ ns.
        \item \textbf{Step 3 (L3 Cache):} Sizes $256$KB - $6$MB. Latency $\sim 10$ ns.
        \item \textbf{Step 4 (Main Memory):} Sizes $> 10$MB. Latency $\sim 100$ ns.
    \end{itemize}
    \item \textbf{Conclusion:} You can physically "see" the cache sizes of a computer by measuring access latency spikes.
\end{itemize}

\subsubsection{Juypter Notebook Content: A More Accurate Measure of Memory Poke Latency}
\begin{itemize}
    \item \textbf{The Measurement Problem:} A naive loop `for i in range(n): x = A[i]` measures both the memory access AND the loop overhead (Python logic, index increment).
    \item \textbf{The Solution (Loop Unrolling):}
    \begin{itemize}
        \item Perform many accesses inside a single loop iteration.
        \item Code: `sum += A[i] + A[i+1] + ... + A[i+9]`
    \end{itemize}
    \item \textbf{Effect:} Amortizes the loop overhead across many operations, bringing the measured time closer to the true hardware latency (approx 1 ns for L1 vs 3-4 ns naive).
\end{itemize}

\newpage

%----------------------------------------------------------------------------------------
% Exam Traps Section
%----------------------------------------------------------------------------------------
\section{Exam Traps: Danger Zone}
\begin{itemize}
    \item \textbf{The Definition Trap:} Data Science is not just "analyzing data"; the professor explicitly defines it as \textit{rational decision making} (Big vs. Small decisions).
    \item \textbf{The Traffic Graph Trap:} Higher density does \textbf{not} always mean higher flow. After the "critical density" peak, increasing density \textit{decreases} flow (Traffic Jam).
    \item \textbf{The "Matrix" Trap:} Matrices are homogeneous (numbers only) and must fit in \textbf{one} computer's memory. If the data is 50TB or has mixed types (strings/ints), it's a Table or DataFrame, not a Matrix.
    \item \textbf{The Parallelism Limit:} Parallelism improves \textbf{Throughput} (total volume), but it rarely improves \textbf{Latency} (time for one unit). Adding 100 computers won't make a single network request 100x faster.
    \item \textbf{The "Dirty" Penalty:} A cache miss is bad. A cache miss on a \textbf{dirty} block is worse because you pay double the latency: 1) Write the old dirty data to RAM, 2) Read the new data from RAM.
    \item \textbf{Language Defaults:} NumPy is \textbf{Row-Major}. Iterating column-by-column kills performance (10-20x slower) because it breaks spatial locality.
    \item \textbf{The Bottleneck Reality:} In Big Data, the bottleneck is almost always \textbf{Disk I/O} (moving data), not CPU speed. "Faster Math" doesn't help if the CPU is waiting for data.
\end{itemize}

\newpage
%----------------------------------------------------------------------------------------
% Practice Quiz Section
%----------------------------------------------------------------------------------------
\section{Practice Quiz}

% Question 1
\subsection*{Question 1}
\subsubsection*{The Spatial Locality Check}
Which operation benefits most from \textbf{Spatial Locality}?
\begin{enumerate}[a]
    \item Updating a single global sum variable 1,000 times.
    \item Reading every element of a 10,000-element integer array.
    \item Traversing a binary tree with 10,000 nodes allocated randomly in memory.
    \item Calculating a factorial using recursion.
\end{enumerate}

\subsubsection*{Answer: B}
\subsubsection*{Brief Explanation}
\begin{itemize}
    \item \textbf{Spatial Locality} refers to accessing memory addresses that are \textit{contiguous} (next to each other).
    \item An array is stored contiguously in memory. Loading the first element likely loads the next 64 bytes (Cache Line) automatically, making subsequent reads nearly instant.
    \item Option A is \textit{Temporal} Locality (same address). Option C has poor locality (random pointers).
\end{itemize}

% Question 2
\subsection*{Question 2}
\subsubsection*{Traffic Flow Theory}
According to the Fundamental Diagram of Traffic, what happens when density exceeds the critical peak?
\begin{enumerate}[a]
    \item Flow remains constant at maximum capacity.
    \item Flow increases linearly with density.
    \item Flow decreases as velocity drops significantly.
    \item Throughput increases, but Latency decreases.
\end{enumerate}

\subsubsection*{Answer: C}
\subsubsection*{Brief Explanation}
\begin{itemize}
    \item Beyond the peak density, cars are too close to move fast. The drop in speed outweighs the increase in density, causing a traffic jam where flow (throughput) drops.
\end{itemize}

% Question 3
\subsection*{Question 3}
\subsubsection*{Data Models}
Which Data Model is characterized by having named columns, heterogeneous types (different types for different columns), and is designed to scale across many disks?
\begin{enumerate}[a]
    \item Matrix
    \item Relation (Table)
    \item Vector
    \item Tensor
\end{enumerate}

\subsubsection*{Answer: B}
\subsubsection*{Brief Explanation}
\begin{itemize}
    \item Matrices are homogeneous and memory-bound. Tables (Relations) allow different types per column and use indices to scale to massive sizes on disk.
\end{itemize}

% Question 4
\subsection*{Question 4}
\subsubsection*{Latency vs. Throughput}
You need to transfer 50TB of data. Option A (Fiber Optic) takes 11 hours. Option B (AWS Snowball/FedEx) takes 24 hours. Which statement is true?
\begin{enumerate}[a]
    \item Option A has higher Latency and higher Throughput.
    \item Option B has higher Latency, but could have higher Throughput if data size increases to 100TB.
    \item Option A is always preferred because Latency is lower.
    \item Throughput is identical because the data size is the same.
\end{enumerate}

\subsubsection*{Answer: B}
\subsubsection*{Brief Explanation}
\begin{itemize}
    \item Physical transport (Snowball) has a fixed high latency (24h travel time) but massive bandwidth. If you scaled to 100TB, Fiber would take 22 hours, while the truck still takes 24 hours, making the throughput comparison dependent on volume.
\end{itemize}

% Question 5
\subsection*{Question 5}
\subsubsection{NumPy Speed}
Why is NumPy matrix multiplication ($\sim$1ms) vastly faster than a pure Python implementation ($\sim$500ms) for a $100 \times 100$ matrix?
\begin{enumerate}[a]
    \item NumPy uses GPU acceleration automatically.
    \item NumPy uses a specialized C++ compiler at runtime.
    \item NumPy calls optimized Fortran libraries (LAPACK).
    \item NumPy avoids memory storage by streaming data.
\end{enumerate}

\subsubsection*{Answer: C}
\subsubsection*{Brief Explanation}
\begin{itemize}
    \item NumPy relies on BLAS/LAPACK libraries, originally written in Fortran (1992), which are highly optimized for vector operations.
\end{itemize}

% Question 6
\subsection*{Question 6}
\subsubsection*{Cache Miss Penalty}
In the context of the Memory Hierarchy, what is a "Dirty Eviction"?
\begin{enumerate}[a]
    \item Removing data that has been corrupted.
    \item Removing data that was read but never used.
    \item Evicting a cache block that has been modified, requiring a write-back to RAM.
    \item Clearing the cache to prevent security leaks.
\end{enumerate}

\subsubsection*{Answer: C}
\subsubsection*{Brief Explanation}
\begin{itemize}
    \item If the CPU modified a value in the cache ("dirty"), that value must be saved to main memory before the cache slot can be reused. This doubles the latency penalty.
\end{itemize}

\newpage
% Question 7
\subsection*{Question 7}
\subsubsection*{Row vs. Column Major}
You are iterating through a standard NumPy array using a nested loop. To maximize performance, which index should be in the \textbf{inner} loop?
\begin{enumerate}[a]
    \item The Row index (iterate top-down).
    \item The Column index (iterate left-right).
    \item It does not matter for NumPy.
    \item It depends on the size of the array.
\end{enumerate}

\subsubsection*{Answer: B}
\subsubsection*{Brief Explanation}
\begin{itemize}
    \item NumPy is \textbf{Row-Major}. Elements in the same row are stored next to each other. Iterating along the column index (moving left-to-right across a row) accesses contiguous memory, maximizing spatial locality.
\end{itemize}

% Question 8
\subsection*{Question 8}
\subsubsection*{Hierarchy Scale}
Approximately how much slower is a Random Disk Access compared to a Main Memory (RAM) Access?
\begin{enumerate}[a]
    \item 10x
    \item 100x
    \item 1,000x
    \item 100,000x (5 orders of magnitude)
\end{enumerate}

\subsubsection*{Answer: D}
\subsubsection*{Brief Explanation}
\begin{itemize}
    \item RAM access is $\sim$100 nanoseconds. Disk seek is $\sim$10 milliseconds. $10ms = 10,000,000ns$. The difference is roughly 5-6 orders of magnitude ($10^5$).
\end{itemize}

\newpage

% Question 9
\subsection*{Question 9}
\subsubsection*{Big Data Latency Chain}
When calculating $C = A \times B$ on a dataset larger than memory, which step dominates the execution time?
\begin{enumerate}[a]
    \item Computing the product in the ALU.
    \item Reading A and B from Disk.
    \item Allocating memory variables.
    \item The operating system scheduler.
\end{enumerate}

\subsubsection*{Answer: B}
\subsubsection*{Brief Explanation}
\begin{itemize}
    \item In Big Data, the bottleneck is moving data from slow storage (Disk) to fast memory. The CPU spends most of its time waiting for data (I/O Bound).
\end{itemize}

% Question 10
\subsection*{Question 10}
\subsubsection*{Data Structures}
Why does a Linked List generally exhibit poor performance compared to an Array for large sequential scans?
\begin{enumerate}[a]
    \item Linked Lists have $O(N^2)$ access time.
    \item Linked Lists use more memory for integers.
    \item Linked List nodes are scattered in memory, causing frequent Cache Misses.
    \item Arrays are always cached in L1 by default.
\end{enumerate}

\subsubsection*{Answer: C}
\subsubsection*{Brief Explanation}
\begin{itemize}
    \item Linked list nodes are allocated dynamically and linked via pointers, scattering them across memory pages. Traversing them requires loading many different pages, whereas an Array loads many elements in a single page (Spatial Locality).
\end{itemize}

\end{document}

--- end {week_01_guide.tex} ---
--- end {Week 1 Material} ---
--- {start Week 1 Quiz} ---
# Question 1
according to recent poll, data scientists spend most time
a) cleaning moving data (CORRECT ANSWER)
b) pickling features/models
c) deploying models in production
d) analyzing/presenting data

# Question 2
you are waiting in line at costco. what is costco's main concern
a) thoughput (CORRECT ANSWER)
b) latency

# Question 3
you are waiting in line at costco. what is your main concern
a) thoughput
b) latency (CORRECT ANSWER)

# Question 4
doing linear algebra in python is much slow than doing the same in matlab
a) not if you use numpy (CORRECT ANSWER)
b) yes it is slower
c) no python is faster than matlab
d) python is slower than matlab if memory is limited

# Question 5
why is SQL called a declaritive lanuage
a) because an SQL query defines what the answer should be not the squence of steps for reaching the answer (CORRECT ANSWER)
b) because it is easier to understand than procedural lanuages
c) because it can not be executed
d) because it specifies the excution path

# Question 6
what is the main technological change that enabled data science
a) digital storage (CORRECT ANSWER)
b) faster more powerful computer
c) multicore cpus
d) 10 Gb/s internet connections

# Question 7
table elements can have
a) differnt types of differnt columns (CORRECT ANSWER)
b) elements of different types
c) different types for different rows

# Question 8
types of table elements
a) have to support multiplication and addition
b) have to support addition
c) don't have to support either multiplication or addition (CORRECT ANSWER)

# Question 9
missing rows in a table
a) consume no storage (CORRECT ANSWER)
b) represented as rows of nulls

# Question 10
what does it mean when the conclusion from statistic analysis are said to be reproduciable
a) data collected from the same similar source yields the same conclusions (CORRECT ANSWER)
b) given the same input, you get the same output
c)it means that the notebook and data are put in a zip file and the same results are generated when run by someone else
d) running the same notebook many times, on random subsets of the data produces similar results

# Question 11
"The distribution of memory access latencies is oftern heavily tailed"
-explain this statement and why it is justified

as dicussed in class the latency of memory access has a heavy tail
suppose instead of considering single latencies we consider the sums of the latencies of disjoint sequences of 100 memory accesses each
suppose further the latencies of differnt accesses are independent
what can you say about the tails of the distribution of the sums?
a) has lighter tails than the individual latencies (CORRECT ANSWER)
b) has heavier tails than the individual latencies
c) both tails are equally heavy

EXPLAINIATION: 
- this is a direct consequence of the central limit theorem.
- the sum of independent random variables converges to the normal distribution which has exponential or light tails

# Question 12
caches sometimes have a dirty flag assicoiated with each page. the flag indicates wheather the page was changed since it was loaded in the cache. explain the reason for the flag and the effect it has on servicing cache misses
-explain

# Question 13
suppose a disk drive has seek latency of 10ms and a base throughput of 100MB/s
the term base throughput here means the rate at which the disk can read data once the reading head is in place
suppose your program reads block of data from random location on disk, give the effective throughput and the effective atency corresponding to the block sizes below
the term effective latency means the time from CPU issuing the read command to the disk to the time the disk block is avaiable in memory
the term efficive throughput means the number of MB that the CPU can read in a second

answering 
latency:ms
thoughput: MB/sec

10MB thoughput in MB/sec
1KB thoughput in MB/sec
10MB latency in ms

# Question 14
matrix elements
must have all the same type
can have different types for a) differnt columns (CORRECT ANSER)
b) can have differnt types for c) differnt rows

# Question 15
types of table elements don't have to support either multiplication or addition
a) must have different rows
b) don't have to support either multiplicaation or addition (CORRECT ANSWER)

# Question 16
if A and B are matrcies, A-B is well defined if
A and B have the same shapes (CORRECT ANSWER)
A and B are symmetric
the number of columns in A is equal to the number of rows in B

# Question 17
matrix A (n_rows,m_columns) and matrix B (j_rows,k_columns). what has to be equal for AxB and what is result of shape.
--- end {Week 1 Quiz} ---
--- {start Week 2 Material} ---
--- start{3.1_slides.pdf} ---
A short history of affordable
    massive computing.
Super computers

• Cray, Deep Blue, Blue Gene …
• Specialized hardware
• Extremely expensive
• created to solve specialized important problems
Data Centers
Data Centers
• The physical aspect of ”the cloud”
• Collection of commodity computers
• VAST number of computers (100,000’s)
• Created to provide computation for large and small organizations.
• Computation as a commodity.
Making History: Google 2003
• Larry Page and Sergey Brin develop a method for storing very large
  files on multiple commodity computers.
• Each file is broken into fixed-size chunks.
• Each chunk is stored on multiple chunk servers.
• The locations of the chunks is managed by the master
HDFS: Chunking files
                                                               File 1, Chunk 1
   File 1           File 1, Chunk 1   Copy   File 1, Chunk 1        Copy 2
            Split                                 Copy 1
                                                               File 1, Chunk 2
                    File 1, Chunk 2          File 1, Chunk 2        Copy 2
                                                  Copy 1
                                                                          File 1, Chunk 2
                                                                               Copy 3


                                                               File 2, Chunk 1
   File 2           File 2, Chunk 1   Copy   File 2, Chunk 1        Copy 2
            Split                                 Copy 1
                                                               File 2, Chunk 2
                    File 2, Chunk 2          File 2, Chunk 2        Copy 2
                                                  Copy 1
HDFS: Distributing Chunks

                   File 1, Chunk 1
 File 1, Chunk 1        Copy 2
      Copy 1
                   File 1, Chunk 2
 File 1, Chunk 2        Copy 2
      Copy 1
                        File 1, Chunk 2
                             Copy 3


                   File 2, Chunk 1
 File 2, Chunk 1        Copy 2
      Copy 1
                   File 2, Chunk 2
 File 2, Chunk 2        Copy 2
      Copy 1
Properties of GFS/HDFS
• Commodity Hardware: Low cost per byte of storage.
• Locality: data stored close to CPU.
• Redundancy: can recover from server failures.
• Simple abstraction: looks to user like standard file system (files,
  directories, etc.) Chunk mechanism is hidden.
Redundancy
  Locality
Task:
Sum all of the
elements in file 1
Map-Reduce
• HDFS is a storage abstraction
• Map-Reduce is a computation abstraction that works well with HDFS
• Allows programmer to specify parallel computation without knowing
  how the hardware is organized.
• We will describe Map-Reduce, using Spark, in a later section.
Spark
• Developed by Matei Zaharia , amplab, 2014
• Hadoop uses shared file system (disk)
• Spark uses shared memory – faster, lower latency.
• Will be used in this course

• Recall word count by sorting,
  we will redo it using map-reduce!
The Cloud
• The common name for data centers.
• What is better? Cloud or your local computers?
   • Cloud vs. Local: Rent vs. own: if we want a lot of power for a short time, it is
     cheaper to rent.
   • Centralized IT: shared staff, shared maintenance, shared upgrade.
   • Storage:
      • Long term – cloud storage (multiple TB) much more expensive than local.
      • Moving TB to/from cloud slow / expensive / physical (snowball)
   • Like a huge supermarket, there are many choices and it is not easy to find the
     best combination.
Summary
• Big data analysis is performed on large clusters of commodity
  computers. – computation as a service.
• HDFS (Hadoop file system): break down files to chunks, make copies,
  distribute randomly.
• Hadoop Map-Reduce: a computation abstraction that works well with
  HDFS
• Spark: Sharing memory instead of sharing disk.

--- end {3.1_slides.pdf} ---
--- start{3.1_slides.txt} ---
A short history of affordable
massive computing.

Super computers
• Cray, Deep Blue, Blue Gene …
• Specialized hardware
• Extremely expensive
• created to solve specialized important problems

Data Centers

Data Centers
• The physical aspect of ”the cloud”
• Collection of commodity computers
• VAST number of computers (100,000’s)
• Created to provide computation for large and small organizations.
• Computation as a commodity.

Making History: Google 2003
• Larry Page and Sergey Brin develop a method for storing very large
files on multiple commodity computers.
• Each file is broken into fixed-size chunks.
• Each chunk is stored on multiple chunk servers.
• The locations of the chunks is managed by the master

HDFS: Chunking files
File 1

Split

File 1, Chunk 1

Copy

File 1, Chunk 2

File 1, Chunk 1
Copy 1
File 1, Chunk 2
Copy 1

File 1, Chunk 1
Copy 2
File 1, Chunk 2
Copy 2
File 1, Chunk 2
Copy 3

File 2

Split

File 2, Chunk 1
File 2, Chunk 2

Copy

File 2, Chunk 1
Copy 1
File 2, Chunk 2
Copy 1

File 2, Chunk 1
Copy 2
File 2, Chunk 2
Copy 2

HDFS: Distributing Chunks

File 1, Chunk 1
Copy 1
File 1, Chunk 2
Copy 1

File 2, Chunk 1
Copy 1
File 2, Chunk 2
Copy 1

File 1, Chunk 1
Copy 2
File 1, Chunk 2
Copy 2

File 1, Chunk 2
Copy 3

File 2, Chunk 1
Copy 2
File 2, Chunk 2
Copy 2

Properties of GFS/HDFS
• Commodity Hardware: Low cost per byte of storage.
• Locality: data stored close to CPU.
• Redundancy: can recover from server failures.
• Simple abstraction: looks to user like standard file system (files,
directories, etc.) Chunk mechanism is hidden.

Redundancy

Locality
Task:
Sum all of the
elements in file 1

Map-Reduce
• HDFS is a storage abstraction
• Map-Reduce is a computation abstraction that works well with HDFS
• Allows programmer to specify parallel computation without knowing
how the hardware is organized.
• We will describe Map-Reduce, using Spark, in a later section.

Spark
• Developed by Matei Zaharia , amplab, 2014
• Hadoop uses shared file system (disk)
• Spark uses shared memory – faster, lower latency.
• Will be used in this course
• Recall word count by sorting,
we will redo it using map-reduce!

The Cloud
• The common name for data centers.
• What is better? Cloud or your local computers?
• Cloud vs. Local: Rent vs. own: if we want a lot of power for a short time, it is
cheaper to rent.
• Centralized IT: shared staff, shared maintenance, shared upgrade.
• Storage:
• Long term – cloud storage (multiple TB) much more expensive than local.
• Moving TB to/from cloud slow / expensive / physical (snowball)

• Like a huge supermarket, there are many choices and it is not easy to find the
best combination.

Summary
• Big data analysis is performed on large clusters of commodity
computers. – computation as a service.
• HDFS (Hadoop file system): break down files to chunks, make copies,
distribute randomly.
• Hadoop Map-Reduce: a computation abstraction that works well with
HDFS
• Spark: Sharing memory instead of sharing disk.


--- end {3.1_slides.txt} ---
--- start{3.1_transcript.txt} ---
(lighthearted music) (screen whooshing) - Okay, so before we get into Spark, I think it's useful to have
a little bit of history of this kind of highly parallel
and affordable computing. So non-affordable, high,
very fast computing has been around, usually
called supercomputing. And supercomputers,
there're a list of names of such famous computers, Cray, Deep Blue, Blue Gene, and they are machines that
use very specialized hardware to achieve a particular type
of computation efficiently. And because they use specialized hardware, specialized chips, they're very expensive, because chips, their
price depends very much on how many of them are being used. So those are used only in
that very specialized setup and those are created to make progress on very important highly
compute intensive problems. On the other hand, we have
what are called data centers. So this is a picture of data center on the outside and this is on the inside. You see that there're racks
and racks of computers. These computers are pretty much standard kind of commodity computers. And this is what is actually
called the cloud, right? So when we do our
computation in the cloud, when we use TikTok or we use Gmail, all of these things are being
done in data centers, okay? So those are collections
of commodity computers. So the computers that are in these centers are nothing special. They are more or less the
same as the kind of computers that you would have as
a workstation at home. And their main property
is that they are cheap and that they can do a lot of computation compared to their price. So the thing that makes
these data centers powerful is that you don't have one
or two or 100 computers but rather you have maybe
hundreds of thousands of computers in such a center. And that's why these
centers look like this. It's really a factory of computing and usually the factory
is put next to a river or some other way of cooling because these computers
create so much heat that needs to be taken out somewhere. So this is something that is
used to provide computation for large and small organizations and we're all using them all
the time in our daily life. So it's computation as a commodity. It's like you have electricity
coming into your home, you have water, you maybe have gas. This is another type of commodity that comes into your home and
provides you with computation. So how did this get started? The start of it was in Google in 2003, where Larry Page and Sergey
Brin, the founders of Google, were looking for a method
to store very large files on multiple commodity computers. So they were already collecting
large amounts of data and it was not cost-effective to store them in super large
and powerful computers. They wanted to use the computers that were around used by
people as workstations. So how do you do that? You take each file and
assume it's big file, let's say gigabytes, and you break it into fixed-size chunks. Okay, let's say 256 kilobyte. And then each chunk you store on multiple chunk servers, okay? So you now think about your
many commodity computers as basically each one storing a collection of chunks unrelated to what this computer is specifically designed to do. And the locations of the chunks are managed by a master node. So there is someplace in this cluster, that there is a head node. And that head node knows for
each chunk from each file where it is actually stored. So it looks something like this. It's called, it was
called Google File System and now it's called
the Hadoop File System. The files are broken into chunks, okay? So each file here is
broken into two chunks. And then you make several copies from each chunk, okay? So you have a file 1, chunk 2, and then another copy for file 1, chunk 2. So you have multiple
copies of the same data. Soon we'll see why. And so now you basically
have all of the information that was on the original files
but broken up into pieces and copied multiple times. And now you distribute the files, okay? So you have your master node that basically is going to
know where each chunk is, and you're going to take
each one of the chunks and put it on randomly selected computers. Okay, so now everything
is stored somewhere, but it's like a hologram. It's like every piece
is at a random place. So it's only the master that knows how to put them together. Okay, so what are the properties of this? First and most important, this is used commodity hardware. So you're using computers that are cheap. You have locality because you have data
stored next to a CPU. So every piece of data is stored somewhere in this distributed system
and it's close to some CPU that can immediately
access it and work on it. You have redundancy, so
you have the same piece of information, the same chunk, you have copies in multiple machines. And you have a simple abstraction, okay? So to you when you're working on the Hadoop File System, it just looks like a regular file systems. There are files and directories and sub-directories and so on. You don't need to worry about all of the different distributed chunks that is going on underneath. And you get redundancy. So that has to do with having
multiple copies of each chunk. Suppose that a particular
chunk server here crashed or some hardware problem
or software problem, doesn't matter, but it's
no longer accessible. Well, you don't need to worry about it in terms of the computation because every chunk that is here, you have another copy
of it on another machine that is still working, okay? So you can basically just say, okay, the computation I was doing
on this chunk server is dead. I lost that computation. But I still have the
data that is the input so I can start this junk
server to work on it. And when you have 100,000
computers in a data center, you have invariably,
at any moment of time, hundreds of computers
that are crashed, okay? So you basically or constantly need to kind of replace these computers. But this is done in a
very transparent way. You just take this computer out, you put a new computer that is blank, and then the system knows how to populate that new computer with chunks so that it has its own part of
the universe of file chunks. Locality is the other part which you get from the same piece. So suppose that you want
to do some computation such as sum all of the
elements in file 1, okay? So you have the elements in file 1, you have one here, you
have the second chunk here, you have one here, and you
have another piece here, and then you have one here, okay? So a bad solution would be to say, okay, this chunk server
should calculate the sum of everything that it has, okay? Then you'll have to compute
this sum and then this sum, and sum these things together. But you can instead have
this one, let's change the, so you have this one
be summed on this chunk and the chunk 2 for file 1, this one, be done on this machine. So they can work in parallel. And then as a result, you get the computation be done
in half the amount of time. Okay, so MapReduce is what we're going to talk about in Spark. So what we have in HDFS
is a storage abstraction. It's basically a storage abstraction. This storage abstraction
lets us access files as if they were in one place. And the underlying system takes care of the distribution. MapReduce is a computation
abstraction that works well with this distributed
file system abstraction. So now we are going to have
a computation language, a computation method that
works very well with the HDFS and which allows us to
basically compute things, write our program in a simple way, and have it automatically be distributed and running on many computers to reduce the amount of time. Okay, so it allows the programmer to specify parallel computation without knowing how the
hardware is organized. And this is super important. Why? Because you don't want
to write your program for a specific hardware, then it would mean that
every new hardware, every new cluster that you want to use, you have to tweak various
parameters inside your software. This software basically, this approach gives you an abstraction. You don't know what is
happening at the lowest level, but you can basically still
give efficient commands to the computer and they will be executed according to the particular hardware. So we will describe MapReduce using Spark in a later section. So what is Spark? So it was developed in
2014 by Matei Zaharia. And Hadoop is the older system. It uses a file system to do
the distribution storage. The Spark shares memory. So you have, instead of
sharing spaces on disks, you share memories. And so that is much faster and you can do computation
in a much faster way. And that's what we will be using. Okay, so what is the cloud? The cloud is a common
name for data centers. What is better, cloud
or your local computer? It's really a question
about renting versus buying. If you get computational
resources on the cloud, you usually pay per hour of use. So you don't have to pay
the whole cost of the thing. But if you need a lot of
computation all of the time, then it might be better to
own rather than to rent. You have centralized IT. So rather than having your
own IT team in your office, you have the IT team that is running, let's say, the Amazon cloud. And because they have so many computers that they're running together,
they're more efficient. They're less responsive to your requests because they have so many clients, but the price that you
pay is at a discount. So what about storage? Long-term cloud storage is
much more expensive than local. So for storage, for long-term
storage, let's say, years, you don't want to store
things on the cloud. And also, moving data from
the cloud and to the cloud is a very expensive
thing in its own right. So what you want to usually
do is have either applications that don't need a lot of storage or that you have the
storage on a rolling basis, that you store it for a
long-term on some other service, and then on the compute service, you just store what
you need at that point. So you can think about it
as like a huge supermarket. There are so many choices when
you go to the Amazon cloud. It's not easy to necessarily
find the best combination. So to summarize, big data
analysis is performed on large clusters of commodity computers, computation as a service. So this is basically, we
have a lot of computers in this location and we're
going to rent you some number of compute units for a
given amount of time. HDFS, the Hadoop File System, is a system that breaks files into chunks, makes copies, and then
distributes it across machines. And Hadoop MapReduce is
a computation abstraction that works well with
the Hadoop File System. Okay, so we haven't really
gone into MapReduce yet but we'll do that in the next videos.
--- end {3.1_transcript.txt} ---
--- start{3.2_notebook.md} ---
# Spark Basics 1

This notebook introduces two fundamental objects in Spark:

* The Spark Context
*  The Resilient Distributed DataSet or RDD

## Spark Context

We start by creating a **SparkContext** object named **sc**. In this case we create a spark context that uses 4 *executors* (one per core)

```python
import os
os.environ["PYSPARK_PYTHON"]="python3"
os.environ["PYSPARK_DRIVER_PYTHON"] = "python3"

#start the SparkContext
from pyspark import SparkContext 
sc = SparkContext(master="local[4]")
print(sc)

#RETURNS
# -> <SparkContext master=local[4] appName=pyspark-shell>
```

## Only one sparkContext at a time!
* Spark is designed for single user
* Only one sparkContext per program/notebook.
* Before starting a new sparkContext. Stop the one currently running

```python
# sc.stop() #commented out so that you don't stop your context by mistake
```

## RDDs

RDD (or Resilient Distributed DataSet) is the main novel data structure in Spark. You can think of it as a list whose elements are stored on several computers.

The elements of each RDD are distributed across the worker nodes which are the nodes that perform the actual computations. This notebook, however, is running on the Driver node. As the RDD is not stored on the driver-node you cannot access it directly. The variable name RDD is really just a pointer to a python object which holds the information regardnig the actual location of the elements.

## Some basic RDD commands

### Parallelize
* Simplest way to create an RDD.
* The method `A=sc.parallelize(L)`, creates an RDD named `A` from list `L`
* `A` is an RDD of type `PythonRDD`

```python
A=sc.parallelize(range(3))
A

# RETURNS
# -> PythonRDD[1] at RDD at PythonRDD.scala:48
```

### Collect
* RDD content is distributed among all executors.
* `collect()` is the inverse of `parallelize()`
* collects the elements of the RDD
* Returns a `list`

```python
L=A.collect()
print(type(L))
print(L)

# RETURNS
# -> <class 'list'>
# -> [0, 1, 2]
```

Using `.collect()` eliminates the benefits of parallelism

It is often tempting to `.collect()` and RDD, make it into a list, and then process the list using standard python. However, note that this means that you are using only the head node to perform the computation which means that you are not getting any benefit from spark.

Using RDD operations, as described below, will make use of all of the computers at your disposal.

### Map
* applies a given operation to each element of an RDD
* parameter is the function defining the operation.
* returns a new RDD.
* Operation performed in parallel on all executors.
* Each executor operates on the data local to it.

```python
A.map(lambda x: x*x).collect()

# RETURNS
# -> [0, 1, 4]
```

Note: Here we are using lambda functions, later we will see that regular functions can also be used.

### Reduce
* Takes RDD as input, returns a single value.
* Reduce operator takes two elements as input returns one as output.
* Repeatedly applies a reduce operator
* Each executor reduces the data local to it.
* The results from all executors are combined.

The simplest example of a 2-to-1 operation is the sum:

```python
A.reduce(lambda x,y:x+y)

# RETURNS
# -> 3
```

Here is an example of a reduce operation that finds the shortest string in an RDD of strings.

```python
words=['this','is','the','best','mac','ever']
wordRDD=sc.parallelize(words)
wordRDD.reduce(lambda w,v: w if len(w)<len(v) else v)

# RETURNS
# -> 'is'
```

### Properties of reduce operations

* Reduce operations must not depend on the order
    * Order of operands should not matter
    * Order of application of reduce operator should not matter
* Multiplication and summation are good:
$$1 + 3 + 5 + 2\quad 5 + 3 + 1 + 2$$
* Division and subtraction are bad:
$$1 - 3 - 5 - 2\quad 1 - 3 - 5 - 2$$

### Why must reordering not change the result?

You can think about the reduce operation as a binary tree where the leaves are the elements of the list and the root is the final result. Each triplet of the form (parent, child1, child2) corresponds to a single application of the reduce function.

The order in which the reduce operation is applied is determined at run time and depends on how the RDD is partitioned across the cluster. There are many different orders to apply the reduce operation.

If we want the input RDD to uniquely determine the reduced value all evaluation orders must must yield the same final result. In addition, the order of the elements in the list must not change the result. In particular, reversing the order of the operands in a reduce function must not change the outcome.

For example the arithmetic operations multiply `*` and add `+` can be used in a reduce, but the operations subtract `-` and divide `/` should not.

Doing so will not raise an error, but the result is unpredictable.

```python
B=sc.parallelize([1,3,5,2])
B.reduce(lambda x,y: x-y)

# RETURNS
# -> -9
```

Which of these the following orders was executed?
$$((1-3)-5)-2$$

or

$$(1-3)-(5-2)

### Using regular functions instead of lambda functions
* lambda function are short and sweet.
* but sometimes it's hard to use just one line.
* We can use full-fledged functions instead.

```python
A.reduce(lambda x,y: x+y)

# RETURNS
# -> 3
```

Suppose we want to find the

* last word in a lexicographical order
* among
* the longest words in the list.
We could achieve that as follows

```python
def largerThan(x,y):
    if len(x)>len(y): return x
    elif len(y)>len(x): return y
    else:  #lengths are equal, compare lexicographically
        if x>y: 
            return x
        else: 
            return y
        
wordRDD.reduce(largerThan)

# RETURNS
# -> 'this'
```

## Summary
We saw how to:

* Start a SparkContext
* Create an RDD
* Perform Map and Reduce operations on an RDD
* Collect the final results back to head node.

--- end {3.2_notebook.md} ---
--- start{3.2_slides.pdf} ---
MapReduce
  DSC 232R
Achieving locality by being oblivious to order
• To minimize cache misses we want to process data sequentially.
• To compute in parallel on several CPUs, we want processing in
  each CPU to be independent of the others.
• As a programmer, we want to achieve sequentiality and
  parallelism, without knowing the details of the hardware.
• Approach: write code that expresses the desired end result,
  without specifying how to get there.
• MapReduce: perform operations on arrays without specifying the
  order of the computation.
Map: square each item
• list L =[0,1,2,3]
• Compute the square of each item
• output: [0,1,4,9]
 Traditional            MapReduce

                        computation order is
                        not specified




compute from first to
last in order
Reduce: compute the sum
• A list L=[3,1,5,7]
• Find the sum (16)
 Traditional            MapReduce

                        computation order is
                        not specified




compute from first to
last in order
Map + Reduce
• list L=[0,1,2,3]
• Compute the sum of the squares
• Note the differences
  Traditional                         MapReduce


                                      computation order is not
                                      specified

                                      Execution plan
compute from first to last in order

Immediate execution
Order independence
• The result of map or reduce must not depend on the order
sum does not depend on computation order




           Result does not depend on order
difference depends on computation order




             Result depends on order
Computing the average incorrectly
Average = data.reduce(lambda a,b: (a+b)/2)

data=[1,2,3], average is 2
Computed Average = ((1+2/2+3)/2 = 2.25
Computing the average correctly
sum,count = data.map(lambda x: (x,1))
     .reduce(lambda P1,P2:
           (P1[0]+P2[0], P1[1]+P2[1]))

Average = sum/count

     [1,2,3].map(lambda x: (x,1)) = [(1,1),(2,1),(3,1)]
     sum, count = [(1,1),(2,1),(3,1)].reduced() = 6,3
     average = 6/3 = 2
     data=[1,2,3], average is 2
Why Order Independence?
• Computation order can be chosen by compiler/optimizer.
• Allows for parallel computation of sums of subsets.
   • Modern hardware calls for parallel computation but parallel
     computation is very hard to program.
• Using MapReduce programmer exposes to the compiler
  opportunities for parallel computation.
Spark and MapReduce
• MapReduce is the basis for many systems.
• For big data: Hadoop and Spark.

--- end {3.2_slides.pdf} ---
--- start{3.2_transcript.txt} ---
(light airy music) - [Instructor] So let's
start looking into map reduce and how we use it. So what is the idea of map reduce? The idea is to achieve locality. You remember locality as we talked about memory access by
being oblivious to order. Okay? So we want to give the
compiler essentially the opportunity to be efficient and make things local by telling by having the software not over
specify what it wants to do. Okay? So this minimizes cache misses and allows us to compute things in parallel on multiple computers without too much dependence
between the computers. So as a programmer, the
important thing is we don't need to know how
all this is achieved. This is achieved underneath what we see by the Spark operating
system and runtime system and this allows us to
write software that can run on multiple different types of hardware. So what is the approach? We're going to write code that expresses the desired end result. Okay, so the desired end result without specifying how
exactly to get there. Okay? So that is what's going
to give the compiler the freedom to do things more efficiently. So map reduce performs
operations on a raise without specifying the
order of the computation and spark will optimize the
order of the computation on the fly as it sees what
resources are available what CPUs are available. Okay, so let's start
with the map operation which is a very, very simple operation. We want to, we have a list of values 0, 1, 2, 3 and we want to
compute the square of each item. So we want to get a new list, 0, 1, 4,9. So each element here is
related to one element here but we don't really care about what order this would be executed. So how do we write that
in traditional Python? We can write it in the following way here. So basically we define a list and then for every element in our input
list we append i squared. So notice that here we're
defining the loop to go from the first to the
last element in the list. So we are defining an order we can also write it in this way but it doesn't really make a
difference to the execution. Again, we're just running it from the first element
till the last element. On the other hand in map reduce we write
something like this. We say here is the operation map and the parameter to the
operation is this function, sorry up to here that maps X to X times X. Okay, so that's the operation
and we want to do that on L. We're not saying anything about the order in which to execute it. So here is compute form first to the last and here the computation
order is not specified. Okay, so that leaves it makes it possible to
run some of the computation on one computer and some
of it on another computer. Reduce is an operation that reduces a list into a single number or single element. So here we have a list of 3, 1, 5, 7 and we
want to compute the sum. Okay? So this is 16 is the sum
of these four elements. So again, we have two ways to do it. This is a little tongue in cheek. This is basically just, we
have an operation called sum. So it does it, but if we really want to look
into our own implementation we have again a loop
that goes from the first to the last element in L and
adds that element into I. Okay, so one by one here
we have the map reduce way of operating it, which is to to map every two elements X
and Y into their sum X plus Y. Okay? But how to order what to how to organize these different sums. We don't say this is just a binary sum and we somehow want it to
operate on all of the list and make it smaller until
it becomes one element. So again, this is with an order specified and here the computation
order is not specified. Okay? And then we can do map plus reduce. So we can say that the
list is elements 0, 1, 2, 3 and we want to compute
the sum of the squares. So first we want to map every element to its square and then we
want to sum them, okay? So again, look at the differences between how you would do
it in a regular Python code versus how you would
do it using map reduce. So in the regular Python code we have this summation
variable S that we're going to add the elements to. We go through the elements
in the list in the loop from the first to the
last and every time we add to S this I times I,
or we can write it as a as a sum of reduction
of list comprehension. Okay? So that's the way to do it traditionally. In map reduce we write it more abstractly. We just say we want to reduce something and this something is a
map of the list, okay? So we take every element in
the list, map it to i squared and then we perform reduce on that summation, on that result, okay? At what order these things
are going to be done we don't say anything about it. So here we compute from
first to last in order and here computation
order is not specified. So you get the theme. So basically the idea
is I'm going to tell you what I want to compute but I'm not going to tell you anything about the order so that you
can optimize it in real time. So this is what we call
immediate execution. So the every command is executed as it is being read
essentially after compilation. And here there is no execution. This is basically we're
just saying this is our plan for what we want to compute and we haven't necessarily
computed anything yet. So order independence. So the result of the map
are reduced, must depend must not depend on the order must not depend on the order because if it does depend on the
order, then we don't know then then what we will get
out is not deterministic. So here is something that
we can look at summation. So in summation there is no relationship there's no dependence on the order. So what we get here is
that in the loop order in the regular python order,
we first add five and seven we get 12, we add to that
three, we get 15, we add one we get 16 and we get three, we get 19. But we can do things in a different order. So for instance here we're
going to start by adding seven and three to get 10 in the
same time we can add one and three to get four, right? So those are different
summation can be done the two can be done in parallel. Then we can add the 10
and the four to get 14. And finally we add to it five to get 19. So the result, the 19 is the
same independent of the order. And that's important because we want to be able
to optimize the execution in any way that we want. Okay? Suppose that we wanted to do in a similar way the difference. So we could write a a reduced command that
would take differences but then we are not guaranteed
to get the same results in different runs. So here is the order that
you get in Python five minus seven is minus two,
minus three is minus five minus one is minus six, and
minus three is minus nine. Okay? On the other hand, if we
take a different order seven minus three is four,
one minus three is minus two four minus minus two is six and
five minus six is minus one. So here we got minus nine
and here we got minus one. So that basically means
that reduce cannot work with differences, okay? We have to do something else. If we want to do differences for instance we can use positive and
negative numbers and then just adding them
would simulate differences. Okay? So let's look at how
to compute the average. So the average is reduce
of lambda of A and B of A plus B over two. So that seems reasonable
enough if you have two variables, two numbers A and B. If you take A plus B over two,
that gives you the average. But what happens if we do
this reduce on a longer list? So let's say that the data
is one and two and three the average we know is two. But now if we do the the map reduce we get one plus two over two. That gives us 1.5. We add to that three you get 4.5 and divided
by two we get 2.25. So we don't get the right answer, right? So even though this seems like
the right way to do things it isn't. So let me show you what is
the right way to do things. Okay? So here is the the right way to do it. Basically what you do is you do a map of X to X comma one, right? So you just basically map
each X to the pair X comma one and then you do the
reduce operation where you get these pairs. P one is a pair and P
two is a pair and you add the first part and you add the second part and that's the result. Okay? So what is going on here? At the end of that we get a
pair, which is a sum and a count and we take the ratio to get the average. Okay, so let's see an example. So suppose we are trying to
do the average of one and two and three again, and we are doing this map of Lambda X two X one. So all we get is one
comma one, two comma one, and three comma one, okay? Now If we look at the reduce,
what does that do? So basically what we get is that this one is added to this one and it added to this two
and is added to this three. So that gives us six here and then these ones are
added and they give us three. So what are these ones? These ones are simply the count
of the number of elements. As we go through the reduce we are counting the number of elements and we're computing there some and at the end we have the sum and the count of everything
and we can just take the ratio. Okay, so this requires a
little bit more thought in order to do it in the correct way. So map reduce can work on it. So why is order independence so important? Again, computation order can be chosen by the compiler optimizer. So suppose you don't
have just one CPU working on doing your summation. Suppose it's a summation
of a billion elements and you want to summit using 100 CPUs then each CPU is going
to do some of the work. And so you're not sure at which
order things will be done. This allows parallel computation. And so modern hardware is
something that we want to use parallel computation because there's so many
cores in any laptop. However, it's not easy to program, right? Because you need to think
about how exactly the the computation is going to be broken up. map reduce. The solution is to abstract
it to say I don't really care at what order you will do things. Okay, so map reduce the programmer exposes to the compiler opportunities
for parallel computation because I'm not telling you
exactly at what order to do and you can choose the order that is best. So spark and map reduce. So map reduce is a system that
has existed for a long time. It's kind of a programming
abstraction, but in big data it is comes
two main systems, Hadoop which was the older system and Spark which is the system that
we're going to learn. Okay, so I'll see you next time.
--- end {3.2_transcript.txt} ---
--- start{3.3_notebook.md} ---
# Dataframes

* Dataframes are a restricted sub-type of RDDs.
* Restricting the type allows for more optimization.
* Dataframes store two dimensional data, similar to the type of data stored in a spreadsheet.
    * Each column in a dataframe can have a different type.
    * Each row contains a record.
* Similar to pandas dataframes and R dataframes

```python
#import findspark
#findspark.init()
from pyspark import SparkContext
import os

os.environ["PYSPARK_PYTHON"]="python3"
os.environ["PYSPARK_DRIVER_PYTHON"] = "python3"

sc = SparkContext(master="local[4]")
sc.version

# RETURNS
# -> '3.5.0'
```

```python
import os
import sys

from pyspark import SparkContext
from pyspark.sql import SQLContext
from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType
%pylab inline

# RETURNS
# -> %pylab is deprecated, use %matplotlib inline and import the required libraries.
# -> Populating the interactive namespace from numpy and matplotlib
```

```python
# Just like using Spark requires having a SparkContext, using SQL requires an SQLContext
sqlContext = SQLContext(sc)
sqlContext

# RETURNS
# -> <pyspark.sql.context.SQLContext at 0x7fffd88057d0>
```

## Spark sessions
[A newer API for spark dataframes](https://spark.apache.org/docs/latest/sql-getting-started.html#starting-point-sparksession)

We will stick to the old API in this class.

A new interface object has been added in Spark 2.0 called SparkSession. A spark session is initialized using a builder

```python
spark = SparkSession.builder \
         .master("local") \
         .appName("Word Count") \
         .config("spark.some.config.option", "some-value") \
         .getOrCreate()
```

Using a SparkSession a Parquet file is read as follows:

```python
df = spark.read.parquet('python/test_support/sql/parquet_partitioned')
```

## Constructing a DataFrame from an RDD of Rows

Each Row defines it's own fields, the schema is inferred.

```python
# One way to create a DataFrame is to first define an RDD from a list of Rows 
_list=[Row(name=u"John", age=19),
       Row(name=u"Smith", age=23),
       Row(name=u"Sarah", age=18)]
some_rdd = sc.parallelize(_list)
some_rdd.collect()

# RETURNS
# -> [Row(name='John', age=19), Row(name='Smith', age=23), Row(name='Sarah', age=18)]
```

```python
# The DataFrame is created from the RDD or Rows
# Infer schema from the first row, create a DataFrame and print the schema
some_df = sqlContext.createDataFrame(_list)
some_df.printSchema()

# RETURNS
# -> root
# ->  |-- name: string (nullable = true)
# ->  |-- age: long (nullable = true)
```

```python
# A dataframe is an RDD of rows plus information on the schema.
# performing **collect()* on either the RDD or the DataFrame gives the same result.
print(type(some_rdd),type(some_df))
print('some_df =',some_df.collect())
print('some_rdd=',some_rdd.collect())

# RETURNS
# -> <class 'pyspark.rdd.RDD'> <class 'pyspark.sql.dataframe.DataFrame'>
# -> some_df = [Row(name='John', age=19), Row(name='Smith', age=23), Row(name='Sarah', age=18)]
# -> some_rdd= [Row(name='John', age=19), Row(name='Smith', age=23), Row(name='Sarah', age=18)]
```

## Defining the Schema explicitly

The advantage of creating a DataFrame using a pre-defined schema allows the content of the RDD to be simple tuples, rather than rows.

```python
# In this case we create the dataframe from an RDD of tuples (rather than Rows) and provide the schema explicitly
another_rdd = sc.parallelize([("John", 19), ("Smith", 23), ("Sarah", 18)])
# Schema with two fields - person_name and person_age
schema = StructType([StructField("person_name", StringType(), False),
                     StructField("person_age", IntegerType(), False)])
  
# Create a DataFrame by applying the schema to the RDD and print the schema
another_df = sqlContext.createDataFrame(another_rdd, schema)
another_df.printSchema()
# root
#  |-- age: binteger (nullable = true)
#  |-- name: string (nullable = true)

# RETURNS
# -> root
# ->  |-- person_name: string (nullable = false)
# ->  |-- person_age: integer (nullable = false)
```

## Loading DataFrames from disk

There are many maethods to load DataFrames from Disk. Here we will discuss three of these methods

* Parquet
* JSON (on your own)
* CSV (on your own)

In addition, there are API's for connecting Spark to an external database. We will not discuss this type of connection in this class

### Loading dataframes from JSON files

JSON is a very popular readable file format for storing structured data. Among it's many uses are twitter, javascript communication packets, and many others. In fact this notebook file (with the extension .ipynb is in json format. JSON can also be used to store tabular data and can be easily loaded into a dataframe.

```python
# when loading json files you can specify either a single file or a directory containing many json files.
print('--- json file')
path = "../Data/people.json"
!cat $path 

# Create a DataFrame from the file(s) pointed to by path
people = sqlContext.read.json(path)
print('\n--- dataframe\n people is a',type(people))
# The inferred schema can be visualized using the printSchema() method.
people.show()

print('--- Schema')
people.printSchema()

# RETURNS
"""
--- json file
{"name":"Michael"}
{"name":"Andy", "age":30}
{"name":"Justin", "age":19}

--- dataframe
 people is a <class 'pyspark.sql.dataframe.DataFrame'>
+----+-------+
| age|   name|
+----+-------+
|NULL|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+

--- Schema
root
 |-- age: long (nullable = true)
 |-- name: string (nullable = true)
"""
```

### Excercise: Loading csv files into dataframes
Spark 2.0 includes a facility for reading csv files. In this excercise you are to create similar functionality using your own code.

You are to write a class called csv_reader which has the following methods:

`__init__(self,filepath)`: recieves as input the path to a csv file. It throws an exeption `NoSuchFile` if the file does not exist.
`Infer_Schema()` opens the file, reads the first 10 lines (or less if the file is shorter), and infers the schema. The first line of the csv file defines the column names. The following lines should have the same number of columns and all of the elements of the column should be of the same type. The only types allowd are `int`,`float`,`string`. The method infers the types of the columns, checks that they are consistent, and defines a dataframe schema of the form:

```python
schema = StructType([StructField("person_name", StringType(), False),
                     StructField("person_age", IntegerType(), False)])
```

If everything checks out, the method defines a `self`. variable that stores the schema and returns the schema as it's output. If an error is found an exception `BadCsvFormat` is raised.

`read_DataFrame()`: reads the file, parses it and creates a dataframe using the inferred schema. If one of the lines beyond the first 10 (i.e. a line that was not read by `InferSchema`) is not parsed correctly, the line is not added to the Dataframe. Instead, it is added to an RDD called `bad_lines`. The methods returns the dateFrame and the `bad_lines` RDD.

### Parquet files

* Parquet is a popular columnar format.
* Spark SQL allows SQL queries to retrieve a subset of the rows without reading the whole file.
* Compatible with HDFS : allows parallel retrieval on a cluster.
* Parquet compresses the data in each column.
`<reponame>.parquet` is usually a directory with many files or subdirectories.

### Spark and Hive

* Parquet is a file format not an independent database server.
* Spark can work with the Hive relational database system that supports the full array of database operations.
* Hive is compatible with HDFS.

```python
dir='../Data'
parquet_file=dir+"/users.parquet"
!ls $dir

# RETURNS
# -> Moby-Dick.txt  namesAndFavColors.parquet  people.json  users.parquet  Weather
```

```python
#load a Parquet file
print(parquet_file)
df = sqlContext.read.load(parquet_file)
df.show()

# RETURNS
# ->
"""
../Data/users.parquet
+------+--------------+----------------+
|  name|favorite_color|favorite_numbers|
+------+--------------+----------------+
|Alyssa|          NULL|  [3, 9, 15, 20]|
|   Ben|           red|              []|
+------+--------------+----------------+
"""
```

```python
df2=df.select("name", "favorite_color")
df2.show()

# RETURNS
# -> 
"""
+------+--------------+
|  name|favorite_color|
+------+--------------+
|Alyssa|          NULL|
|   Ben|           red|
+------+--------------+
"""
```

```python
outfilename="namesAndFavColors.parquet"
!rm -rf $dir/$outfilename
df2.write.save(dir+"/"+outfilename)
!ls -ld $dir/$outfilename

# RETURNS
# -> drwxr-xr-x 6 jovyan users 192 Apr 14 17:52 ../Data/namesAndFavColors.parquet
```

## Lets have a look at a real-world dataframe

This dataframe is a small part from a large dataframe (15GB) which stores meteorological data from stations around the world.

```python
from os.path import split,join,exists
from os import mkdir,getcwd,remove
from glob import glob

# create directory if needed
notebook_dir=getcwd()
data_dir=join(split(notebook_dir)[0],'Data')
weather_dir=join(data_dir,'Weather')

file_index='NY'
zip_file='%s.tgz'%(file_index)

weather_parquet = join(weather_dir, zip_file[:-3]+'parquet')
print(weather_parquet)
df = sqlContext.read.load(weather_parquet)
df.show(1)

# RETURNS
# ->
"""
/home/jovyan/Library/CloudStorage/GoogleDrive-ssingal@ucsd.edu/My Drive/UCSD/Notes/6th Quarter - Spring 24/DSC 232R - BDA with Spark/GitHub/lecture-notebooks/Data/Weather/NY.parquet
+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+
|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|
+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+
|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|
+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+
only showing top 1 row
"""
```

```python
#selecting a subset of the rows so it fits in slide.
df.select('station','year','measurement').show(5)

# RETURNS
# ->
"""
+-----------+----+-----------+
|    station|year|measurement|
+-----------+----+-----------+
|USW00094704|1945|   PRCP_s20|
|USW00094704|1946|   PRCP_s20|
|USW00094704|1947|   PRCP_s20|
|USW00094704|1948|   PRCP_s20|
|USW00094704|1949|   PRCP_s20|
+-----------+----+-----------+
only showing top 5 rows
"""
```

## Summary

* Dataframes are an efficient way to store data tables.
* All of the values in a column have the same type.
* A good way to store a dataframe in disk is to use a Parquet file.
* Next: Operations on dataframes.

--- end {3.3_notebook.md} ---
--- start{3.3_slides.pdf} ---
               Spark Basics
• Spark Context
• Resilient Distributed Dataset (RDD)
Spark Context
• Spark is complex distributed software.
• The python interface to spark is called pyspark
• SparkContext is a python class, defined as part of pyspark which
  manages the communication between the user's program and spark.
• We start by creating a SparkContext object named sc.
Resilient Distributed DataSets (RDDs)
     Driver Node
                                Worker Node1 (CPU)
                                                                       Worker Node3 (CPU)
Driver Program                        Executor 1
   (Python)                                     Worker Node2 (CPU)          Executor 3
                                 RDD1          RDD2
                               partitions1   partitions1Executor 2     RDD1          RDD2
                                                                     partitions3   partitions3
   RDD1          RDD2
                                                 RDD1          RDD2
                                               partitions2   partitions2
    Spark Context
       (Scala)
                        1. Driver node
                        2. Driver Program
                        3. Spark Context
                        4. RDDs
                        5. Partitions
                        6. Executors
Another example:
Find the shortest word in a list
Summary
• Spark - Context
• RDDs
• Map
• Reduce
• More details, and excercises, in the jupyter notebook.
• Next time: more about RDDs

--- end {3.3_slides.pdf} ---
--- start{3.3_transcript.txt} ---
(gentle music) (slides swooshing) - So we talked last time about MapReduce, and how it is used in the abstract. And now we're going to look specifically, how this is done in Spark. So we'll start to know how to actually use MapReduce within Spark. So we're going to
describe two main object. One is the SparkContext
and the other is the RDD, or the Resilient Distributed
Dataset (sniffles). So what is the SparkContext? - [Instructor] Spark
itself is a complicated distributed software that
runs on many computers at the same time with all
kinds of communication. The way that you want
to communicate with it, is to use MapReduce, and similar commands that abstract all of this complexity. So the python interface to
Spark is called pyspark, and the SparkContext is a python class, which is defined as part of pyspark, and manages the communication between the user program and Spark. So we start Spark program, or at least the part
that really uses Spark with creating a SparkContext object. And here, we call it sc. So this is the command. We import from pyspark, we import the SparkContext, and we define sc to be a SparkContext. And we say here as a parameter, optional parameter that we want it to run on three chords, three of
the chords that we have. Okay, and then we say, "Print out sc." So it tells us what it is. Okay, so it is a pyspark
context, SparkContext (sniffles). RDDs are a more complex
object to think about. - [Instructor] It's basically how you, you the program thinks about
storage that is distributed in many computers. Okay, so the RDD is this kind of storage that is not necessarily on the computer that the python program is running on, but it is distributed
on many other computers. Okay, so what we have is the Driver node. So this is, in general, the node where the controls, the other slave nodes or
worker nodes (sniffles). And in this node, we have our python program that we wrote. And then through the SparkContext, this is the SparkContext that we created, we communicate with the
other computers, okay? We don't communicate with them directly, we just give general commands
to the SparkContext, okay? And what we have as storage is RDD, and RDD two, RDD one, and RDD two. And these are two arrays, if you want to think about it. And what exists on our Driver
node is just a pointer. It's just a place that
says, "Here is where you can get these parts that
are actually distributed." So they're distributed
in the following way: (instructor sniffles) Each RDD is broken into Partition. So here's RDD one, Partition
one, RDD one, Partition two, and RDD one, Partition
three is hiding behind here, and similarly for RDD two. Okay, so each one of their arrays, think about it, it's a very big array, something maybe like a hundred gigabyte. And each one of them is
stored on multiple computers, broken up into pieces just
like the Google File System, or the HDFS. Okay, and (sniffles) when you
want, when the program here wants to do some operation,
like a Map or Reduce on one of the RDDs, it
basically goes through here. And the Spark system
through the SparkContext, basically guides what the
worker nodes should do with their own RDDs and
what they should communicate back or to each other. Okay, so what did we talk about? We talked about (sniffles)
the Driver node, the node that is the head node. Then we have the Driver program, which is really the
program that you write. We have the SparkContext, which you can think about as a conduit, or a bridge, or a representative between your program and the Spark system. RDDs are these distributed data arrays. Partitions are each one of the parts, and Executors that I guess
I haven't talked about yet, Oh, here they are, Executor one and Executor two, that's the CPU that is
going to work with this RDD. So this RDD one is local to Executor one. So it can work with it quickly, fast, just like locality that we talked about in regular computer
systems (sniffles). So how do you create an RDD? Well, the simplest way (sniffles), is to take a list of elements, and say to Spark, "I want you
to make this into an RDD." So here it is, here is the list
of elements zero, one, two. And then I call the SparkContext, and tell it, "To parallelize this." Okay, so now I have A as
the pointer to an RDD. And here is the description of it. RDD collection in parallelize and so on. Okay, so basically this A stores a pointer to this distributed array. Now of course this is a tiny array, so there's no point of doing it, but suppose it was instead of three, it was 3 million or 3 billion? Collect is the opposite of parallelize. And in collect we take the RDD content from all of the different Executors, and bring it to the head node, or to the place where
our python program runs. And what it generates is a list, okay? So it's just the reverse of parallelize. So if we do A collect is L. What is L? L is a list. And it has exactly what we
put originally into the RDD. Okay, so nothing surprising here. Okay, what is map? So Map is, we talked about it last time, but it applies a given operation to each element of an RDD. Okay, so that's what we want to execute. And the parameter that this
map gets is a function, that defines the operation. Okay, so it's common to
use an anonymous function. So on a function that doesn't have a name, it just lambda function that
maps x to x squared, okay? And so what we do here
is we take an RDD A, we perform the map operation, and then we do collect
to get it as a list, so we can look at it. Remember, if you don't do a collect, what you have is just a
pointer to an RDD (sniffles), and you can't look at that, or see any part of it directly. Reduce is similar, is as
we talked about before, we want to take any two elements, and we want to combine them. And this is done on each one
of the Executors separately. And then the results are
combined into the head node. Okay, so we really get
a parallel computation, because each part of the data is going to be reduced
separately on its own Executor. Okay, so here's how it looks. We take the same A, we perform reduce where we take x and y, and we add x and y. Okay, so this is summation,
which we talked about before. And the sum of zero, one,
two, is indeed three. Reduce generates a single number, so that you don't need to collect. It reduces automatically
something that gives you a single element that is in the head node. Okay, here's another example: So suppose we have a list of words, this is the best Mac ever, okay? And we make it into wordRDD, so an RDD can have any
kind of elements in it, even of different types. It's just like a list,
it just distributed. And here we have an RDD of words, and now we do a reduce
that is a little different. We basically get w,v to two words, and we output w, if the length of w is smaller than the length of v and
otherwise we output v. So what are we doing? For a repair, we output
for a repair of words, we output the shorter word. Okay, so if we run that on this list, we get as an output is, okay? That's the shortest word
in this, in this sentence. (instructor sniffles) Sometimes you want to
use regular expressions instead of lambda functions. So sometimes you have something
relatively complicated that you want to do in
terms of a map or a reduce. And so lambda functions are nice, because they are compact, and they make your code look shorter, but sometimes it's hard to use. And so we can use full-fledged
functions instead. And so here's an example (sniffles). So suppose what we want to find in an RDD, is the last word in lexicographical order among the longest words in the list, okay? So let's say that if there's
just one longest word, then you output that. But if there are several, you want to choose the one that is latest in the lexicographical order. So how would you do that? So here's the little
program that takes x and y. (instructor sniffles) It checks first for the length of y, if x is larger than y, then return x. If y is larger than x, it returns y. And if the lengths are equal, then it keeps the one that is later in the lexicographical order. Okay, and then this function is given as a parameter into the reduce. Okay, so you can write
quite complicated things to be executed inside the reduce. (instructor sniffles) All right, so to summarize, we talked about the SparkContext, which is the bridge
between your Spark program your pythons program, and
the Spark operating system. RDDs, which is the way that we abstract the distributed lists, if you will, map, which is the operation of
doing an individual operation on each one element of the RDD, and reduce which reduces the
whole RDD into one element. So we're going to look at more details, and exercises in the Jupyter notebooks. So that's where we will go next. See you then.
--- end {3.3_transcript.txt} ---
--- start{3.4_notebook.md} ---
# Spark Basics 2

## Chaining

We can chain transformations and aaction to create a computation pipeline

Suppose we want to compute the sum of the squares
$$\Sigma_{i=1}^{n} x_{i}^{2}$$
where the elements $x_i$ are stored in an RDD.

```python
#start the SparkContext
import os
os.environ["PYSPARK_PYTHON"]="python3"
os.environ["PYSPARK_DRIVER_PYTHON"] = "python3"

from pyspark import SparkContext
sc = SparkContext(master="local[4]")
print(sc)

# RETURNS
# -> <SparkContext master=local[4] appName=pyspark-shell>
```

### Create an RDD

```python
B=sc.parallelize(range(4))
B.collect()

# RETURNS
# -> [0, 1, 2, 3]
```

### Sequential syntax for chaining

Perform assignment after each computation

```python
Squares=B.map(lambda x:x*x)
Squares.reduce(lambda x,y:x+y) 

# RETURNS
# -> 14
```

### Cascaded syntax for chaining

Combine computations into a single cascaded command

```python
B.map(lambda x:x*x)\
   .reduce(lambda x,y:x+y)

# RETURNS
# -> 14
```

### Both syntaxes mean exactly the same thing
The only difference:

* In the sequential syntax the intermediate RDD has a name Squares
* In the cascaded syntax the intermediate RDD is anonymous

The execution is identical!

### Sequential execution

The standard way that the map and reduce are executed is

* perform the map
* store the resulting RDD in memory
* perform the reduce

### Disadvantages of Sequential execution

* Intermediate result (Squares) requires memory space.
* Two scans of memory (of B, then of Squares) - double the cache-misses.

### Pipelined execution

Perform the whole computation in a single pass. For each element of B

    * Compute the square
    * Enter the square as input to the reduce operation.

## Advantages of Pipelined execution

    * Less memory required - intermediate result is not stored.
    * Faster - only one pass through the Input RDD.

## Lazy Evaluation

This type of pipelined evaluation is related to Lazy Evaluation. The word Lazy is used because the first command (computing the square) is not executed immediately. Instead, the execution is delayed as long as possible so that several commands are executed in a single pass.

The delayed commands are organized in an Execution plan

For more on Pipelined execution, Lazy evaluation and Execution Plans see spark programming guide/RDD operations

## An instructive mistake

Here is another way to compute the sum of the squares using a single reduce command. Can you figure out how it comes up with this unexpected result?

```python
C=sc.parallelize([1,1,2])
C.reduce(lambda x,y: x*x+y*y)

# RETURNS
# -> 8
```

### Answer:

* `reduce` first operates on the pair $(1,1)$ , replacing it with $1^{2}+1^{2} = 2$ 
reduce then operates on the pair $(2,2)$ , giving the final result $2^{2}+2^{2} = 8$ 

## getting information about an RDD

RDD's typically have hundreds of thousands of elements. It usually makes no sense to print out the content of a whole RDD. Here are some ways to get manageable amounts of information about an RDD

Create an RDD of length n which is a repetition of the pattern 1,2,3,4

```python
n=1000000
B=sc.parallelize([1,2,3,4]*int(n/4))

#find the number of elements in the RDD
B.count()

# RETURNS
# -> 1000000
```

```python
# get the first few elements of an RDD
print('first element=',B.first())
print('first 5 elements = ',B.take(5))

# RETURNS
# ->
"""
first element= 1
first 5 elements =  [1, 2, 3, 4, 1]
"""
```

### Sampling an RDD

* RDDs are often very large.
* Aggregates, such as averages, can be approximated efficiently by using a sample.
* Sampling is done in parallel and requires limited computation.

The method `RDD.sample(withReplacement,p)` generates a sample of the elements of the RDD. where

* `withReplacement` is a boolean flag indicating whether or not a an element in the RDD can be sampled more than once.
* `p` is the probability of accepting each element into the sample. Note that as the sampling is performed independently in each partition, the number of elements in the sample changes from sample to sample.

```python
# get a sample whose expected size is m
# Note that the size of the sample is different in different runs
m=5.
print('sample1=',B.sample(False,m/n).collect()) 
print('sample2=',B.sample(False,m/n).collect())

# RETURNS
# ->
"""
sample1= [4, 1, 3, 2, 1, 1, 1]
sample2= [2, 2, 2, 1, 3, 2, 2]
"""
```

### Things to note and think about

* Each time you run the previous cell, you get a different estimate
* The accuracy of the estimate is determined by the size of the sample $n \cdot p$
See how the error changes as you vary 
* Can you give a formula that relates the variance of the estimate to $(p \cdot n)$ ? 
    * *note: (The answer is in the Probability and statistics course).*

### filtering an RDD

The method `RDD.filter(func)` Return a new dataset formed by selecting those elements of the source on which func returns true.

```python
print('the number of elements in B that are > 3 =',B.filter(lambda n: n > 3).count())

# RETURNS
# -> the number of elements in B that are > 3 = 250000
```

### Removing duplicate elements from an RDD
The method `RDD.distinct()` Returns a new dataset that contains the distinct elements of the source dataset.

This operation requires a shuffle in order to detect duplication across partitions.

```python
# Remove duplicate element in DuplicateRDD, we get distinct RDD
DuplicateRDD = sc.parallelize([1,1,2,2,3,3])
print('DuplicateRDD=',DuplicateRDD.collect())
print('DistinctRDD = ',DuplicateRDD.distinct().collect())

# RETURNS
# ->
"""
DuplicateRDD= [1, 1, 2, 2, 3, 3]
DistinctRDD =  [1, 2, 3]
"""
```

### flatmap an RDD
The method `RDD.flatMap(func)` is similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item).

```python
text=["you are my sunshine","my only sunshine"]
text_file = sc.parallelize(text)
# map each line in text to a list of words
print('map:',text_file.map(lambda line: line.split(" ")).collect())
# create a single list of words by combining the words from all of the lines
print('flatmap:',text_file.flatMap(lambda line: line.split(" ")).collect())

# RETURNS
# -> 
"""
map: [['you', 'are', 'my', 'sunshine'], ['my', 'only', 'sunshine']]
flatmap: ['you', 'are', 'my', 'sunshine', 'my', 'only', 'sunshine']
"""
```

### Set operations

In this part, we explore set operations including union,intersection,subtract, cartesian in pyspark

```python
rdd1 = sc.parallelize([1, 1, 2, 3])
rdd2 = sc.parallelize([1, 3, 4, 5])
```

1. union(other)
* Return the union of this RDD and another one.
* Note that that repetitions are allowed. The RDDs are bags not sets
* To make the result a set, use .distinct

```python
rdd2=sc.parallelize(['a','b',1])
print('rdd1=',rdd1.collect())
print('rdd2=',rdd2.collect())
print('union as bags =',rdd1.union(rdd2).collect())
print('union as sets =',rdd1.union(rdd2).distinct().collect())

# RETURNS
# ->
"""
rdd1= [1, 1, 2, 3]
rdd2= ['a', 'b', 1]
union as bags = [1, 1, 2, 3, 'a', 'b', 1]
union as sets = [1, 'a', 2, 3, 'b']
"""
```

2. intersection(other)

* Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did.Note that this method performs a shuffle internally.

```python
rdd2=sc.parallelize([1,1,2,5])
print('rdd1=',rdd1.collect())
print('rdd2=',rdd2.collect())
print('intersection=',rdd1.intersection(rdd2).collect())

# RETURNS
# ->
"""
rdd1= [1, 1, 2, 3]
rdd2= [1, 1, 2, 5]
intersection= [1, 2]
"""
```

3. subtract(other, numPartitions=None)

* Return each value in self that is not contained in other.

```python
print('rdd1=',rdd1.collect())
print('rdd2=',rdd2.collect())
print('rdd1.subtract(rdd2)=',rdd1.subtract(rdd2).collect())

# RETURNS
# ->
"""
rdd1= [1, 1, 2, 3]
rdd2= [1, 1, 2, 5]
rdd1.subtract(rdd2)= [3]
"""
```

4. cartesian(other)

* Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of elements (a, b) where a is in self and b is in other.

```python
rdd2=sc.parallelize([1,1,2])
rdd2=sc.parallelize(['a','b'])
print('rdd1=',rdd1.collect())
print('rdd2=',rdd2.collect())
print('rdd1.cartesian(rdd2)=\n',rdd1.cartesian(rdd2).collect())

# RETURNS
# ->
"""
rdd1= [1, 1, 2, 3]
rdd2= ['a', 'b']
rdd1.cartesian(rdd2)=
 [(1, 'a'), (1, 'b'), (1, 'a'), (1, 'b'), (2, 'a'), (2, 'b'), (3, 'a'), (3, 'b')]
"""
```

## Summary
* Chaining: creating a pipeline of RDD operations.
* counting, taking and sampling an RDD
* More Transformations: `filter`, `distinct`, `flatmap`
* Set transformations: `union`, `intersection`, `subtract`, `cartesian`

--- end {3.4_notebook.md} ---
--- start{3.4_slides.pdf} ---
Basic Spark 2 – Part 1
        DSC 232R
1.1 Chaining
We can chain transformations and actions to create a computation
pipeline
Suppose we want to compute the sum of the squares
                               𝑛

                              ෍ 𝑥2𝑖
                              𝑖=1


where the elements xi are stored in an RDD.
1.1.1 Create an RDD
1.1.2 Sequential syntax for chaining
Perform assignment after each computation
1.1.3 Cascaded syntax for chaining
Combine computations into a single cascaded command
1.1.4 Both syntaxes mean exactly the same
thing
The only difference:

   • In the sequential syntax the intermediate RDD has a name
     Squares
   • In the cascaded syntax the intermediate RDD is anonymous

The execution is identical!
1.1.5 Sequential execution
The way MapReduce are executed by a standard system (not
Spark!)

  • perform the map
  • store the resulting RDD in memory
  • perform the reduce

1.1.6 Disadvantages of Sequential execution
 1. Intermediate result (Squares) requires memory space.
 2. Two scans of memory (of B, then of Squares) – double the
    cache-misses.
1.1.7 Pipelined execution
Spark performs the whole computation in a single pass. For each
element of B

  1. Compute the square.
  2. Enter the square as input to the reduce operation.


1.1.8 Advantages of Pipelined execution
  1. Less memory required – intermediate result is not stored.
  2. Faster – only one pass through the Input RDD.
1.1.9 Lazy Evaluation
This type of pipelined evaluation is related to Lazy Evaluation. The
word Lazy is used because the first command (computing the
square) is not executed immediately. Instead, the execution is
delayed as long as possible so that several commands are
executed in a single pass.

The delayed commands are organized in an Execution plan.

For more on Pipelined execution, Lazy evaluation and Execution
Plans see spark programming guide/RDD operations
    1.1.10 An instructive mistake
    Here is another way to compute the sum of the squares using a
    single reduce command. Can you figure out how it comes up with
    this unexpected result?




     1.1.10.1 Answer:
1. reduce first operates on the pair (1,1), replacing it with 1 2 + 12 = 2
2. reduce then operates on the pair (2,2), giving the final result 2 2 + 22 = 8

--- end {3.4_slides.pdf} ---
--- start{3.4_transcript.txt} ---
- ♪ Music ♪ - Hi. In the previous videos I told you about the map use operation and I told you about the R D D. So those are the
distributed data structure that that are used at
the basic unit in Spark. That's the basic type of data structure. It is done now to start
to see what we can do with R D D and how we operate on. So the first thing I'd like
to talk about is chaining. Chaining is a way to
combine different operation into a sequence that is
executed or a pipeline. So suppose that we want
to compute something like the sum of the
squares of some variables. And the elements Xi are already
stored for us in an R D D. So we're going to first create the R D D. So what we're going to
do is create a list zero one, two three, and
then call the operation paralyzed to create the R D D called B. So we have two waves of chaining these two operations X squared taking the square of every
element and then taking the sum One way is the sequential
chaining where we say that squares is an R D D that is equal to B dot map of lambda,
X goes to X square. Okay? And then we take
squares, that's new R D D and we take the operation
reduce on that square. Okay? So, we take the operation reduce and then we get the
final result which is 14. Okay? We have a different
way of writing this which is called cascaded and that's just to not create
this intermediate R D D but simply to perform a map operation on B and then perform a reduce on the result without
giving the result any name. Okay? So that's sometimes
a more succinct way to write things. Now the important thing to realize that both things mean
exactly the same thing. So even though it seems to us that as we're writing it that
things are written sequentially so they would be executed sequentially that is not the case. Though they're going to be executed in an order that is determined
by the spark system. The difference is really that in this case the R D D is anonymous, right? So the R D D in the middle
doesn't have a name. We can't refer to it. So let's talk about the execution. So first let's think about the
execution that would happen in a standard map produced
system that did not spark that did not parallel system. So the way that it'll happen is that we first perform the map then we store the resulting result in an R D D that is in memory and then we're going to perform a review on that new R D D, okay? The disadvantages are two. The first one is that to
store the intermediate result requires memory space. Now that might not seem
very consequential when you have four elements, but suppose that you had 40 million elements, okay? Then having another intermediate array for the intermediate result
would cost significant amount in terms of memory. And the other is that we're using two scan
to perform this operation. So first we're scanning
everything in order to square everything, and then
we're scanning another time in order to add the result. And it would be nicer if we
could just do it in one scan because you remember in
locality of computation if we can just use the same locations the sequential locations
as efficiently as possible then we run faster in terms
of a very large dataset. Okay? So the pipelined
execution that performs is taking the whole computation
in a single pass. So for each element in B you first compute the square
and then you enter the square as an input to the reduced operation. So the reduce operation is happening at the same time as the, okay. So the advantages are two. First it uses less memory intermediate results are not stored cause they're immediately
consumed by the reduce and it's faster because it's only one pass through the R D D. So this kind of evaluation
that we're talking about here is often referred
to as lazy evaluation. So the word lazy is because we get the first
command that what we should do and we literally do nothing,
we just put this command in a kind of program plan that of what we're going to do when
we will do something, right? So rather than executing
things immediately we basically say, "No let's
more more operations accumulate and then we can execute
all of them at once and we'll do it more efficiently." So that is what is called
here the execution plan. That the central idea in
spark where you take chunks of code and you basically
combine them together into a single thing to be executed and then you send this thing to the workers to execute it
on different parts of the data. Okay? So there's more you can
read about laser evaluation and execution plan when
you follow this link. Okay? So let's talk about
an instructive mistake something that we're writing in Spark that seems logical enough, but in fact doesn't do what
we expect it to do. So we have here something
that we paralyzed this list one one two and we want to calculate
the sum of the squares. So we say, okay, we are going to reduce X and Y to x squared plus y squared. That seems logical enough, right? Because if we just had these
two elements that indeed would give us the right result,
but we have three elements. So what would happen if
we have three element? So the first operation would be on this pair one, one, okay? So this pair over here, and replacing that with the one square
plus one squared is two. Okay? And then the second
operation on this pair, two two which is the two that we got from the first pair and the
two that is written right here. Then we get the final
result. That is eight. And this is not the right answer, right? One square plus one square
plus two squared is six. Okay? So the, the problem is that we are not really
thinking about this operation as an operation that
needs to be executable in any order of of thing, right? So the intermediate results
are going to be stored in this X, So you don't want
to square this result.
--- end {3.4_transcript.txt} ---
--- start{3.5_slides.pdf} ---
Basic Spark 2 – Part 2
        DSC 232R
1.2 getting information about an RDD
RDDs typically have hundreds of thousands of elements. It usually
makes no sense to print out the content of a whole RDD. Here are
some ways to get manageable amounts of information about an
RDD.
Create an RDD of length n which is a repetition of the pattern
1,2,3,4
1.2.1 Sampling an RDD
• RDDs are often very large.
• Aggregates, such as averages, can be approximated efficiently by
  using a sample.
• Sampling is done in parallel and requires limited computation.
1.2.3 Filtering an RDD
The method RDD.filter(func) Returns a new dataset formed
by selecting those elements of the source on which func remains
true.
1.2.4 Removing duplicate elements from an
RDD
The method RDD.distinct() Returns a new dataset that
contains the distinct elements of the source dataset.

This operation requires a shuffle in order to detect duplication
across partitions.
1.2.5 flatmap an RDD
The method RDD.flatMap(func) is similar to map, but each
input item can be mapped to 0 or more output items (so func
should return a Seq rather than a single item).
1.2.6 Set operations
In this part, we explore set operations including union, intersection,
subtract, cartesian in PySpark.
1. union(other)
   • Return the union of this RDD and another one.
   • Note that repetitions are allowed. The RDDs are bags not sets.
   • To make the result a set, use .distinct
2. intersection(other)
      • Return the intersection of this RDD and another one. The
        output will not contain any duplicate elements, even if the
        input RDDs did. Note that this method performs a shuffle
        internally.
3. subtract(other,numPartitions=None)
     • Return each value in self that is not contained in other.
4. cartesian(other)
     • Return the Cartesian product of this RDD and another one,
       that is, the RDD of all pairs of elements (a,b) where a is in
       self and b is in other.
1.3 Summary
• Chaining: creating a pipeline of RDD operations.
• Counting, taking and sampling an RDD
• More transformations: filter, distinct, flatmap
• Set transformations: union, intersection, subtract,
  cartesian

--- end {3.5_slides.pdf} ---
--- start{3.5_transcript.txt} ---
(bright lively music) - Okay, so let's think about how we can get information about an RDD. When we want to say we
want to get information about something that is in memory, that's a pretty simple thing. We just go and we look at it. We bring it or print it out. But when we have RDDs, they're, first of all,
very large typically. And secondly, they're not
really on our computer, they're on other worker computers that are doing the calculation for us. So we can collect all of the data and then we'll have it to see locally, but usually that's not desirable
because then we are losing all of the ability, all of the advantages of the distributed parallel computation. So there are other ways of, like, poking and peeking into the content of an RDD, even though it's away. Okay, so let's see how we do that. So here, we're going to create
an RDD of some length n, which will be a million. And it's going to have 1, 2,
3, 4, 1, 2, 3, 4, 1, 2, 3, 4. Okay. So that's basically the two command. I create that and now we have this B. That is a medium-large RDD and it's out of our computer. Okay, so one first thing
that we might want to do is to find the number of elements in an RDD, and that's an easy thing. We can just do B.count. Another thing is that we can
just ask for the first element or the first few element, that's
the command first or take. Next, we have a more
sophisticated operation, which is sampling. So we have an RDD. Maybe with a million or maybe
with a billion data points. And we want to calculate,
let's say, the average. Now, that is possible, to sum everything and
divide by the number, but that is pretty expensive operation. So we can get almost the same
answer if we average a sample. So we just take a subset,
a randomly chosen subset of the element and then
we operate just on those. That would be faster. So here is an example of that, that in our case here is how many elements we want on average, that's m. Okay, so that's m that we want. And then we do a sample
operation with m/n. So m/n is the ratio between
the number of elements that we want and the total
number of elements in the RDD. And so this is basically the probability of taking an element in from that RDD. And so here is a first
sample and a second sample, and what you see is that the first sample and the second samples, not only are they different, which is natural when
you sample at random, but they have a different length. So why do they have a different length? Because what we specify, just the probability of getting this, taking a specific element, not the exact number of
elements that we want. And why don't we want it to give us the exact number of element? Because that would require
a significant coordination. If you just sample with some probability, you can do it separately on each worker and then combine all of the results. If you want to have a specific number, then you have to have
the workers coordinate how much elements each
one of them will generate. Okay? So that would require more work. So that's the way that it's done in Spark. And if you want a specific element, then you just choose a P that
would be a little bit larger than what you need and then you trim down to the size that you want. Another operation is filtering the RDD. So that takes an RDD and generates a new
smaller RDD by selecting, by giving a rule for
which elements to select. So what we have here is, we're taking the same B,
RDD, that we had before, and then we pick the operation filter and what the function that
we give filter is a function that just gives true or false, okay? And if it gives true, then
you get these elements and if you give false, then
you don't take the element. Okay? So we're basically saying that out of these elements
that are, I don't remember, 0, 1, 2, 3, 4, we are just taking the
elements that are larger than, that are larger than 3, so it's just 4, and that's indeed like a
quarter of the elements, okay? So 250,000 out of a million. Another operation that
we want to do sometimes is RDD.distinct. That seems like a very natural operation. We don't want repetitions
of the same element. So that is a natural one, but in fact it turns out that
it's a very expensive one. Why is it expensive? Because it requires a shuffle operation. It requires all of the
workers to communicate so that they can find out
whether one has a copy of an element that the other one has so that they can remove it. Here, we have two operations. So we parallelize this list that has two 1s, two 2s and two 3s, okay? It's this list here. And then we just collect this one. So we just get it as it is. Or we ask for the distinct
element and then we collect. And so we get just 1 and 2 and 3. flatMap of an RDD is
like the map operation, but it's an operation that
assumes that the function that you have as a map, generates a list and it
concatenates these list, rather than having these
lists in separate element. So here is a natural way
that you want to use that. So suppose that you have text that is made out of two sentences and you want an RDD that is
just all the words, okay? So how do you do that? First of all, you do a map
that separates each list, splits it up into each sentence, each string split out the words, so you have a list of words. And then you run flatMap. So here, it's the same
operation, here and here, but here we're doing it with a map and here we're doing it with a flatMap. So what you get in the first case is, you get two lists, right? Because each operation gives you a list. And so you just get a list of two lists. While if you do flatMap, you just get one list that
has all of the element and that's sometimes
more of what you want. Some operations that we
have are set operations. So we can work with sets and we can basically take unions of sets, intersections of sets, subtract one set from another, and we can take cartesian operation. So here is union. So it basically creates
the union of two RDDs, which basically is just a
combination of all the element. Now, it's not really a set at this point. It might not have started even with a set. So in order to make it a set, we need to use distinct, right? So if we basically take here two RDDs and we take the union of them, so this 1, 1, 2, 3 is one RDD,
and this is the other RDD. If we just take the union, then we get a bag, or a thing
that has elements repeating. This one has one 1 repeating and this 1 also appears in the other list. And if we want it at sets, we have to use the operation distinct so that it would remove
all of the multiple copies that we give: 1, a, 2, 3, and b. intersection is similar. But intersection
automatically uses distinct. So it has to perform this
extensive operation internally. Okay? So even though this was a
bag with the repeating 1s and this was a bag, then if we take the
intersection of the two, we get just the elements 1 and 2, we don't get the element twice. Similarly, subtract is
when you take one RDD and you remove all of the
elements from a different RDD. And then finally, you have an
operation that is Cartesian where you take two RDD and you create a new RDD that is pairs, all possible pairs of taking
one element from the first and one element from the second. That can be a very, very big RDD. Sometimes it's the thing that you want. Okay, so here it is. We have: one list that is 1, 1, 2; one list that is a and b, okay? And we generate them as RDD. And then we take the
Cartesian rdd1 and rdd2, we get 1, a; 1, b; 1, a; 1, b. So because of the two 1s here. And then 2, a; and 2, b. Okay? So we get the product and
all of the repetition. Alright, to summarize, chaining is a way of creating
a pipeline of RDD operation. To get a information about
an RDD, we use counting, taking and sampling, and then there is more transformations: filter, distinct, flatmap. And set transformation. Like union, intersection,
subtract, and cartesian. And there are many other one. We will talk about some
more in the next video. See you then.
--- end {3.5_transcript.txt} ---
--- start{4.1_notebook.md} ---
# Execution Plans, Lazy Evaluation and Caching

Task: calculate the sum of squares :

$$ \Sigma_{i=1}^{n} x_{i}^{2} $$

The standard (or busy) way to do this is

1. Calculate the square of each element.
2. Sum the squares.

This requires storing all intermediate results.

$$ S = \Sigma_{i=1}^{n} x_{i}^{2} $$

## Lazy Evaluation

Unlike a regular python program, map/reduce commands do not always perform any computation when they are executed. Instead, they construct something called an execution plan. Only when a result is needed does the computation start. This approach is also called lazy execution.

The benefit from lazy execution is in minimizing the the number of memory accesses. Consider for example the following map/reduce commands:

```python
A=RDD.map(lambda x:x*x).filter(lambda x: x%2==0)
A.reduce(lambda x,y:x+y) 
```

The commands defines the following plan. For each number `x` in the RDD:

1. Compute the square of `x`
2. Filter out `x*x` whose value is odd.
3. Sum the elements that were not filtered out.

A naive execution plan is to square all items in the RDD, store the results in a new RDD, then perform a filtering pass, generating a second RDD, and finally perform the summation. Doing this will require iterating through the RDD three times, and creating 2 interim RDDs. As memory access is the bottleneck in this type of computation, the execution plan is slow.

A better execution plan is to perform all three operations on each element of the RDD in sequence, and then move to the next element. This plan is faster because we iterate through the elements of the RDD only once, and because we don't need to save the intermediate results. We need to maintain only one variable: the partial sum, and as that is a single variable, we can use a CPU register.

For more on RDDs and lazy evaluation see here in the [spark manual](https://spark.apache.org/docs/latest/rdd-programming-guide.html)

## Experimenting with Lazy Evaluation
The `%%time` magic

The `%%time` command is a cell magic which measures the execution time of the cell. We will mostly be interested in the wall time, which includes the time it takes to move data in the memory hierarchy.

For more on jupyter magics See here

### Preparations

In the following cells we create an RDD and define a function which wastes some time and then returns `cos(i)`. We want the function to waste some time so that the time it takes to compute the `map` operation is significant.

```python
import os
os.environ["PYSPARK_PYTHON"]="python3"
os.environ["PYSPARK_DRIVER_PYTHON"] = "python3"

from pyspark import SparkContext
sc = SparkContext(master="local[4]")  #note that we set the number of workers to 3

# We create an RDD with one million elements to amplify the effects of lazy evaluation and caching.

%%time
RDD=sc.parallelize(range(1000000))

# RETURNS
# ->
"""
CPU times: user 0 ns, sys: 0 ns, total: 0 ns
Wall time: 222 ms
"""
```

It takes about 01.-0.5 sec. to create the RDD.

```python
print(RDD.toDebugString().decode())

# RETURNS
# ->
"""
(4) PythonRDD[1] at RDD at PythonRDD.scala:48 []
 |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:175 []
"""
```

### Define a computation

The role of the function `taketime` is to consume CPU cycles.

```python
from math import cos
def taketime(i):
    [cos(j) for j in range(100)]
    return cos(i)

%%time
taketime(1)

# RETURNS
# ->
"""
CPU times: user 0 ns, sys: 0 ns, total: 0 ns
Wall time: 32.7 µs
0.5403023058681398
"""
```

### Time units
* 1 second = 1000 Milli-second ($ms$)
* 1 Millisecond = 1000 Micro-second ($\mu s$)
* 1 Microsecond = 1000 Nano-second ($ns$)

### Clock Rate
One cycle of a 3GHz cpu takes $\frac{1}{3}ns$
 

`taketime(1000)` takes about 25 $\mu s$ = 75,000 clock cycles.

### The `map` operation

```python
%%time
Interm=RDD.map(lambda x: taketime(x))

# RETURNS
# ->
"""
CPU times: user 0 ns, sys: 0 ns, total: 0 ns
Wall time: 19.1 µs
"""
```

### How come so fast?

* We expect this map operation to take 1,000,000 * 25 $\mu s$= 25 Seconds.
* Why did the previous cell take just 29 $\mu s$?
* Because no computation was done
* The cell defined an execution plan, but did not execute it yet.

Lazy Execution refers to this type of behaviour. The system delays actual computation until the latest possible moment. Instead of computing the content of the RDD, it adds the RDD to the execution plan.

Using Lazy evaluation of a plan has two main advantages relative to immediate execution of each step:

1. A single pass over the data, rather than multiple passes.
2. Smaller memory footprint becase no intermediate results are saved.
### Execution Plans

At this point the variable Interm does not point to an actual data structure. Instead, it points to an execution plan expressed as a dependence graph. The dependence graph defines how the RDDs are computed from each other.

The dependence graph associated with an RDD can be printed out using the method `toDebugString()`.

```python
print(Interm.toDebugString().decode())

# RETURNS
# ->
"""
(4) PythonRDD[2] at RDD at PythonRDD.scala:48 []
 |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:175 []
"""
```

Interm = ``(4) PythonRDD[2]` at RDD at `PythonRDD.scala:48 []`

`______(4)`` corresponds to the number of partitions

RDD =   |  `ParallelCollectionRDD[0]` at parallelize at `PythonRDD.scala:489 []`

At this point only the two left blocks of the plan have been declared.

### Actual execution

The reduce command needs to output an actual output, spark therefor has to actually execute the `map` and the reduce. Some real computation needs to be done, which takes about 1 - 3 seconds (Wall time) depending on the machine used and on it's load.

```python
%%time
print('out=',Interm.reduce(lambda x,y:x+y))

# RETURNS
# ->
"""
out= -0.2887054679684464
CPU times: user 4 ms, sys: 8 ms, total: 12 ms
Wall time: 6.59 s
"""
```

### How come so fast? (take 2)

* We expect this map operation to take 1,000,000 * 25 $\mu s$ = 25 Seconds.
* Map+reduce takes only ~4 second.
* Why?
* Because we have 4 workers, rather than one.
* Because the measurement of a single iteration of taketime is an overestimate.

### Executing a different calculation based on the same plan.

The plan defined by `Interm` might need to be executed more than once.

Example: compute the number of map outputs that are larger than zero.

```python
%%time
print('out=',Interm.filter(lambda x:x>0).count())

# RETURNS
# ->
"""
out= 500000
CPU times: user 8 ms, sys: 0 ns, total: 8 ms
Wall time: 7.86 s
"""
```

### The price of not materializing

* The run-time (3.4 sec) is similar to that of the reduce (4.4 sec).
* Because the intermediate results in `Interm` have not been saved in memory (materialized)
* They need to be recomputed.

The middle block: `Map(Taketime)` is executed twice. Once for each final step.

### Caching intermediate results

* We sometimes want to keep the intermediate results in memory so that we can reuse them later without recalculating. * This will reduce the running time, at the cost of requiring more memory.
* The method `cache()` indicates that the RDD generates in this plan should be stored in memory. Note that this is a plan to cache. The actual caching will be done only when the final result is needed.

```python
%%time
Interm=RDD.map(lambda x: taketime(x)).cache()

# RETURNS
# ->
"""
CPU times: user 4 ms, sys: 0 ns, total: 4 ms
Wall time: 14.2 ms
"""
```

By adding the Cache after `Map(Taketime)`, we save the results of the map for the second computation.

### Plan to cache
The definition of `Interm` is almost the same as before. However, the plan corresponding to `Interm` is more elaborate and contains information about how the intermediate results will be cached and replicated.

Note that `PythonRDD[4]` is now [Memory Serialized 1x Replicated]

```python
print(Interm.toDebugString().decode())

# RETURNS
# ->
"""
(4) PythonRDD[5] at RDD at PythonRDD.scala:48 [Memory Serialized 1x Replicated]
 |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:175 [Memory Serialized 1x Replicated]
"""
```

### Comparing plans with and without cache

Plan with Cache

```python
4) PythonRDD[33] at RDD at PythonRDD.scala:48 [Memory Serialized 1x Replicated]
 |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:489 [Memory Serialized 1x Replicated]
```

The difference is that the plan for both RDDs includes [Memory Serialized 1x Replicated] which is the plan to materialize both RDDs when they are computed.

### Creating the cache
The following command executes the first map-reduce command and caches the result of the `map` command in memory.

```python
%%time
print('out=',Interm.reduce(lambda x,y:x+y))

# RETURNS
# ->
"""
out= -0.2887054679684464
CPU times: user 4 ms, sys: 4 ms, total: 8 ms
Wall time: 11.2 s
"""
```

### Using the cache

This time `Interm` is cached. Therefor the second use of `Interm` is much faster than when we did not use cache: 0.25 second instead of 1.9 second. (your milage may vary depending on the computer you are running this on).

```python
%%time
print('out=',Interm.filter(lambda x:x>0).count())

# RETURNS
# ->
"""
out= 500000
CPU times: user 8 ms, sys: 0 ns, total: 8 ms
Wall time: 256 ms
"""
```

## Summary

* Spark uses Lazy Evaluation to save time and space.
* When the same RDD is needed as input for several computations, it can be better to keep it in memory, also called `cache()`.
* Next Video, Partitioning and Gloming

# Partitioning and Gloming
* When an RDD is created, you can specify the number of partitions.
* The default is the number of workers defined when you set up SparkContext
```python
A=sc.parallelize(range(1000000))
print(A.getNumPartitions())
# RETURNS
# -> 4
```

We can repartition A into a different number of partitions.
```python
D=A.repartition(10)
print(D.getNumPartitions())

# RETURNS
# -> 10
```

We can also define the number of partitions when creating the RDD.

```python
A=sc.parallelize(range(1000000),numSlices=10)
print(A.getNumPartitions())
# RETURNS
# -> 10
```

## Why is the #Partitions important?
* They define the unit the executor works on.
* You should have at least as pany partitions as workers.
* Smaller partitions can allow more parallelization.

## Repartitioning for Load Balancing

Suppose we start with 10 partitions, all with exactly the same number of elements
```python
A=sc.parallelize(range(1000000))\
    .map(lambda x:(x,x)).partitionBy(10)
print(A.glom().map(len).collect())

# RETURNS
# -> [100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000, 100000]
```

* Suppose we want to use filter() to select some of the elements in A.
* Some partitions might have more elements remaining than others.

```python
#select 10% of the entries
B=A.filter(lambda pair: pair[0]%5==0)
# get no. of partitions
print(B.glom().map(len).collect())

# RETURNS
# -> [100000, 0, 0, 0, 0, 100000, 0, 0, 0, 0]
```

* Future operations on B will use only two workers.
* The other workers will do nothing,because their partitions are empty.
* To fix the situation we need to repartition the RDD.
* One way to do that is to repartition using a new key.
* The method `.partitionBy(k)` expects to get a `(key,value)` RDD where keys are integers.
* Partitions the RDD into `k` partitions.
* The element `(key,value)` is placed into partition no. `key % k`

```python
C=B.map(lambda pair:(pair[1]/10,pair[1])).partitionBy(10) 
print(C.glom().map(len).collect())

# RETURNS
# -> [20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000]
```

* Another approach is to use random partitioning using `repartition(k)`
* An advantage of random partitioning is that it does not require defining a key.
* A disadvantage of random partitioning is that you have no control on the partitioning.

```python
C=B.repartition(10)
print(C.glom().map(len).collect())

# RETURNS
# -> [20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000]
```
## Glom()

* In general, spark does not allow the worker to refer to specific elements of the RDD.
* Keeps the language clean, but can be a major limitation.
* `glom()` transforms each partition into a tuple (immutabe list) of elements.
* Creates an RDD of tules. One tuple per partition.
* workers can refer to elements of the partition by index.
* but you cannot assign values to the elements, the RDD is still immutable.
* Now we can understand the command used above to count the number of elements in each partition.
* We use `glom()` to make each partition into a tuple.
* We use len on each partition to get the length of the tuple - size of the partition.
* We collect the results to print them out.

```python
print(C.glom().map(len).collect())

# RETURNS
# -> [20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000]
```

* A more elaborate example
* There are many things that you can do using `glom()`

* Below is an example, can you figure out what it does?

```python
def getPartitionInfo(G):
    d=0
    if len(G)>1: 
        for i in range(len(G)-1):
            d+=abs(G[i+1][1]-G[i][1]) # access the glomed RDD that is now a  list
        return (G[0][0],len(G),d)
    else:
        return(None)

output=B.glom().map(lambda B: getPartitionInfo(B)).collect()
print(output)

# RETURNS
# -> [(0, 100000, 999990), None, None, None, None, (5, 100000, 999990), None, None, None, None]
```

## Summary
* We learned why partitions are important and how to control them.
* We Learned how `glom()` can be used to allow workers to access their partitions as lists.

--- end {4.1_notebook.md} ---
--- start{4.1_slides.pdf} ---
 Execution Plans, Lazy
Evaluation, and Caching
         DSC 232R
1.2 Lazy Evaluation
• Postpone computing the square until result is needed.
• No need to store intermediate results.
• Scan through the data once, rather than twice.
1.1 Task: calculate the sum of squares
                               𝑛

                             ෍ 𝑥2𝑖
                              𝑖=1

The standard (or busy) way to do this is

   1. Calculate the square of each element.
   2. Sum the squares.

This requires storing all intermediate results.
Busy Evaluation
       𝑛

 𝑆 = ෍ 𝑥2𝑖
      𝑖=1
Busy Evaluation
       𝑛

 𝑆 = ෍ 𝑥2𝑖
      𝑖=1
Busy Evaluation
       𝑛

 𝑆 = ෍ 𝑥2𝑖
      𝑖=1
Lazy Evaluation
       𝑛

 𝑆 = ෍ 𝑥2𝑖
      𝑖=1
Lazy Evaluation
       𝑛

 𝑆 = ෍ 𝑥2𝑖
      𝑖=1
2 Experimenting with Lazy Evaluation
We create an RDD with one million elements to demonstrate the
effects of lazy evaluation.
2.2 Define a Computation
The role of the function taketime is to consume CPU cycles.
2.3 Time Units
• 1 second = 1000 Milli-seconds (ms)
• 1 Millisecond = 100 Micro-seconds (μs)
• 1 Microsecond = 1000 Nano-seconds (ns)


2.4 Clock Rate
                             1
One cycle of a 3GHz cpu takes ns
                             3
A single execution of taketime takes about 25 μs = 75,000 clock
cycles.
2.3 The map Operation




2.6 How come so fast?
• We expect this map operation to take 1,000,000 * 25 μs = 25
  seconds
• Why did the previous cell take just 29 μs?
 • Because no computation was done.
 • The cell defined an execution plan, but did not execute it yet.




At this point, only the two left blocks of the plan have been
declared.
2.8 Actual Execution
The reduce command needs to output an actual output. Spark
therefore has to actually execute the map and the reduce. Some
real computation needs to be done, which takes about 1-3 seconds
(Wall time) depending on the machine used and on its load.
2.9 How come so fast? (Take 2)
• We expect this map operation to take 1,000,000 * 25 μs = 25
  seconds
• Map + reduce takes only ~4 seconds
• Why?

• Because we have four workers rather than one.
• Because the measurement of a single iteration of taketime is an
  overestimate.
2.10 Executing a different calculation based
on the same plan
The plan defined by Interm might need to be executed more than
once.

Example: Compute the number of map outputs that are larger than
zero.
2.11 The price of not materializing
• The run-time (3.4 sec) is similar to that of the reduce (4.4 sec).
• Because the intermediate results in Interm have not been saved
  in memory (materialized).
• They need to be recomputed.
The middle block: Map(Taketime) is executed twice, once for
each final step.
2.12 Caching intermediate results
• We sometimes want to keep the intermediate results in memory
  so that we can reuse them later without recalculating.
• This will reduce the running time, at the cost of requiring more
  memory.
• The method cache() indicates that the RDD generates in this
  plan should be stored in memory. Note that this is a plan to
  cache. The actual caching will be done only when the final result
  is needed.
By adding the Cache after Map(Taketime), we save the results of
the map for the second computation.
     2.13 Plan to cache
The definition of Interm is almost the same as before. However, the plan
corresponding to Interm is more elaborate and contains information about
how intermediate results will be cached and replicated.

Note that PythonRDD[4] is now [Memory Serialized 1x Replicated].

We can check on the plan by applying .toDebugString() to the RDD.
2.14 Creating the cache
The following command executes the first map-reduce command
and caches the result of the map command in memory.
2.15 Using the cache
This time Interm is cached. Therefore, the second use of Interm
is much faster than when we did not use cache: 0.25 second
instead of 1.9 second. (Your milage may vary depending on the
computer you are running this on).
3 Summary of evaluation plans
• Spark uses Lazy Evaluation to save time and space.
• When the same RDD is needed as input for several computations,
  it can be better to keep it in memory, also called cache().
• Next Video, Partitioning and Gloming

--- end {4.1_slides.pdf} ---
--- start{4.1_transcript.txt} ---
(lively music) - So in the previous video, I told you about some
operations that you can do with Spark and RDDs and how
these operations fit together. What I want to do this time is dig deeper into how you can make these
things really run fast, right? Because the whole point of
Spark is that you can run things on cluster of computers
and get the results therefore significantly faster. If you run Spark on your own computer, there is very little point
in general in doing that. Okay. So we're going to talk
about execution plans, lazy evaluation, and caching. We haven't really talked about caching. We talked a little bit
about execution plans and lazy evaluation, but I think it's worthwhile to dig deeper. And I recommend that you review this. So this is a notebook that
you should have access to. And there is much more
information in the notebook than what I will be talking about. And I think it's really important that you go and review that
notebook, play with it, until you feel like you really
understand what is going on. Of course, ask question
whenever you don't understand. Okay, so the task we're
going to do is a task that we talked about before. It's calculating the sum of square, and the standard or busy way to do this is to calculate the square of
each element in the RDD, then sum the squares, and
then you get the result, okay? This requires storing all
of the intermediate results, and that is a big disadvantage,
as we will see in a minute. Okay, so here is a visualization of busy evaluation. We have this RDD initially and then we run a square
operation, a map square, and we get this new RDD, okay? that has the squares or
the square of each element. Then we run the reduce operation, which basically takes all of
the elements in the second RDD, in the intermediate RDD, and then calculates the
sum and we get the result. Okay, so that is a disadvantage. First, because we are calculating, we're storing this intermediate result, which doubles the amount
of memory that we need. And the second is that we
pass through the data once to calculate the squares and then we pass through
the data a second time to calculate the sum. So the idea of lazy evaluation is to postpone computing the square
until the result is needed. So when you get the command
through the map of squares, you actually don't do anything. You don't need to store
any intermediate result. And you scan through the
data once rather than twice. So let's see how that works. We start at the beginning of this RDD. And then we square 2 to get
4, we square 5 to get 25, and then we sum these two
elements, and get 29, okay? Now this second RDD is
not really materialized. We're not storing all of
these intermediate results. I'm just showing them
here for illustration. So here we have the third element. We take the -6 and we take the square root to 36, and then we take the 36, add it to the 29, and get the 65, okay? And now we can forget about the 36, as we forgot about the previous element. So let's do some experiment
to see how this works in practice. So we're going to create an
RDD with 1 million elements to demonstrate the effects
of lazy evaluation. So we basically just take the range, zero to 1 million minus 1, range 1 million; and we parallelize it to create an RDD. This is not really a very big RDD, but it would suffice for demonstration. Okay, now we're going
to define an operation that we are going to use for map, okay? So map, if you just square,
is a very tiny operation, takes very, very little time, so redoing that operation is not costly. But I want an operation
that is relatively costly. So I just called it
taketime to indicate that. And it just does some computation that is not really very important, but it basically calculate the cosine of all of the numbers
between zero and 99, okay? So just something to do, and then it returns the cosine of the element that you
got with any (inaudible). So nothing really smart here, it's just a way to waste time. Okay? So if we calculate the time
that it take, that's here, about 44.3 microsecond, nanosecond. So I always get these things confused. So to help me, I have here
a little table to remind you that one second is 1,000 millisecond. One millisecond is 1,000 microsecond. and one microsecond is 1,000 nanosecond. So basically, second is
a billion nanosecond. Many, many, many nanoseconds in a second. And in the clock rate, that we typically have three
gigahertz in modern computers, it means that one cycle takes
1/3 of nanosecond, okay? So it basically means that we can execute, we have 3 billion operations per second. Now, the single execution
of taketime takes about 25 microsecond, right 25 microseconds. So that's about 75,000 clock cycle. Okay, so here is the map operation. We basically say, Interm is R.map of lambda x and taketime x, okay? So on each x in each one
of the million elements we do the taketime operation, and this is the amount
of time that it took. That is very, very little, right? 24 microsecond is just about the time that it takes to compute this once, to compute taketime once. So how come it is so fast, right? I mean, what we expected is
maybe something like 25 second. How come it is so fast? The previous took about 29 microsecond. Here it took actually 24 microsecond. And this is because no
computation was done, okay? So this is a manifestation
of the lazy execution. Got an operation. The operation might be quite expensive, but we haven't done it. So therefore, that finished very quickly. Okay? So you can actually read
out the execution plan that you have right now in the system, the R, the RDD R, and the Interm, intermediate RDD both can be written out and this is the description
of the steps that they're saying. So this is an explicit way of saying, "This is what we are going to do when this map is going to be executed. Okay? So we can think
about it in this way: We can have an RDD and this RDD, this RDD might have been
already materialized, and this, then we run on it a Map time and then we're going
to run on it a reduce. And then at the end, we're
going to have a number. Okay? So this is the pipeline of what we are doing. So this whole thing, from parallelized map and
reduce is the pipeline. We haven't reached the reduce step. Okay? So here is the execution. So the reduce command needs
to output an actual number. That it cannot say, "Oh, I will
calculate something later." Therefore, we have to actually operate and this is what happened. So we take the intermediate RDD and take an operation reduce
that just sums the result. And then we see that
what this takes in terms of Wall time is 2.61 second, okay? So this still is fast, right? Because we expected 25 second and what we got is something more like 3 second or 2 second. Why is that? Well, one reason is because we
actually have on my computer 4 workers, we have 4 core.
And so each core is a CPU. And the other is that actually
the measurement that they did of how long it takes to run taketime is an overestimate because there's more
things happening in Python that cost significant amount of time that is not really the time
that the cover of time take. But still it takes time,
it takes 2 1/2 second. Okay? So what do we see here? We saw that we have this plan
that is being accumulated as we write the command and
only when we need to execute in order to generate something, then it actually takes the computation. That's the lazy computation. Now, the problem with
the lazy computation is that sometimes we actually
need the intermediate results for other calculations. Okay, so it might be that
this intermediate results of taking the squares over
the square of every element is actually something that
later we would want to use and it would be costly to redo it, right? So that's the idea, the idea that helps with that
is what's called caching. Okay? So here's the intermediate result. And now we do a different
command on it, filter. And what we see is that the
amount of time that it took to do filter is significant and the reason is because
it actually executed all of the map command again, right? Because the intermediate
results were not held anywhere and so we paid this price of recalculating the intermediate result. So a lot of what you do as
a programmer in Spark is that you basically say, "Okay, which parts of the
calculation do I need to store or to cache and which
parts I can just let go and not materialize them?" Okay, so the price of not materializing, the numbers here, not
exactly the right ones, but the runtime is similar
to that of the reduce, right? That's because we needed
to calculate things twice. The intermediate result in
Interm have not been saved in memory. So it's kind of interesting to just think about it for a minute. What is Interm? Interm is just a plan to
calculate something, right? So it's a plan to calculate the squares. It is not an actual
storage of the squares. So it's an RDD that has not
been materialized, okay? So it needs to be recomputed
whenever it's needed. So two seconds might not be a big issue, but let's say it's an hour,
then it starts to be an issue. Okay, so what we have right now is just this kind of pipeline. So we have this pipeline going into here and then we have another
pipeline that basically goes to here and this pipeline
requires us to recompute this map. Okay, so what we want to do is to cache intermediate results. We sometimes want to keep
this intermediate results so that we don't need to recalculate them, and this will reduce the
running time at the cost of requiring more memory, right? Because now we're back to
storing intermediate things. So we can decide which
pieces we want to store or otherwise we just don't
store them and recalculate them. So there is a command
for doing exactly that and it's called cache. But the surprising, a little
bit strange things about it, it's not really immediately storing, it's just planning to store. So when you write the command cache, it's really a plan to cache. It's part of the lazy plan. Okay? So if I'm writing this, I have R.map, Interm is R.map taketime and then this is cache, cache. But it doesn't take much more time. Okay? So when does it take time? It will take the time when it
is actually executing. Okay? So we put here a cache,
right after the Map, and once we will calculate the Map, we will basically store the
intermediate result in the cache and then we can take the cache and quickly calculate
the reduce or the filter or anything else that we might want to do. So here is the plan to cache and you can read the
details of the plan here. Not going to do that. And now we're going to
actually create the cache. Okay? So now that we do a reduce, that takes a significant amount of time. But this time is now amortized. So we basically materialize
the intermediate result. So we're not going to need to
pay this amount of time again. And now we're going to
use the cache. Okay? So what we see is now we are
basically doing the exact same command as before:
Interm.filter, okay? And then we see that the time that it took is 169 millisecond instead of, like, two second. Okay? So that's basically a demonstration of the utility of a cache. Okay, so to summarize evaluation plan, Spark uses lazy evaluation
to save time and space. When the same RDD is needed as input for several computations, it can be better to keep it in memory, and this is also called caching. Okay. So I'll see you next time.
--- end {4.1_transcript.txt} ---
--- start{4.2_notebook.md} ---
# Operations on (key,val) RDDs

## Types of spark operations

There are Three types of operations on RDDs: Transformations, Actions and Shuffles.

* The most expensive operations are those the require communication between nodes.

### Transformations: RDD $\rightarrow$ RDD.

* Examples map, filter, sample, More
* No communication needed.

### Actions: RDD $\rightarrow$ Python-object in head node.

* Examples: reduce, collect, count, take, More
* Some communication needed.
### Shuffles: RDD $\rightarrow$ RDD, shuffle needed

* Examples: sort, distinct, repartition, sortByKey, reduceByKey, join More
* A LOT of communication might be needed.

## Key/value pairs

* A python dictionary is a collection of key/value pairs.
* The key is used to find a set of pairs with the particular key.
* The value can be anything.
* Spark has a set of special operations for (key,value) RDDs.

Spark provides specific functions to deal with RDDs in which each element is a key/value pair. Key/value RDDs expose new operations (e.g. aggregating and grouping together data with the same key and grouping together two different RDDs.) Such RDDs are also called pair RDDs. In python, each element of a pair RDD is a pair tuple.

```python
import os
os.environ["PYSPARK_PYTHON"]="python3"
os.environ["PYSPARK_DRIVER_PYTHON"] = "python3"

from pyspark import SparkContext
sc = SparkContext(master="local[4]")
```

## Creating (key,value) RDDS

### Method 1: `parallelize` a list of pairs.

```python
pair_rdd = sc.parallelize([(1,2), (3,4)])
print(pair_rdd.collect())

# RETURNS
# -> [(1, 2), (3, 4)]
```

### Method 2: `map` a function that maps elements to key/value pairs.

```python
regular_rdd = sc.parallelize([1, 2, 3, 4, 2, 5, 6])
pair_rdd = regular_rdd.map( lambda x: (x, x*x) )
print(pair_rdd.collect())

# RETURNS
# -> [(1, 1), (2, 4), (3, 9), (4, 16), (2, 4), (5, 25), (6, 36)]
```

## Transformations on (key,value) rdds

### `reduceByKey(func)`

Apply the reduce function on the values with the same key.

```python
rdd = sc.parallelize([(1,2), (2,4), (2,6)])
print("Original RDD :", rdd.collect())
print("After transformation : ", rdd.reduceByKey(lambda a,b: a+b).collect())

# RETURNS
# -> 
"""
Original RDD : [(1, 2), (2, 4), (2, 6)]
After transformation :  [(1, 2), (2, 10)]
"""
```

### `sortByKey()`

Sort RDD by keys in ascending order.

```python
rdd = sc.parallelize([(2,2), (1,4), (3,6)])
print("Original RDD :", rdd.collect())
print("After transformation : ", rdd.sortByKey().collect())

# RETURNS
# ->
"""
Original RDD : [(2, 2), (1, 4), (3, 6)]
After transformation :  [(1, 4), (2, 2), (3, 6)]
"""
```

Note: The output of sortByKey() is an RDD. This means that RDDs do have a meaningful order, which extends between partitions.

### `mapValues(func)`

Apply func to each value of RDD without changing the key.

```python
rdd = sc.parallelize([(1,2), (2,4), (2,6)])
print("Original RDD :", rdd.collect())
print("After transformation : ", rdd.mapValues(lambda x: x*2).collect())

# RETURNS
# ->
"""
Original RDD : [(1, 2), (2, 4), (2, 6)]
After transformation :  [(1, 4), (2, 8), (2, 12)]
"""
```

### `groupByKey()`

Returns a new RDD of `(key,<iterator>)` pairs where the iterator iterates over the values associated with the key.


`Iterators` are python objects that generate a sequence of values. Writing a loop over `n` elements as

```python
for i in range(n):
    ## do something...
```

is inefficient because it first allocates a list of `n` elements and then iterates over it. Using the iterator `xrange(n)` achieves the same result without materializing the list. Instead, elements are generated on the fly.

To materialize the list of values returned by an iterator we will use the list comprehension command:

```python
[i for i in list]
```

```python
rdd = sc.parallelize([(1,2), (2,4), (2,6)])
print("Original RDD :", rdd.collect())
print("After transformation : ", rdd.groupByKey().mapValues(lambda x:[a for a in x]).collect())

# RETURNS
# ->
"""
Original RDD : [(1, 2), (2, 4), (2, 6)]
After transformation :  [(1, [2]), (2, [4, 6])]
"""
```

### `flatMapValues(func)`

Similar to `flatMap()`: creates a separate key/value pair for each element of the list generated by the map operation.

func is a function that takes as input a single value and returns an iterator that generates a sequence of values. The application of flatMapValues operates on a key/value RDD. It applies `func`to each value, and gets an list (generated by the iterator) of values. It then combines each of the values with the original key to produce a list of key-value pairs. These lists are concatenated as in `flatMap`

```python
rdd = sc.parallelize([(1,2), (2,4), (2,6)])
print("Original RDD :", rdd.collect())
# the lambda function generates for each number i, an iterator that produces i,i+1
print("After transformation : ", rdd.flatMapValues(lambda x: list(range(x,x+2))).collect())

# RETURNS
# ->
"""
Original RDD : [(1, 2), (2, 4), (2, 6)]
After transformation :  [(1, 2), (1, 3), (2, 4), (2, 5), (2, 6), (2, 7)]
"""
```

### (Advanced) `combineByKey(createCombiner, mergeValue, mergeCombiner)`

Combine values with the same key using a different result type.

This is the most general of the per-key aggregation functions. Most of the other per-key combiners are implemented using it.

The elements of the original RDD are considered here values

Values are converted into combiners which we will refer to here as "accumulators". An example of such a mapping is the mapping of the value word to the accumulator (word,1) that is done in WordCount.

accumulators are then combined with values and the other combiner to generate a result for each key.

For example, we can use it to calculate per-activity average durations as follows. Consider an RDD of key/value pairs where keys correspond to different activities and values correspond to duration.

```python
rdd = sc.parallelize([("Sleep", 7), ("Work",5), ("Play", 3), 
                      ("Sleep", 6), ("Work",4), ("Play", 4),
                      ("Sleep", 8), ("Work",5), ("Play", 5)])

sum_counts = rdd.combineByKey(
    (lambda x: (x, 1)), # createCombiner maps each value into a  combiner (or accumulator)
    (lambda acc, value: (acc[0]+value, acc[1]+1)),
#mergeValue defines how to merge a accumulator with a value (saves on mapping each value to an accumulator first)
    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators
)

print(sum_counts.collect())
duration_means_by_activity = sum_counts.mapValues(lambda value:
                                                  value[0]*1.0/value[1]) \
                                            .collect()
print(duration_means_by_activity)

# RETURNS
# ->
"""
[('Work', (14, 3)), ('Play', (12, 3)), ('Sleep', (21, 3))]
[('Work', 4.666666666666667), ('Play', 4.0), ('Sleep', 7.0)]
"""
```


To understand combineByKey(), it’s useful to think of how it handles each element it processes. As combineByKey() traverses through the elements in a partition, each element either has a key it hasn’t seen before or has the same key as a previous element.

If it’s a new key, createCombiner() is called to create the initial value for the accumulator on that key. In the above example, the accumulator is a tuple initialized as (x, 1) where x is a value in original RDD. Note that createCombiner() is called only when a key is seen for the first time in each partition.

If it is a key we have seen before while processing that partition, it will instead use the provided function, mergeValue(), with the current value for the accumulator for that key and the new value.

Since each partition is processed independently, we can have multiple accumulators for the same key. When we are merging the results from each partition, if two or more partitions have an accumulator for the same key, we merge the accumulators using the user-supplied mergeCombiners() function. In the above example, we are just adding the 2 accumulators element-wise.

## Transformations on two (key-value) RDDs

```python
rdd1 = sc.parallelize([(1,2),(2,1),(2,2)])
rdd2 = sc.parallelize([(2,5),(3,1)])
print('rdd1=',rdd1.collect())
print('rdd2=',rdd2.collect())

# RETURNS
# ->
"""
rdd1= [(1, 2), (2, 1), (2, 2)]
rdd2= [(2, 5), (3, 1)]
"""
```

1. `subtractByKey`

Remove from RDD1 all elements whose key is present in RDD2.

```python
print('rdd1=',rdd1.collect())
print('rdd2=',rdd2.collect())
print("Result:", rdd1.subtractByKey(rdd2).collect())

# RETURNS
# ->
"""
rdd1= [(1, 2), (2, 1), (2, 2)]
rdd2= [(2, 5), (3, 1)]
Result: [(1, 2)]
"""
```

2. `join`

* A fundamental operation in relational databases.
* assumes two tables have a key column in common.
* merges rows with the same key.

Suppose we have `key,value` datasets

|dataset 1| |..........| dataset 2 | | |-------------|-------------------------------------| |-------------|-----------------| | key=name | (gender,occupation,age) | | key=name | hair color | | John | (male,cook,21) | | Jill | blond | | Jill | (female,programmer,19) | | Grace | brown |
| John | (male, kid, 2) | | John | black | | Kate | (female, wrestler, 54) |

When `Join` is called on datasets of type `(Key, V)` and `(Key, W)`, it returns a dataset of `(Key, (V, W))` pairs with all pairs of elements for each key. Joining the 2 datasets above yields:

|   key = name | (gender,occupation,age),haircolor |
|--------------|-----------------------------------|
| John         | ((male,cook,21),black)            |
| John         | ((male, kid, 2),black)            |
| Jill         | ((female,programmer,19),blond)    |

```python
print('rdd1=',rdd1.collect())
print('rdd2=',rdd2.collect())
print("Result:", rdd1.join(rdd2).collect())

# RETURNS
# ->
"""
rdd1= [(1, 2), (2, 1), (2, 2)]
rdd2= [(2, 5), (3, 1)]
Result: [(2, (1, 5)), (2, (2, 5))]
"""
```

## Variants of join.
There are four variants of join which differ in how they treat keys that appear in one dataset but not the other.

* `join` is an inner join which means that keys that appear only in one dataset are eliminated.
* `leftOuterJoin` keeps all keys from the left dataset even if they don't appear in the right dataset. The result of leftOuterJoin in our example will contain the keys John, Jill, Kate
* `rightOuterJoin` keeps all keys from the right dataset even if they don't appear in the left dataset. The result of leftOuterJoin in our example will contain the keys Jill, Grace, John
* `FullOuterJoin` keeps all keys from both datasets. The result of leftOuterJoin in our example will contain the keys Jill, Grace, John, Kate

    In outer joins, if the element appears only in one dataset, the element in (K,(V,W)) that does not appear in the dataset is represented bye None

3. `rightOuterJoin`

Perform a right join between two RDDs. Every key in the right/second RDD will be present at least once.

```python
print('rdd1=',rdd1.collect())
print('rdd2=',rdd2.collect())
print("Result:", rdd1.rightOuterJoin(rdd2).collect())

# RETURNS
# ->
"""
rdd1= [(1, 2), (2, 1), (2, 2)]
rdd2= [(2, 5), (3, 1)]
Result: [(2, (1, 5)), (2, (2, 5)), (3, (None, 1))]
"""
```

4. `leftOuterjoin`: Perform a left join between two RDDs. Every key in the left RDD will be present at least once.

```python
print('rdd1=',rdd1.collect())
print('rdd2=',rdd2.collect())
print("Result:", rdd1.leftOuterJoin(rdd2).collect())

# RETURNS
# ->
"""
rdd1= [(1, 2), (2, 1), (2, 2)]
rdd2= [(2, 5), (3, 1)]
Result: [(1, (2, None)), (2, (1, 5)), (2, (2, 5))]
"""
```

## Actions on (key,val) RDDs

```python
rdd = sc.parallelize([(1,2), (2,4), (2,6)])
```

1. `countByKey()`

Count the number of elements for each key. Returns a dictionary for easy access to keys.

```python
print("RDD: ", rdd.collect())
result = rdd.countByKey()
print("Result:", result)

# RETURNS
# ->
"""
RDD:  [(1, 2), (2, 4), (2, 6)]
Result: defaultdict(<class 'int'>, {1: 1, 2: 2})
"""
```

2. `collectAsMap()`

Collect the result as a dictionary to provide easy lookup.

```python
print("RDD: ", rdd.collect())
result = rdd.collectAsMap()
print("Result:", result)
# RETURNS
# ->
"""
RDD:  [(1, 2), (2, 4), (2, 6)]
Result: {1: 2, 2: 6}
"""
```

3. `lookup(key)`

Return all values associated with the provided key.

```python
print("RDD: ", rdd.collect())
result = rdd.lookup(2 )
print("Result:", result)

# RETURNS
# ->
"""
RDD:  [(1, 2), (2, 4), (2, 6)]
Result: [4, 6]
"""
```

## Summary
* We saw some more of the operations on Pair RDDs
* For more, see the spark RDD programming guide
* Next DataFrames and Spark-SQL


--- end {4.2_notebook.md} ---
--- start{4.2_slides.pdf} ---
Key-Value Operations
       DSC 232R
Operations on (key,val) RDDs
RDDs are similar to Python lists in that each element can be of a
different type and size.
(key.value) RDDs are useful to store and retrieve records. Examples:

• Key = SNN, Value = personal information
• Key = (Longitude, Latitude), Value = address
• Key = word, Value = number of occurrences
1.1 Three types of Spark operations
• Transformations
• Actions
• Shuffles
Transformations: RDD → RDD.
• Examples: map, filter, sample, more
• No communication needed.
Actions: RDD → Python-object in head node.
• Examples: reduce, collect, count, take, more
• Some communication needed.
Shuffles: RDD → RDD, shuffle needed
• Examples: sort, distinct, repartition, sortByKey, reduceByKey, join
  more
• A LOT communication might be needed.
1.4 Transformations on (key,value) rdds
1.4.1 reduceByKey(func)
Apply the reduce function on the values with the same key.
1.4.2 sortByKey():
Sort RDD by keys in ascending order.
1.4.3 mapValues(func):
Apply func to each value of RDD without changing the key.
1.5 Transformations on two (key-value) RDDs
1.5.1 1. subtractByKey:
Remove from RDD1 all elements whose key is present in RDD2.
1.5.2 2. join:
• A fundamental operation in relational databases.
• Assumes two tables have a key column in common.
• Merges rows with the same key.

Suppose we have two (key,value) datasets.
Accessible Table of Page 14

 Dataset 1                               Dataset 2
 key=name    (gender, occupation, age)   key=name    hair color
 John        (male, cook, 21)            Jil         blond
 Jill        (female, programmer, 19)    Grace       brown
 John        (male, kid, 2)              John        black
 Kate        (female, wrestler, 54)
When Join is called on datasets of type (Key, V) and (Key,
W), it returns a dataset of (Key, (V, W)) pairs with all pairs of
elements for each key. Joining the 2 datasets above yields:
Accessible Table of Page 16


     key = name   (gender, occupation, age), hair color
     John         ((male, cook, 21), black)
     John         ((male, kid, 2), black)
     Jill         ((female, programmer, 19), blond)
1.7 Actions on (key,val) RDDs
1.7.1 1. countByKey():
Count the number of elements for each key. Returns a dictionary for
easy access to keys.
1.7.2 2. collectAsMap():
Collect the result as a dictionary to provide easy lookup.
1.7.3 3. lookup(key):
Return all values associated with the provided key.
2 Summary
• We saw some more of the operations on Pair RDDs.
• For more, see the Spark RDD programming guide.
• An example of an actual Spark program: Word-Count.

--- end {4.2_slides.pdf} ---
--- start{4.2_transcript.txt} ---
- Hi. So, we talked about RDD's and how to, how to do transformations
and actions on RDD's, and now we're going to talk
about a special kind of RDD. RDD's in which each element
is a pair, a key and a value. So you can use any kind of, any kind of Python
structure inside an RDD. But this specific type
has a lot of support because then you'll see it's very useful. Okay so, here is an example. We have the key social security number, and the value is personal information. Or the key is longitude and latitude, and the value is the address. It corresponds to this location. And then the key is a word, and the value is the number
of occurrences of this word. We'll see that in the next
video when we do word counting. We have a key that identifies the record, and then the value is the record itself. So what kind of things
can we do with that? We can do three types of spark operations, transformations, actions, and shuffles. And just to remind you, transformations take an
RDD and generate a new RDD. And examples are map filter, sample, and you don't need any
communication for doing that. Each computer can do it by itself. Then, an action is something that collects all of the data from the RDD, and somehow comes up
with a single element, that is stored in the head note. In this case we have things like reduce, collect, count, take, and you need some communication. You need communication
from the workers machines to the head note. And finally we have shuffles, so shuffles are really
the heavy operations that we try to do not too many of because they take a lot of time. These are things like sort,
distinct, repartition, sortByKey, reduceByKey, and
we'll see what these things are. And these things require
a lot of communication, so basically data needs to
move from worker to worker, and that requires communication
of large amounts of data through the ethernet connecting them. Let's start with some
transformations on key value RDD's. The first one that is really very useful is something called reduceByKey. It generates a new RDD, where the, for each key you have the
result of the reduction on the values that are
associated with that key. So you can sum all of the
values that are associated with that particular word. So you have the sum of
these values for the word. So here is an example of that, suppose that you have an RDD that is an RDD consisting of three pairs. In each one of them the
first element is the key, and the second element is the value. The RDD, the reduce operation
that we're doing here is this, just summing. So we're basically summing
all of the elements according to one of the keys. So here we have two key values, 2 and 1. And one just, we keep it there because it's just one element. And for the other one we sum
the 4 and the 6 to get this 10. That's basically the result of the reduce. SortByKey is to take all
of our key value pairs and just sort them
according to the key value. That's a shuffle, and it basically says okay if I have these 3 elements, then I'm going to sort
them according to the key which is the first element 1, 2, 3, and what I get as the
result is the same pairs but ordered by their key. The map values is a simple operation. We're going to operate on each value without changing the key Here we have the operation which is take x and
replace it by 2 x, times 2. So what we get from the
original RDD being this, we just have the first pair, the key remains of the second is 4. They key remains of the
second is 2 times 4 is 8. The key remains and the value is 2 times 6 which is 12. Now notice that key value maps don't have to have the, each key appearing only once. This is the thing about
key value pairs in spark, that you don't have unique keys. You're not guaranteed unique keys. Now let's look at some transformations on two key value pairs. We take two RDD's, each one of them is a
collection of key value pairs and we want to somehow do an
operation to combine them. So subtractByKey means
that you take all the keys that are elements in RDD 2, all of the keys in RDD 2 and we remove the elements
with those keys from RDD 1. Here we have RDD 1, it
has the keys 1, 2, and 2. And RDD 2 has the keys 2 and 3. So they key 2 is the critical one, it basically removes these two pairs. And what we're left with is
just the first pair, 1 and 2. That's how that works. Join is one of the special operations that is meant to combine 2
key value pair collections. It is very much one of the key operations in relational database. This gives us some of the
relational database abilities inside spark. Suppose that we have these two datasets. Here we have dataset
1, the key is the name and the value is gender, occupation, age. These are the four
records that we have here. You notice that we have two Johns. And then here we have database 2, we have 3, we have basically a name
and then the hair color and here we have these 3 people. We want to do join, so
that means we want to take we want to take John here, here is John, and here is John, and John. And we basically want to add
that color black to John. And for Jill, we want
to add the color blonde. So Jill we have the color blonde, we add blonde here. And for Kate we, we don't have Kate over here, so we're not going to join those. How does that work? It returns, what the join operation returns, it returns a new key value pair. Key value collection, but
the key is the same key as appeared in the two sets
and the value is a pair. Which is the value from 1 set and the value from the other set. If you combine these 2 key value RDD's, what you get is a set that has only the, the common key. So John appears in both
and Jill appears in both. And then for each John we
have black hair we added, and for each Jill we added blonde. All right, so so here we have a small
join that we're doing just between 2 simple RDD's. And we have 1, 2, and 2 is key. Here, 1, 2, and 2. And in RDD 2 we have the keys 2 and 3. What we get in the result, is we get 2 record
corresponding to the key 2. Both of them have the key 2, which is the only common one. And then one of them has 1 and 5, so 1 taken from here and
the 5 taken from here, and then in the other one we get 2 and 5. Now here we get the 2 from
here and the same 5 from here. That's how this join works,
this is an inner join if you know the SQL terminology. There are, I think variants that you
can do other types of joins. What are the actions that
we have for key value RDD. We have countByKey, so we can count the number
of elements for each key. So just like count, but it
does it separately on each key and returns a new, returns a dictionary, it's no longer an RDD
that is in the head note. This is different from doing
reduceByKey with addition because that generates the same result but essentially as an RDD. It depends on whether
you want to say that, whether you assume that the
number of keys that you have is big or small. Let's consider the next
operation which is collectAsMap. CollectAsMap basically
takes a key value RDD and returns it as a dictionary. So it's easier to use on the python side. But there is one tricky thing here, which is that it only
returns one value per key. Which one it will choose,
there's no telling. And here is what we get
if we have this RDD, if we have this RDD, then 1, 2, 2, 4, and 2, 6. We see that there are 2
elements with the same key and it just chose one of them, 2, 6. So 1 maps to 2, and 2 maps to 6, the 2, 4 has been lost. Lookup is another operation
that is similar to databases. We want to return all the
values for a particular key. Here is this RDD again that
we had 1, 2, 2, 4, 2, 6. We ask to look up the key 2. So basically that give us
the 2, 4, and the 2, 6, so we return the values 4 and 6. This one does keep all of the
values associated with that. To summarize we saw some of
the operations with pair RDD. And there's many more, you can see this notebook has more detail than what I went over. Also there is the RDD program guide that you can use that's linked. And it will get you to
the full information about RDD's with just single elements
or with key value pairs. Next what we're going to do is, we're going to look at the actual program so we can see how RDD's
and all of these operations are used together to do something useful. I'll see you soon.
--- end {4.2_transcript.txt} ---
--- start{4.3_notebook.md} ---
# Word Count

Counting the number of occurances of words in a text is a popular first exercise using map-reduce.

## The Task

Input: A text file consisisting of words separated by spaces.
Output: A list of words and their counts, sorted from the most to the least common.

We will use the book "Moby Dick" as our input.

```python
mport os
os.environ["PYSPARK_PYTHON"]="python3"
os.environ["PYSPARK_DRIVER_PYTHON"] = "python3"

#start the SparkContext
from pyspark import SparkContext
sc=SparkContext(master="local[4]")

# set import path
import sys
sys.path.append('../../Utils/')
#from plan2table import plan2table

def pretty_print_plan(rdd):
    for x in rdd.toDebugString().decode().split('\n'):
        print(x)
```

## Define an RDD that will read the file
* Execution of read is lazy
* File has been opened.
* Reading starts when stage is executed.

```python
%%time
data_dir='../Data'
filename='Moby-Dick.txt'
text_file = sc.textFile(data_dir+'/'+filename)
type(text_file) 

# RETURNS
# ->
"""
CPU times: user 4 ms, sys: 0 ns, total: 4 ms
Wall time: 1.89 s
"""
```

## Steps for counting the words
* split line by spaces.
* map word to (word,1)
* count the number of occurances of each word.

```python
%%time
words =     text_file.flatMap(lambda line: line.split(" "))
not_empty = words.filter(lambda x: x!='') 
key_values= not_empty.map(lambda word: (word, 1)) 
counts=     key_values.reduceByKey(lambda a, b: a + b)

# RETURNS
# ->
"""
CPU times: user 16 ms, sys: 0 ns, total: 16 ms
Wall time: 239 ms
"""
```

## `flatMap()`

Note the line:

```python
words = text_file.flatMap(lambda line: line.split(" "))
```

Why are we using `flatMap`, rather than `map`?

The reason is that the operation `line.split(" ")` generates a list of strings, so had we used `map` the result would be an RDD of lists of words. Not an RDD of words.

The difference between `map` and `flatMap` is that the second expects to get a list as the result from the map and it concatenates the lists to form the RDD.

## The execution plan

In the last cell we defined the execution plan, but we have not started to execute it.

* Preparing the plan took ~100ms, which is a non-trivial amount of time,
* But much less than the time it will take to execute it.
* Lets have a look a the execution plan.

### Understanding the details

To see which step in the plan corresponds to which RDD we print out the execution plan for each of the RDDs.

Note that the execution plan for `words`, `not_empty` and `key_values` are all the same.

```python
pretty_print_plan(text_file)

# RETURNS
# ->
"""
(2) ../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []
 |  ../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []
"""
```

```python
pretty_print_plan(words)
# RETURNS
# ->
"""
(2) PythonRDD[6] at RDD at PythonRDD.scala:48 []
 |  ../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []
 |  ../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []
"""
```


```python
pretty_print_plan(not_empty)
# RETURNS
# ->
"""
(2) PythonRDD[7] at RDD at PythonRDD.scala:48 []
 |  ../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []
 |  ../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []
"""
```

```python
pretty_print_plan(key_values)
# RETURNS
# ->
"""
(2) PythonRDD[8] at RDD at PythonRDD.scala:48 []
 |  ../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []
 |  ../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []
"""
```

```python
pretty_print_plan(counts)
# RETURNS
# ->
"""
(2) PythonRDD[9] at RDD at PythonRDD.scala:48 []
 |  MapPartitionsRDD[5] at mapPartitions at PythonRDD.scala:122 []
 |  ShuffledRDD[4] at partitionBy at NativeMethodAccessorImpl.java:0 []
 +-(2) PairwiseRDD[3] at reduceByKey at <timed exec>:4 []
    |  PythonRDD[2] at reduceByKey at <timed exec>:4 []
    |  ../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []
    |  ../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []
"""
```

| Execution plan | RDD | Comments | | :---------------------------------------------------------------- | :------------: | :--- | |(2)_PythonRDD[6] at RDD at PythonRDD.scala:48 []| counts | Final RDD| |_/__MapPartitionsRDD[5] at mapPartitions at PythonRDD.scala:436 []| ---"--- | |_/__ShuffledRDD[4] at partitionBy at NativeMethodAccessorImpl.java:0 [| ---"--- | RDD is partitioned by key | |_+-(2)_PairwiseRDD[3] at reduceByKey at <timed exec>:4 []| ---"--- | Perform mapByKey | |____/__PythonRDD[2] at reduceByKey at <timed exec>:4 []| words, not_empty, key_values | The result of partitioning into words| | | | removing empties, and making into (word,1) pairs| |____/__../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at Nat| text_file | The partitioned text | |____/__../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMeth| ---"--- | The text source |

## Execution

Finally we count the number of times each word has occured. Now, finally, the Lazy execution model finally performs some actual work, which takes a significant amount of time.

```python
%%time
## Run #1
Count=counts.count()  # Count = the number of different words
Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y) # 
print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))
# RETURNS
# ->
"""
Different words=33781, total words=215133, mean no. occurances per word=6.37
CPU times: user 16 ms, sys: 0 ns, total: 16 ms
Wall time: 3.45 s
"""
```

### Amortization
When the same commands are performed repeatedly on the same data, the execution time tends to decrease in later executions.

The cells below are identical to the one above, with one exception at Run #3

Observe that `Run #2` take much less time that `Run #1`. Even though no `cache()` was explicitly requested. The reason is that Spark caches (or materializes) `key_values`, before executing `reduceByKey()` because performng reduceByKey requires a shuffle, and a shuffle requires that the input RDD is materialized. In other words, sometime caching happens even if the programmer did not ask for it.

```python
%%time
## Run #2
Count=counts.count()
Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)
print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))

# RETURNS
# ->
"""
Different words=33781, total words=215133, mean no. occurances per word=6.37
CPU times: user 12 ms, sys: 0 ns, total: 12 ms
Wall time: 414 ms
"""
```

### Explicit Caching
In `Run #3` we explicitly ask for `counts` to be cached. This will reduce the execution time in the following run by a little bit, but not by much.

```python
%%time
## Run #3, cache
Count=counts.cache().count()
Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)
print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))

# RETURNS
# ->
"""
Different words=33781, total words=215133, mean no. occurances per word=6.37
CPU times: user 8 ms, sys: 4 ms, total: 12 ms
Wall time: 605 ms
"""
```

```python
%%time
#Run #4
Count=counts.count()
Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)
print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))

# RETURNS
# ->
"""
Different words=33781, total words=215133, mean no. occurances per word=6.37
CPU times: user 8 ms, sys: 4 ms, total: 12 ms
Wall time: 297 ms
"""
```

```python
%%time
#Run #5
Count=counts.count()
Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)
print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))

# RETURNS
# ->
"""
Different words=33781, total words=215133, mean no. occurances per word=6.37
CPU times: user 8 ms, sys: 4 ms, total: 12 ms
Wall time: 430 ms
"""
```

## Summary

This was our first real pyspark program, hurray!

### Some things you learned:
1. An RDD is a distributed immutable array. It is the core data structure of Spark is an RDD.
2. You cannot operate on an RDD directly. Only through Transformations and Actions.
3. Transformations transform an RDD into another RDD.
4. Actions output their results on the head node.
5. After the action is done, you are using just the head node, not the workers.

### Lazy Execution
1. RDD operations are added to an Execution Plan.
2. The plan is executed when a result is needed.
3. Explicit and implicit caching cause internediate results to be saved.

Next: Finding the most common words.

# Finding the most common words
* `counts`: RDD with 33301 pairs of the form (word,count).
* Find the 5 most frequent words.
* Method1: collect and sort on head node.
* Method2: Pure Spark, collect only at the end.

## Method1: collect and sort on head node

### Collect the RDD into the driver node

* Collect can take significant time.

```python
%%time
C=counts.collect()

# RETURNS
# ->
"""
CPU times: user 52 ms, sys: 4 ms, total: 56 ms
Wall time: 206 ms
"""
```

### Sort

* RDD collected into list in driver node.
* No longer using spark parallelism.
* Sort in python
* will not scale to very large documents.

```python
%%time
C.sort(key=lambda x:x[1])
print('most common words\n'+'\n'.join(['%s:\t%d'%c for c in reversed(C[-5:])]))

# RETURNS
# ->
"""
most common words
the:	13766
of:	6587
and:	5951
a:	4533
to:	4510
CPU times: user 12 ms, sys: 0 ns, total: 12 ms
Wall time: 25.5 ms
"""
```

### Compute the mean number of occurances per word.

```python
Count2=len(C)
Sum2=sum([i for w,i in C])
print('count2=%f, sum2=%f, mean2=%f'%(Count2,Sum2,float(Sum2)/Count2))

# RETURNS
# -> count2=33781.000000, sum2=215133.000000, mean2=6.368462
```

## Method2: Pure Spark, `collect` only at the end.

* Collect into the head node only the more frquent words.
* Requires multiple stages

### Step 1 split, clean and map to `(word,1)`

```python
%%time
word_pairs=text_file.flatMap(lambda x: x.split(' '))\
    .filter(lambda x: x!='')\
    .map(lambda word: (word,1))

# RETURNS
# ->
"""
CPU times: user 0 ns, sys: 0 ns, total: 0 ns
Wall time: 35.3 µs
"""
```

### Step 2 Count occurances of each word.

```python
%%time
counts=word_pairs.reduceByKey(lambda x,y:x+y)

# RETURNS
# ->
"""
CPU times: user 8 ms, sys: 0 ns, total: 8 ms
Wall time: 42.6 ms
"""
```

### Step 3 Reverse ``(word,count)` to `(count,word)` and sort by key

```python
%%time
reverse_counts=counts.map(lambda x:(x[1],x[0]))   # reverse order of word and count
sorted_counts=reverse_counts.sortByKey(ascending=False)

# RETURNS
# ->
"""
CPU times: user 20 ms, sys: 4 ms, total: 24 ms
Wall time: 1.29 s
"""
```

### Full execution plan

We now have a complete plan to compute the most common words in the text. Nothing has been executed yet! Not even a single byte has been read from the `file Moby-Dick.txt`!

For more on execution plans and lineage see jace Klaskowski's blog

```python
print('word_pairs:')
pretty_print_plan(word_pairs)
print('\ncounts:')
pretty_print_plan(counts)
print('\nreverse_counts:')
pretty_print_plan(reverse_counts)
print('\nsorted_counts:')
pretty_print_plan(sorted_counts)

# RETURNS
# ->
"""
word_pairs:
(2) PythonRDD[30] at RDD at PythonRDD.scala:48 []
 |  ../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []
 |  ../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []

counts:
(2) PythonRDD[31] at RDD at PythonRDD.scala:48 []
 |  MapPartitionsRDD[23] at mapPartitions at PythonRDD.scala:122 []
 |  ShuffledRDD[22] at partitionBy at NativeMethodAccessorImpl.java:0 []
 +-(2) PairwiseRDD[21] at reduceByKey at <timed exec>:1 []
    |  PythonRDD[20] at reduceByKey at <timed exec>:1 []
    |  ../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []
    |  ../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []

reverse_counts:
(2) PythonRDD[32] at RDD at PythonRDD.scala:48 []
 |  MapPartitionsRDD[23] at mapPartitions at PythonRDD.scala:122 []
 |  ShuffledRDD[22] at partitionBy at NativeMethodAccessorImpl.java:0 []
 +-(2) PairwiseRDD[21] at reduceByKey at <timed exec>:1 []
    |  PythonRDD[20] at reduceByKey at <timed exec>:1 []
    |  ../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []
    |  ../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []

sorted_counts:
(2) PythonRDD[33] at RDD at PythonRDD.scala:48 []
 |  MapPartitionsRDD[29] at mapPartitions at PythonRDD.scala:122 []
 |  ShuffledRDD[28] at partitionBy at NativeMethodAccessorImpl.java:0 []
 +-(2) PairwiseRDD[27] at sortByKey at <timed exec>:2 []
    |  PythonRDD[26] at sortByKey at <timed exec>:2 []
    |  MapPartitionsRDD[23] at mapPartitions at PythonRDD.scala:122 []
    |  ShuffledRDD[22] at partitionBy at NativeMethodAccessorImpl.java:0 []
    +-(2) PairwiseRDD[21] at reduceByKey at <timed exec>:1 []
       |  PythonRDD[20] at reduceByKey at <timed exec>:1 []
       |  ../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []
       |  ../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []
"""
```

| Execution plan   | RDD |  Comments |    | :---------------------------------------------------------------- | :------------: | :--- |
    |`(2)_PythonRDD[6] at RDD at PythonRDD.scala:48 []`| **counts** | Final RDD|
    |`_/__MapPartitionsRDD[5] at mapPartitions at PythonRDD.scala:436 []`| **---\"---** |
    |`_/__ShuffledRDD[4] at partitionBy at NativeMethodAccessorImpl.java:0 [`| **---\"---** | RDD is partitioned by key |
    |`_+-(2)_PairwiseRDD[3] at reduceByKey at <timed exec>:4 []`| **---\"---** | Perform mapByKey |
    |`____/__PythonRDD[2] at reduceByKey at <timed exec>:4 []`| **words, not_empty, key_values** | The result of  partitioning into words|
    | | |  removing empties, and making into (word,1) pairs|
    |`____/__../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at Nat`| **text_file** | The partitioned text |
    |`____/__../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMeth`| **---\"---** | The text source |

### Step 4 Take the top 5 words

```python
%%time
D=sorted_counts.take(5)
print('most common words\n'+'\n'.join(['%d:\t%s'%c for c in D]))

# RETURNS
# ->
"""
most common words
13766:	the
6587:	of
5951:	and
4533:	a
4510:	to
CPU times: user 8 ms, sys: 0 ns, total: 8 ms
Wall time: 401 ms
"""
```

## Summary
We showed two ways for finding the most common words:

1. Collecting and sorting at the head node. -- Does not scale.
2. Using RDDs to the end.


--- end {4.3_notebook.md} ---
--- start{4.3_slides.pdf} ---
Word Count
   DSC 232R
1 Word Count
Counting the number of occurrences of words in a text is a popular
first exercise using MapReduce.
1.1 The Task
Input: A text file consisting of words separated by spaces.
Output: A list of words and their counts, sorted from the most to
the least common.

We will use the book “Moby Dick” as our input.
1.3 Define an RDD that will read the file
• Execution of read is lazy.
• File has been opened.
• Reading starts when stage is executed.
• What is a stage — explained later
1.4 Steps for counting the words
• Split line by spaces.
• Map word to (word, 1).
• Count the number of occurrences of each word.
1.5 The execution plan
In the last cell we defined the execution plan, but we have not
started to execute it.

   • Preparing the plan took ~100ms, which is a non-trivial amount
     of time,
   • But much less than the time it will take to execute it.
   • Let’s have a look at the execution plan.
1.5.1 Understanding the details
To see which step in the plan corresponds to which RDD we print
out the execution plan for each of the RDDs.

Note that the execution plan for words, not_empty, and
key_values are all the same.
        Accessible Table of Page 8
Execution plan                                                          RDD                 Comments
(2)_PythonRDD[6] at RDD at PythonRDD.scala:48 [ ]                       counts              Final RDD
_/__MapPartitionsRDD[5] at mapPartitions at PythonRDD.scala:436 [ ]     ---″---

__/___ShuffledRDD[4] at partitionBy at                                  ---″---             RDD is partitioned by
NativeMethodAccessorImpl.java:0 [                                                           key
__+-(2)_PairwiseRDD[3] at reduceByKey at <timed exec>:4 [ ]             ---″---             Perform mapByKey
___/__PythonRDD[2] at reduceByKey at <time exec>:4 [ ]                  words, not_empty,   The result of partitioning
                                                                        key_values          into words
                                                                                            Removing empties, and
                                                                                            making into (word,1)
                                                                                            pairs

___/__../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at Nat   text_file           The partitioned text
___/__../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMeth                       The text source
                                                                        ---″---
1.6 Execution
Finally, we count the number of times each word has occurred.
Now, the Lazy execution model finally performs some actual work,
which takes a significant amount of time.
1.7 Summary
• This was our first real PySpark program, hurray!
1.7.0.1 Some things you learned:
1. An RDD is a distributed immutable array. The core data
   structure of Spark is an RDD.
2. You cannot operate on an RDD directly. Only through
   Transformations and Actions.
3. Transformations transform an RDD into another RDD.
4. Actions output their results to your jupyter notebook.
5. Computations done after actions do not use Spark resources,
   only resources on the host of the jupyter notebook.
1.7.0.1 Lazy Execution
1. RDD operations are added to an Execution Plan.
2. The plan is executed when a result is needed.
3. Explicit and implicit caching cause intermediate results to be
   saved.

Next: Finding the most common words.
2 Finding the most common words
• counts: RDD with 33301 pairs of the form (word,count).
• Find the 5 most frequent words.
• Method1: collect and sort on head node.
• Method2: Pure Spark, collect only at the end.
2.1 Method1: collect and sort on head
node
2.1.1 Collect the RDD into the driver node

   • Collect can take significant time.




Note: Pen is covering “sys.”
2.1.2 Sort
• RDD collected into list in driver node.
• No longer using Spark parallelism.
• Sort in Python.
• Will not scale well to very large documents.
2.2 Method2: Pure Spark, collect only at
the end
• Collect into the head node only the more frequent words.
• Requires multiple stages.
2.2.1 Step 1 split, clean and map to
(word,1)
2.2.2 Step 2 Count occurrences of each word
2.2.3 Step 3 Reverse (word,count) to
(count,word) and sort by key




Note: Pen is covering “total.” Text underneath red ink is (lambda x:
x[1], x[0])).
2.2.4 Full execution plan
We now have a complete plan to compute the most common words
in the text. Nothing has been executed yet! Not even a single byte
has been read from the file Moby-Dick.txt!

For more on execution plans and lineage, see Jace Klaskowski’s
blog.
sorted_counts:
     Accessible Table of Page 23
Execution plan                                                           RDD
(2)_PythonRDD[20] at RDD at PythonRDD.scala:48 [ ]                       sorted_counts
__/___MapPartitionsRDD[19] at mapPartitions at PythonRDD.scala:436 [ ]   ---″---
_/__ShuffledRDD[18] at partitionBy at NativeMethodAccessorImpl.java:0    ---″---
_+-(2)_PairwiseRDD[17] at sortByKey at <timed exec>:2 [ ]                ---″---
___/__PythonRDD[16] at sortByKey at <timed exec>:2 [ ]                   ** counts, reverse_counts**
___/__MapPartitionsRDD[13] at MapPartitions at PythonRDD.scala:436 []    ---″---
___/__ShuffledRDD[12] at partitionBy at NativeMethodAccessorImpl.java    ---″---
___/+-(2)_PairwiseRDD[11] at reduceByKey at <timed exec>:1 [ ]           ---″---
_____/__PythonRDD[10] at reduceByKey at <timed exec>:1 [ ]               Word_pairs
_____/__../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at      ---″---
_____/__../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeM     ---″---
2.2.5 Step 4 Take the top 5 words




Note: Pen is covering “CPU” and “Wait.”
2.3 Summary
We showed two ways for finding the most common words:

1. Collecting and sorting at the head node. – Does not scale.
2. Using RDDs for everything, take(5) moves result to head node
   at the end.

See you next time!

--- end {4.3_slides.pdf} ---
--- start{4.3_transcript.txt} ---
- Hi. Today, we're going to
consider actual program that is used using the Spark system. And, that program is called Word Count. And, it is one of the
most standard examples of map reduce. Okay so, it's kind of like a
"hello world" of map reduce. Okay so, what does this word count task consist of? We're getting a text, basically, words separated
by spaces, commas or other, other submission point. And then, we output a list of the words that appear in the document and how many times each word appears, starting from the most common and ending with the least common. Okay so, that's what we want to do and this the idea is to
do it on a large text, so that you can really benefit from using the parallelism of Spark. Okay so, we're going to
use more particular input but really, this would be beneficial when you're talking about
doing it, let's say, for all of AP news wire
or something like that. Okay so, first of all,
we need to open the file and that is creating an RDD that acts at the conduit
for reading the file. The file is open at that point, but nothing is really read out of the file because as we said,
execution in spark is lazy. So, you wait until you
actually need some result before you start reading. So, here is the, the command. So, we say the text file which is going to be an RDD,
is spark context text file and then, it gets the path
and the file on it, okay. And that's what it, it generated, generate an RDD and it
doesn't take a lot of time because it doesn't really read anything. It just sets up the path. Okay so, how are we
going to do the counting? We're going to split the line by spaces. So, we're going to have
each word be an element in in the new RDD that is
now just made word by word and then, each word we're going to append to it at the end, one. So we're going to have a pair,
a value, a key value pair where the key is the word and the value is, essentially, the number of times that this word appears. But right now, we're just for each word we just put the number one, okay. And then, we're going to use that the the reduced by key to count these words. Okay so here is basically
the, the whole program. It has a set of operation. As we remember, these
operations don't necessarily get executed one by one as you read them. They just become a plan. And so, the first one is that we take the text file and
we do a flat map operation. So, we basically create
the flat map operation will create a list of lists. Okay. So, an RDD of lists. So this, from each line in the text, it creates a list of word and, and then, we're going to combine all of these lists
together into one big RDD where each element is one word. Then, we're going to do
some kind of cleanup. So, we're going to basically
filter all the words that are empty, that are
just the empty string. How do this come about? Because if we have multiple spaces, split will generate elements
that are just empty. Okay so, we don't want those. So, we just filter
those, filter those out. We just keep the one that are not empty. Then, we take that RDD and we map each word into the pair where word plus one, word and one, okay, just a pair where word is
a key and one is the value. And now, we do the main operation which is reduced by key to do summation. So for each word, we're going
to sum all of these ones. Each one appears with a
pair with, with that key. And so, summing them
all together will just give us the number of code, okay? So, this is the whole program, okay. And, and this point, it doesn't really yet, take a lot of time to execute because it's still just a plan. So, we can see this plan by using command that that to show us the plan. We can see the plan before
we actually execute it. And the, the plan corresponds to the RDDs that we have as named and also to intermediate result. So for instance, look at words
and not empty and key value. Those are three different RDDs. They're basically part of
the same step of execution which we call a stage. Okay so here is the,
the the, the whole plan. And, the plan essentially
starts at the bottom. So here is, here is the
part that where we opened where we created the, the link to the, to the file. So this says, okay, here is that this file that we're going to read. And then here we have that, we do the partitioning. So we do the, the, the splitting and then, we go and we do the reduce by, reduce by key. And, what you see is that
these steps over here, they're all done. They're all correspond to the
same step of the execution. So, they're just names that are the same step of the execution. Okay and then until we get to the top, that is basically the,
the end of the plan. So the plan goes from the bottom, all the way to the top. Okay. So, the execution is forced when we do simple operation that, that requires us to get a number. So if we say, if we say that on this counts RDD plan,
we just want to do count, we want to know how many
different words there are. Well, if we want to know how
many different words there are, we actually have to do
this whole operation of combining the words and generating the, the word and count appears. Okay. So even though this
is a very simple operation, it's a count operation. It actually takes a significant
amount of time, okay. Still not huge amount of time,
but almost a second. Okay. And then, this operation is generating the sum which is the total number of work, okay. So these are the two, two
things that we're generating. And, and in that sense, we, we are done. Okay so, so this, running this has generated here just
nine different word. The total word is nine. So this is because, because I didn't run the
notebook in the right way. So, when you write the
notebook in the right way, this will be more like 30,000
or something like that. Okay so, we finished our
first real pyspark program. And, here are some things that we learned. So, an RDD is distributed immutable array. It is a core data structure in Spark. You can't operate on the
data on the RDD directly only through Transformations and Actions. What I was saying, I call this
acting behind the curtain. Okay. So you are here, the
data is behind the curtain and you can only indirectly operate on it. So, Transformations
transform one RDD to another. Actions output the results
to your jupyter notebook or to the head node. Computations that you do after actions, do not use the Spark resources. So, if you basically did an
action and you get the result on your head node, then
you're not really using Spark. So, lazy execution is that
RDD operations are added to an execution plan that is executed only
when you need the result. Okay so when we did the count, that actually caused the, the, the whole plan to to, to operate. And then, there is implicit and explicit caching used to
save intermediate results. So you can explicitly say,
I want to cache this part or sometimes Spark would
say, I can't really continue from here unless I have
the, the actual results. So, I'm going to do implicit cache. Okay so, we did half the job, but there's another half that we wanna do which is to find the, the
most common word. Okay so, the RDD had thirty-three
thousand three hundred and one pairs of the form word count. And, and we want to find the five
most frequent words, okay. We just want to see on our string, on our, on our screen, the, the, the which are the five most common words and how many times each
one of them appear. Okay so, we're going to
talk about two method, just to kind of emphasize what Spark, pure Spark is and what not pure Spark. And so the first method is,
we are not going to use Spark, we're going to use collect. We're going to get all of
the counts into our machine and then, we're going to
just do everything locally. The method two is the pure Spark. We're going to do collect or some kind of action at the very end, but everything is going
to be done before the end. It's going to be done behind the screen. And, that's the preferred way because it scaled better
to larger dataset. Okay. So here is the, the naive method. You collect the RDD into the driver node. So basically, you just get C. That is the, that, that, that is all
of the counts collected. And so now, we have them
as a list of pairs in the, in the head node. And so now, we can basically
just do what we need to do in Python. Okay so, we take C and
we sort it according to the second element, element number one. And so that's basically
source them from the most, from the biggest to the, to from the, no, from the smallest to the biggest. And then, we're going to
print the last five element in this pair. And here, they are. The word, "the" appeared 13,000 times, the words, "Of" appeared 65, a thousand, six thousand times and so on. Okay. And, that didn't take a lot of time for us because it's at the
end, a pretty small set. So, so Python can deal
with it with no problem. However, want to show you the way that you would do it if you didn't want to pass this whole
dictionary, this whole list, key value pair, list with the words and the count, you wanted to, to do that sorting on the Spark side and then, just take the five largest ones. Okay so here, I'm just
repeating what we did before. We, we cleaned and mapped
the data to word one. okay. Then, we counted the
occurrence of each word. That's counts. And now, we're going to take counts and we're not going to collect it but instead, we're going to, to, to sort it in Spark. But, we want to sort according to key that the operation that we have and the key is right now, the word. So what we do is, we just
replace for each pair, we replace word count by count word. I can just replace the order. Okay so reverse counts is basically, maps X to maps the X to a reverse order X one, and then X data. So, it's the count and
then the word. Okay. And then, then once we have it this way, we can just use sort by key. Okay. So, that basically sorts
all of the elements but now, by the count
and not by the words. Okay. So, here is the full
execution part, a plan for this. So, you can see that they're
kind of leveled here. So, these are the levels that
are executed together. Right. So, once you reach this kind of jump that basically means that actually, the, the Spark did the work. And then, these are all collected and then this is doing the work. So this, these kind of things, these, these steps are called stages. So, stage is basically a
collection of operation that are done together. Okay so to get to the final results, we just do the operation take five. So that doesn't bring us the whole RDD, but it just brings us
the top five elements. And these top five elements are basically, these things, okay. So in this case it, the difference in execution time is not
really very different. But if it was, as I said,
instead of just Moby Dick, it would be the AP news
wire for the last 10 years. And, that's probably millions of words than such an operation doing it in the Spark side would be faster. Okay so, we showed two ways of finding
the most common words. The first is collecting and
sorting at the head node. But, that does not scale. That does not scale because as I said, basically it depends
on the amount of speed and memory that you have in the head node. And then, we have the way of
using RDDs for everything. Okay. So, I'll see you next time.
--- end {4.3_transcript.txt} ---
--- end {Week 2 Material} ---
--- {start Week 2 Quiz} ---

# Question 1

- which requires the most communicatio

a) shuffles #ANSWER

b) transformations

c) actions

d) no answer in text provided


---

# Question 2

- you are given RDD of string( each string contains words seprated by space) `IN`. your goal is to write a spark comand that will generate triplets(`first`,`second`,`third`) where `count` is the number of words `IN` that has the `length` and start with the letter`first`

```python
# start here
OUT = (IN
       .flatMap(lambda line: line.split())
       .map(lambda w: ((w[0], len(w)), 1))
       .reduceByKey(lambda a,b: a+b)
       .map(lambda kv: (kv[0][0], kv[0][1], kv[1])))
```

---

# Question 3

- you are given an RDD of key-value pairs `IN`. Both key and value are integers. write a command that outputs a key-value RDD `OUT` wher keys are the same as the keys `IN and the values is the average of the values associated with the key

```python
# start here
OUT = (IN
       .mapValues(lambda v: (v, 1))
       .reduceByKey(lambda a,b: (a[0]+b[0], a[1]+b[1]))
       .mapValues(lambda sc: sc[0]/sc[1]))
```

---

# Question 4

- the following question ask that you write code that computes a desired result.
- - the code should be real. it should produce the correct answer when run in a pyspark environment
- your answer must keep the information in an RDD form and translate the result into a pthon object only at the end 
- we recomend try your command in a pyspark notebook to make sure it works as intended, before writing the anser in the space

**You are given an RDD `R` of strings. write a single line command to cound the number of strings whose length is 2**

```python
# start here
ans = R.filter(lambda s: len(s) == 2).count()
```

# Question 5

- the following question ask that you write code that computes a desired result.
- - the code should be real. it should produce the correct answer when run in a pyspark environment
- your answer must keep the information in an RDD form and translate the result into a pthon object only at the end 
- we recomend try your command in a pyspark notebook to make sure it works as intended, before writing the anser in the space

**You are given an RDD `R` of word. Your task is to count the number of appearance of the first two letter(symbols) of the word. For example te first two letter of "The" are "Th" and the first two letters of "!" are "!". List the pairs (letter, count that appear from the most common one to the least common one. Don't list pairs that do not appear. For full points perform collect only at the end of computation**

```python
# start here
OUT = (R
       .map(lambda w: (w[:2], 1))
       .reduceByKey(lambda a,b: a+b)
       .sortBy(lambda kv: kv[1], ascending=False))
```

# Question 6

- Similar to the previous question, but now you count word pairs, not letter pairs
- More precisely: you are given an RDD `R` in which each elemnt is a sentence, where the words are seprated by sentences
- Youa re to transform each line into a list of pairs
- Example
  - "This ice cream is the best!"
  - (This,ice),(ice,cream),(cream,is),(is,the),(the,best)
- Your task is to count the number of pairs each type(same two words)
- As the previous question list the pair that appear from the most common to the least common one. Don't list pairs that do not exist.
- For full points perform collect only at the end 

```python
esult = (

R.

map(lambda line: line.split()).

flatMap(lambda words: [((u, v), 1) for u, v in zip(words, words[1:])]).

reduceByKey(lambda a, b: a + b).

sortBy(lambda x: x[1], ascending=False).
collect()

)
```
--- end {Week 2 Quiz} ---
--- start {Week 3 Material} ---
--- start{5.1_lec.pdf} ---
                  Dataframes
●   Dataframes are a restricted sub-type of RDDS
●   Restricting the type allows for more optimization
Dataframes store two dimensional data, similar to the type of data stored in a
spreadsheet

●   Each column in a dataframe can have a different type
●   Each row contains a record

Similar to pandas dataframes and R dataframes
Constructing a DataFrame from an RDD of Rows
Each row defines its own fields, the schema is inferred
Defining the Scheme explicitly
The advantage of creating a DataFrame using a predefined
scheme allows the content of the RDD to be simple tuples, rather
than rows.
Loading DataFrames from disk
There are many methods to load DataFrame from Disk. Here we
will discuss three of these methods.

1.   Parquet
2.   JSON (on your own)
3.   CSV (on your own)

In addition, there are API’s for connecting Spark to an external
database. We will not discuss this type of connection in this class.
                     Parquet Files

●   Parquet is a popular columnar format
●   Spark SQL allows SQL queries retrieve a subset of the rows
    without reading the whole file
●   Compatible with HDFS: allows parallel retrieval on a cluster
●   Parquet compresses the data in each column
●   <reponame>.parquet is usually a directory with many files
    or subdirectories

In addition, there are API’s for connecting Spark to an external
database. We will not discuss this type of connection in this class.
Let’s have a look at a real-world dataframe
The dataframe is a small part from a small part from a large
dataframe (15GB) which stores meteorological data from stations
around the world
                Summary
●   Dataframes are an efficient way to store data tables
●   All of the values in a column have the same type
●   A good way to store a dataframe in disk is to use a Parquet
    file
●   Next: Operations on dataframes

--- end {5.1_lec.pdf} ---
--- start{5.1_notebook.md} ---

Dataframes 


 Dataframes are a restricted sub-type of RDDs. 
 Restricting the type allows for more optimization.


 Dataframes store two dimensional data, similar to the type of data stored in a spreadsheet. 
   * Each column in a dataframe can have a different type.
   * Each row contains a `record`.


 Similar to [pandas dataframes](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe) and [R dataframes](http://www.r-tutor.com/r-introduction/data-frame)

```python
#import findspark
#findspark.init()
from pyspark import SparkContext
import os

os.environ["PYSPARK_PYTHON"]="python3"
os.environ["PYSPARK_DRIVER_PYTHON"] = "python3"

sc = SparkContext(master="local[4]")
sc.version

"""
RETURNS ->

3.5.0
"""
```

```python
import os
import sys

from pyspark import SparkContext
from pyspark.sql import SQLContext
from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType
%pylab inline

"""
RETURNS ->

%pylab is deprecated, use %matplotlib inline and import the required libraries.
Populating the interactive namespace from numpy and matplotlib
"""
```

```python
Just like using Spark requires having a SparkContext, using SQL requires an SQLContext
sqlContext = SQLContext(sc)
sqlContext 

"""
RETURNS ->

<pyspark.sql.context.SQLContext at 0x7fffd88057d0>
"""
```

#Spark sessions

[A newer API for spark dataframes](https://spark.apache.org/docs/latest/sql-getting-started.html#starting-point-sparksession)

We will stick to the old API in this class.

A new interface object has been added in **Spark 2.0** called **SparkSession**. A spark session is initialized using a `builder`. For example
```python
spark = SparkSession.builder \
         .master("local") \
         .appName("Word Count") \
         .config("spark.some.config.option", "some-value") \
         .getOrCreate()
```

Using a SparkSession a Parquet file is read [as follows:](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.parquet):
```python
df = spark.read.parquet('python/test_support/sql/parquet_partitioned')
```


##Constructing a DataFrame from an RDD of Rows
Each Row defines it's own  fields, the schema is *inferred*.

```python
One way to create a DataFrame is to first define an RDD from a list of Rows 
_list=[Row(name=u"John", age=19),
       Row(name=u"Smith", age=23),
       Row(name=u"Sarah", age=18)]
some_rdd = sc.parallelize(_list)
some_rdd.collect()

"""
RETURNS ->

[Row(name='John', age=19),
 Row(name='Smith', age=23),
 Row(name='Sarah', age=18)]
"""
```

```python
# The DataFrame is created from the RDD or Rows
# Infer schema from the first row, create a DataFrame and print the schema
some_df = sqlContext.createDataFrame(_list)

some_df.printSchema()

"""
RETURNS ->

root
 |-- name: string (nullable = true)
 |-- age: long (nullable = true)
"""
```


```python
A dataframe is an RDD of rows plus information on the schema.
performing **collect()* on either the RDD or the DataFrame gives the same result.
print(type(some_rdd),type(some_df))
print('some_df =',some_df.collect())
print('some_rdd=',some_rdd.collect())

"""
RETURNS ->

<class 'pyspark.rdd.RDD'> <class 'pyspark.sql.dataframe.DataFrame'>
some_df = [Row(name='John', age=19), Row(name='Smith', age=23), Row(name='Sarah', age=18)]
some_rdd= [Row(name='John', age=19), Row(name='Smith', age=23), Row(name='Sarah', age=18)]
"""
```

##Defining the Schema explicitly
The advantage of creating a DataFrame using a pre-defined schema allows the content of the RDD to be simple tuples, rather than rows.

```python
In this case we create the dataframe from an RDD of tuples (rather than Rows) and provide the schema explicitly
another_rdd = sc.parallelize([("John", 19), ("Smith", 23), ("Sarah", 18)])
Schema with two fields - person_name and person_age
schema = StructType([StructField("person_name", StringType(), False),
                     StructField("person_age", IntegerType(), False)])

# Create a DataFrame by applying the schema to the RDD and print the schema
another_df = sqlContext.createDataFrame(another_rdd, schema)
another_df.printSchema()
"""
RETURNS ->

root
 |-- age: binteger (nullable = true)
 |-- name: string (nullable = true)
"""
```

#Loading DataFrames from disk
There are many maethods to load DataFrames from Disk. Here we will discuss three of these methods
1. Parquet
2. JSON (on your own)
3. CSV  (on your own)

In addition, there are API's for connecting Spark to an external database. We will not discuss this type of connection in this class.


##Loading dataframes from JSON files
[JSON](http://www.json.org/) is a very popular readable file format for storing structured data.
Among it's many uses are **twitter**, `javascript` communication packets, and many others. In fact this notebook file (with the extension `.ipynb` is in json format. JSON can also be used to store tabular data and can be easily loaded into a dataframe.

```python
# when loading json files you can specify either a single file or a directory containing many json files.
print('--- json file')
path = "../Data/people.json"
!cat $path 

# Create a DataFrame from the file(s) pointed to by path
people = sqlContext.read.json(path)
print('\n--- dataframe\n people is a',type(people))
The inferred schema can be visualized using the printSchema() method.
people.show()

print('--- Schema')
people.printSchema()

"""
RETURNS ->

--- json file
{"name":"Michael"}
{"name":"Andy", "age":30}
{"name":"Justin", "age":19}

--- dataframe
 people is a <class 'pyspark.sql.dataframe.DataFrame'>
+----+-------+
| age|   name|
+----+-------+
|NULL|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+

--- Schema
root
 |-- age: long (nullable = true)
 |-- name: string (nullable = true)
"""
```
##Excercise: Loading csv files into dataframes

Spark 2.0 includes a facility for reading csv files. In this excercise you are to create similar functionality using your own code.

You are to write a class called `csv_reader` which has the following methods:

 `init__(self,filepath):` recieves as input the path to a csv file. It throws an exeption `NoSuchFile` if the file does not exist.
 `Infer_Schema()` opens the file, reads the first 10 lines (or less if the file is shorter), and infers the schema. The first line of the csv file defines the column names. The following lines should have the same number of columns and all of the elements of the column should be of the same type. The only types allowd are `int`,`float`,`string`. The method infers the types of the columns, checks that they are consistent, and defines a dataframe schema of the form:

```python
schema = StructType([StructField("person_name", StringType(), False),
                     StructField("person_age", IntegerType(), False)])
```

If everything checks out, the method defines a `self.` variable that stores the schema and returns the schema as it's output. If an error is found an exception `BadCsvFormat` is raised.
 `read_DataFrame()`: reads the file, parses it and creates a dataframe using the inferred schema. If one of the lines beyond the first 10 (i.e. a line that was not read by `InferSchema`) is not parsed correctly, the line is not added to the Dataframe. Instead, it is added to an RDD called `bad_lines`.
The methods returns the dateFrame and the `bad_lines` RDD.


##Parquet files


 [Parquet](http://parquet.apache.org/) is a popular columnar format.


 Spark SQL allows [SQL](https://en.wikipedia.org/wiki/SQL) queries to retrieve a subset of the rows without reading the whole file.


 Compatible with HDFS : allows parallel retrieval on a cluster.


 Parquet compresses the data in each column.


 `<reponame>.parquet` is usually a **directory** with many files or subdirectories.


##Spark and Hive
 Parquet is a **file format** not an independent database server.
 Spark can work with the [Hive](https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started) relational database system that supports the full array of database operations.
 Hive is compatible with HDFS.

```python
dir='../Data'
parquet_file=dir+"/users.parquet"
!ls $dir

"""
RETURNS ->

Moby-Dick.txt  namesAndFavColors.parquet  people.json  users.parquet  Weather
"""
```

```python
#load a Parquet file
print(parquet_file)
df = sqlContext.read.load(parquet_file)
df.show()

"""
RETURNS ->

../Data/users.parquet
+------+--------------+----------------+
|  name|favorite_color|favorite_numbers|
+------+--------------+----------------+
|Alyssa|          NULL|  [3, 9, 15, 20]|
|   Ben|           red|              []|
+------+--------------+----------------+
"""
```

```python
df2=df.select("name", "favorite_color")
df2.show()

"""
RETURNS ->

+------+--------------+
|  name|favorite_color|
+------+--------------+
|Alyssa|          NULL|
|   Ben|           red|
+------+--------------+
"""
```

```python
outfilename="namesAndFavColors.parquet"
!rm -rf $dir/$outfilename
df2.write.save(dir+"/"+outfilename)
!ls -ld $dir/$outfilename

"""
RETURNS ->

drwxr-xr-x 6 jovyan users 192 Apr 14 17:52 ../Data/namesAndFavColors.parquet
"""
```
#Lets have a look at a real-world dataframe

This dataframe is a small part from a large dataframe (15GB) which stores meteorological data from stations around the world.

```python
from os.path import split,join,exists
from os import mkdir,getcwd,remove
from glob import glob

create directory if needed
notebook_dir=getcwd()
data_dir=join(split(notebook_dir)[0],'Data')
weather_dir=join(data_dir,'Weather')

file_index='NY'
zip_file='%s.tgz'%(file_index)

weather_parquet = join(weather_dir, zip_file[:-3]+'parquet')
print(weather_parquet)
df = sqlContext.read.load(weather_parquet)
df.show(1)

"""
RETURNS ->


/home/jovyan/Library/CloudStorage/GoogleDrive-ssingal@ucsd.edu/My Drive/UCSD/Notes/6th Quarter - Spring 24/DSC 232R - BDA with Spark/GitHub/lecture-notebooks/Data/Weather/NY.parquet
+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+
|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|
+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+
|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|
+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+
only showing top 1 row
"""
```

```python
#selecting a subset of the rows so it fits in slide.
df.select('station','year','measurement').show(5)

"""
RETURNS ->

+-----------+----+-----------+
|    station|year|measurement|
+-----------+----+-----------+
|USW00094704|1945|   PRCP_s20|
|USW00094704|1946|   PRCP_s20|
|USW00094704|1947|   PRCP_s20|
|USW00094704|1948|   PRCP_s20|
|USW00094704|1949|   PRCP_s20|
+-----------+----+-----------+
only showing top 5 rows
"""
```
# Summary
 Dataframes are an efficient way to store data tables.
 All of the values in a column have the same type.
 A good way to store a dataframe in disk is to use a Parquet file.
 Next: Operations on dataframes.




--- end {5.1_notebook.md} ---
--- start{5.1_transcript.txt} ---
(upbeat music) - Hi, today we'll talk about data frames. Data frames are data
structure that is special for Spark and it's more
specialized than RDD. So it's a, you can say an
a restricted type of RDD. And by restricting the type of things that the RDD can store you can make things more efficient you can optimize things further. So data frames are, you can describe them as two-dimensional data,
similar to spreadsheets. And each column in the
spreadsheet contains contains a type of variable
can be of different types. And each row contains a record. So this is very similar
to panel data frames and our data frames if
you know about them. But this is implemented in the context of Spark
Parlor computation. So let's start by how do
you construct a data frame? So the simplest two way to do
it is to create a data frame for an RDD of row. So here is an example. We have a list of rows. So Row is a special
type that is associated with data frame. And the name of the row is John and the age is 19. So each row has, you know, feel and then if you paralyze this list and then you create an RDD
that collects these rows. Okay? So, so far we just have an RDD of these special things called rows. And then we make an RDD by using the SQL context
to create data train from this list. Okay, so, SQL context is
similar to, to the spark context and it's just built on top of it and gives you the capabilities
to deal with data frames. There's more cells in this
notebook that you can find if you look in your notebook directory under SQL directory and under that number one. Okay, so then once you
have this data frame, you can print it schema. So schema is basically a
description of what are the types of things that are
stored in the data frame. And here we have just
two column name and age. The name is a string and
the age is a long integer. And nullable equal true means that we allow this field
to have a null variable. So it's not a required field. So a data frame is an RDD of row with more information
to about the scheme. Okay? So if you collect
either the data frame the RDD that, that you had
initially, or the data frame from it, you basically get the
same, the same thing, right? So if you, here you have
some data frame collect and some RDD collect, you
get exactly the same thing. So once you bring it back to the head node it looks exactly the same. Okay.Sometimes when you have
large data frames with many many rows, it is better to
define the schema explicitly. That way you don't have to
define the names of things for each row, and you also
have more control over errors. Okay? So what we do here is
we first define this schema okay? And that that schema says what's
inside each, each column. Okay? So there's, there's a
person name at the string type and force means that
it's not nullble, okay? And then you have the the data structure of the the R D D contains the
information, but without name. And then what you do is
you create the data frames with this RDD and the
schema defined, okay? And when you print the schema you see that it's what you expected. So this is a way to define
things in a more structured way Okay? Now in typically what we have with data frames is very
large amount of data. So something on the order, let's
say, of gigabytes at least. And so the way to load efficiently
a data frame is different from the way to load
the efficiently and RDD. So while you can use
standards like JSON and CSV what I'm going to focus on is
this standard called Parquet or how to store your data frames on this. So Parquet is, is a
popular columnar format. Columnar relates to the fact
that things are stored column by column rather than row by row and Spark SQL which is this
framework we're talking about allows doing SQL query
directly on Parquet. So you get just the rows
that you want rather than reading all the rows in as an RDD and then filtering
what you don't want. And it's compatible with the HDFS. So it's compatible with this data with this had data file system. So you can basically
store the Parquet file in a distributed way and
have the speed up from that. And Parquet also makes files smaller by compressing each column because each column has
one type of variables. You can use methods of
compression to store the each column in a more compact way. So usually when you have
a Parquet, a directory it has some name and dot Parquet. It is usually a directory. It is usually not a single
file, but rather a directory and maybe a directory
with sub directories. It holds all of the structure all of your data and the structure the pointers that enable fast retrieval. Okay? So here we have a
particular small example. We have this Parquet file
in data users Parquet and when we want and then we can do, use
this command to read it read the Parquet file,
and then we show it. So that's very much like
doing held in, in pond. So you can show, can see a, a part a small part of the, of the table. And then you can do operations
on this like select, okay? So you can select just the name and the favorite color columns and you'll see a new data
frame that just had those. And you can similarly take a Parquet take a data frame and save
it into a Parquet file. Okay? So here, here is the way to do that. Okay? So it's a pretty simple operation. Okay, so now let's look a little bit at the real world data frame the one data frame that
we're going to use later on in the course. And this is a data frame that has row each row represents a
measurement that was done for a a particular location
in, in particular time. Okay? So this is, this
describes these measurement. And so you see that when you do the show, you
get all of these records all these names for the, for the, for
the different columns. And here is a content but one
row, cause I did show one. And you can select a subset. Okay? So here we do select
station, year and measurement. And we see, we show just five. Okay? So then we can
save the data frame again in a Parquet file. Okay? So summarize data frames
are an efficient way to store data table. All of the values in the
column have a same type and it's a good way to store disk. To store data frame on
disk is a Parquet file. Okay? So next we're going to talk about operations on data frames.
--- end {5.1_transcript.txt} ---
--- start{5.2_lec.pdf} ---
          Dataframe operations
Spark DataFrames allow operations similar to Pandas
dataframes. We demonstrate some of those.

For more see the official guide and this article.
    But first, some important details
Spark and datahub can run in two modes:
 ● Local mode which means that it is run on the same computer as
    the head node. This convenient for small jobs and for debugging.
    Can also be done on your laptop.

●   Remote mode in which the head node and the worker nodes are
    separate. This mode requires that the spark cluster is up and
    running. In this case the full resources of the clusters are
    available. This mode is useful when processing hard jobs.
                     .(describe)
The method df.describe() computers five statistics for each
column of the dataframe df.

The statistics are count,mean,std,min,max
               groupby and agg
The method groupby(col) groups rows according the value of the
column (col).

The method .agg(spec) computes a summary for each group
specified in spec
 Using SQL Queries on DataFrames
There are two main ways to manipulate DataFrames:
          Imperative Manipulation
Using python methods such as .select and .groupby.

●   Advantage: order of operations is specified
●   Disadvantage: you need to describe both what is the result
    you want and how to get it.
    Declarative Manipulation (SQL)

●   Advantage: You need to describe only what is the result you
    want
●   Disadvantage: SQL does not have primitives for common
    analysis operations such as covariance
Counting the number of occurrences of each measurement,
comparatively
Counting the number of occurrences of each measurement,
declaratively

 Registering a dataframe as a table in a database.

 In order to apply SQL commands to a dataframe, it has to first be
 registered as a table in the database managed by sqlContext.
Performing a map command

●   Dataframes do not support map and reduce operations.
●   In order to perform a map or reduce on a dataframe, you first
    need to transform it into an RDD
Performing a map command

●   Dataframes do not support map and reduce operations.
●   In order to perform a map or reduce on a dataframe, you first
    need to transform it into an RDD

●   This is a quick-and-dirty solution.
●   A better way is to use built-in sparkSQL functions
●   Or if you can’t find what you need, you can try and create a
    User-Defined-Function* (UDF)
Aggregations

●   Aggregation can be used, in combination with built-in spark
    SQL functions, to compute statistics of a dataframe.
●   Computation will be fast thanks to combined optimizations
    with database operations.

●   A partial list: count(), approx_count_distinct(), avg(),
    max(), min()
●   Of these, the interesting one is approx_count_distinct()
    which uses sampling to get an approximate count fast.
Approximate Quantile

●   Suppose we want to partition the years into 10 ranges
●   Such that in each range we have approximately the same
    number of records.
●   The method approxQuantile will use a sample to do this for
    us
Approximate Quantile

●   Suppose we want to partition the years into 10 ranges
●   Such that in each range we have approximately the same
    number of records.
●   The method approxQuantile will use a sample to do this for
    us
Approximate Quantile

  ●    Suppose we want to partition the years into 10 ranges
  ●    Such that in each range we have approximately the same number of records.
  ●    The method approxQuantile will use a sample to do this for us
Reading rows selectively from Parquet
Suppose we are only interested in snow measurements. We can apply an
SQL query directly to the Parquet files. As the data is organized in
columnar structures, we can do the selection efficiently without loading the
whole file to memory.

Here the file is small, but in real applications it can consist of hundreds of
millions of records. In such cases loading the data first to memory and then
filtering it is very wasteful.
                 Summary
●   Dataframes can be manipulated decleratively, which allows for
    more optimization.
●   Dataframes can be stored and retrieved from Parquet files.
●   It is possible refer to directly to a parquet file in an SQL query.
●   See you next time!

--- end {5.2_lec.pdf} ---
--- start{5.2_notebook.md} ---

## Dataframe operations
Spark DataFrames allow operations similar to pandas Dataframes. We demonstrate some of those.
 
For more, see the [official guide](https://spark.apache.org/docs/latest/sql-programming-guide.html) and [this article](https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/)


## But first, some important details
Spark on datahub can run in two modes:


* **local mode** which means that it is run on the same computer as the head node. This is convenient for small jobs and for debugging. Can also be done on your laptop.


* **remote mode** in which the head node and the worker nodes are separate. This mode requires that the spark cluster is up and running. In this case the full resources of the clusters are available. This mode is useful when processing large jobs.

```python

import os
import sys

import pyspark
from pyspark import SparkContext
from pyspark.sql import SQLContext
from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType,BinaryType
import os

os.environ["PYSPARK_PYTHON"]="python3"
os.environ["PYSPARK_DRIVER_PYTHON"] = "python3"

%%time
sc = SparkContext('local', 'test')
sc

"""
RETURNS ->

CPU times: user 21.4 ms, sys: 10 ms, total: 31.4 ms
Wall time: 5.08 s
"""

```

```python

%%time

sqlContext = SQLContext(sc)
sqlContext

"""
RETURNS ->

CPU times: user 3.67 ms, sys: 2.15 ms, total: 5.81 ms
Wall time: 134 ms
"""
```

```python
from os.path import split,join,exists
from os import mkdir,getcwd,remove
from glob import glob

# create directory if needed
notebook_dir=getcwd()
data_dir=join(split(notebook_dir)[0],'Data')
weather_dir=join(data_dir,'Weather')

file_index='NY'
zip_file='%s.tgz'%(file_index)

# %%
weather_parquet = join(weather_dir,zip_file[:-3]+'parquet')
print(weather_parquet)
df = sqlContext.read.load(weather_parquet)
df.show(1)

"""
RETURNS ->

+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+
|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|
+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+
|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|
+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+
only showing top 1 row
"""
```

```python
df.printSchema()

"""
RETURNS -> 

root
 |-- Station: string (nullable = true)
 |-- Measurement: string (nullable = true)
 |-- Year: long (nullable = true)
 |-- Values: binary (nullable = true)
 |-- dist_coast: double (nullable = true)
 |-- latitude: double (nullable = true)
 |-- longitude: double (nullable = true)
 |-- elevation: double (nullable = true)
 |-- state: string (nullable = true)
 |-- name: string (nullable = true)
"""
```

```python
%%time
print(df.count())
df.show(1)

"""
RETURNS ->

168398
+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+
|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|
+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+
|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|
+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+
only showing top 1 row

CPU times: user 5.49 ms, sys: 858 µs, total: 6.34 ms
Wall time: 1.6 s
"""
```


### .describe()
The method `df.describe()` computes five statistics for each column of the dataframe `df`.
 
The statistics are: **count, mean, std, min,max**

You get the following man page using the command `df.describe?`
 
```
Signature: df.describe(*cols)
Docstring:
Computes statistics for numeric and string columns.
 
This include count, mean, stddev, min, and max. If no columns are
given, this function computes statistics for all numerical or string columns. 
.. note:: This function is meant for exploratory data analysis, as we make no
    guarantee about the backward compatibility of the schema of the resulting DataFrame.
 
>>> df.describe(['age']).show()
+-------+------------------+
|summary|               age|
+-------+------------------+
|  count|                 2|
|   mean|               3.5|
| stddev|2.1213203435596424|
|    min|                 2|
|    max|                 5|
+-------+------------------+
>>> df.describe().show()
+-------+------------------+-----+
|summary|               age| name|
+-------+------------------+-----+
|  count|                 2|    2|
|   mean|               3.5| null|
| stddev|2.1213203435596424| null|
|    min|                 2|Alice|
|    max|                 5|  Bob|
+-------+------------------+-----+
 
.. versionadded:: 1.3.1
File:      ~/spark-2.2.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py
Type:      method
```

```python
df.describe().select('Station','Measurement','Year').show() 

"""
RETURNS ->

+-----------+-----------+------------------+
|    Station|Measurement|              Year|
+-----------+-----------+------------------+
|     168398|     168398|            168398|
|       NULL|       NULL|1963.4289124573927|
|       NULL|       NULL|30.586766032145377|
|USC00300015|       PRCP|              1871|
|USW00094794|   TOBS_s20|              2013|
+-----------+-----------+------------------+
"""
```

#### groupby and agg
The method `.groupby(col)` groups rows according the value of the column `col`.  
The method `.agg(spec)` computes a summary for each group as specified in `spec`

```python
df.groupby('Measurement').agg({'Year': 'min', 'Station':  'count'}).show()

"""
RETURNS ->

+-----------+--------------+---------+
|Measurement|count(Station)|min(Year)|
+-----------+--------------+---------+
|   TMIN_s20|         13442|     1873|
|       TMIN|         13442|     1873|
|   SNOW_s20|         15629|     1884|
|       TOBS|         10956|     1876|
|   SNWD_s20|         14617|     1888|
|   PRCP_s20|         16118|     1871|
|   TOBS_s20|         10956|     1876|
|       TMAX|         13437|     1873|
|       SNOW|         15629|     1884|
|   TMAX_s20|         13437|     1873|
|       SNWD|         14617|     1888|
|       PRCP|         16118|     1871|
+-----------+--------------+---------+
"""
```

```python
# THis command will load the python module that defines the SQL functions
%load ls ~/spark-latest/python/pyspark/sql/functions.py
```

### Using SQL queries on DataFrames
 
There are two main ways to manipulate  DataFrames:


#### Imperative manipulation
Using python methods such as `.select` and `.groupby`.
* Advantage: order of operations is specified.
* Disrdavantage : You need to describe both **what** is the result you want and **how** to get it.


#### Declarative Manipulation (SQL)
* Advantage: You need to describe only **what** is the result you want.
* Disadvantage: SQL does not have primitives for common analysis operations such as **covariance**


### Using sql commands on a dataframe
Spark supports a [subset](https://spark.apache.org/docs/latest/sql-programming-guide.html#supported-hive-features) of the Hive SQL query language.
 
For example, You can use [Hive `select` syntax](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select) to select a subset of the rows in a dataframe.

To use sql on a dataframe you need to first `register` it as a `TempTable`.
 
for variety, we are using here a small dataframe loaded from a JSON file.


### Counting the number of occurances of each measurement, imparatively

```python
%%time
L=df.groupBy('measurement').count().collect()
#L is a list of Rows (collected DataFrame)

"""
RETURNS ->

CPU times: user 5.34 ms, sys: 2.28 ms, total: 7.62 ms
Wall time: 1.03 s
"""
```

```python
D=[(e.measurement,e['count']) for e in L]
print('The most common mesurements')
sorted(D,key=lambda x:x[1], reverse=True)[:6]

"""
RETURNS ->

The most common mesurements
[('PRCP_s20', 16118),
 ('PRCP', 16118),
 ('SNOW_s20', 15629),
 ('SNOW', 15629),
 ('SNWD_s20', 14617),
 ('SNWD', 14617)]
"""
```


```python
print('The most rare mesurements')
sorted(D,key=lambda x:x[1], reverse=False)[:6]

"""
RETURNS ->

The most rare mesurements
[('TOBS', 10956),
 ('TOBS_s20', 10956),
 ('TMAX', 13437),
 ('TMAX_s20', 13437),
 ('TMIN_s20', 13442),
 ('TMIN', 13442)]
"""
```


### Counting the number of occurances of each measurement, declaratively.
 
#### Registrering a dataframe as a table in a database
 
In order to apply SQL commands to a dataframe, it has to first be registered as a table in the database managed by sqlContext.

```python
# using older sqlContext instead of newer (V2.0) sparkSession
sqlContext.registerDataFrameAsTable(df,'weather') 

%%time
query="""
SELECT measurement,COUNT(measurement) AS count,
                   MIN(year) AS MinYear 
FROM weather  
GROUP BY measurement 
ORDER BY count DESC
"""
print(query)
sqlContext.sql(query).show(5)

"""
RETURNS ->

SELECT measurement,COUNT(measurement) AS count,
                   MIN(year) AS MinYear 
FROM weather  
GROUP BY measurement 
ORDER BY count DESC

+-----------+-----+-------+
|measurement|count|MinYear|
+-----------+-----+-------+
|   PRCP_s20|16118|   1871|
|       PRCP|16118|   1871|
|   SNOW_s20|15629|   1884|
|       SNOW|15629|   1884|
|   SNWD_s20|14617|   1888|
+-----------+-----+-------+
only showing top 5 rows

CPU times: user 2.53 ms, sys: 3.47 ms, total: 6 ms
Wall time: 1.06 s
"""
```

#### Performing a map command
* Dataframes do not support `map` and `reduce` operations.
* In order to perform a `map` or `reduce` on a dataframe, you first need to transform it into an RDD.


* This is a quick-and-dirty solution. 
* A better way is to use [built-in sparkSQL functions](https://spark.apache.org/docs/latest/api/sql/index.html)
* Or if you can't find what you need, you can try and create a [User-Defined-function* (UDF)](https://spark.apache.org/docs/latest/sql-ref-functions-udf-scalar.html)

```python
df.rdd.map(lambda row:(row.Station,row.Year)).take(5)

"""
RETURNS ->

[('USW00094704', 1945),
 ('USW00094704', 1946),
 ('USW00094704', 1947),
 ('USW00094704', 1948),
 ('USW00094704', 1949)]
"""
```


#### Aggregations 
* **Aggregation** can be used, in combination with built-in sparkSQL functions 
to compute statistics of a dataframe.
* computation will be fast thanks to combined optimzations with database operations.

* A partial list : `count(), approx_count_distinct(), avg(), max(), min()`

* Of these, the interesting one is `approx_count_distinct()` which uses sampling to get an approximate count fast.

```python
import pyspark.sql.functions as F # used here just for show.

df.agg({'station':'approx_count_distinct'}).show()

"""
RETURNS ->

+------------------------------+
|approx_count_distinct(station)|
+------------------------------+
|                           339|
+------------------------------+
"""
```

#### Approximate Quantile
 
* Suppose we want to partition the years into 10 ranges
* such that in each range we have approximately the same number of records.
* The method `.approxQuantile` will use a sample to do this for us.

```pyython
%%time
print('with accuracy 0.1: ',df.approxQuantile('Year', [0.1*i for i in range(1,10)], 0.1))

"""
RETURNS ->

with accuracy 0.1:  [1871.0, 1926.0, 1947.0, 1957.0, 1966.0, 1969.0, 1980.0, 1989.0, 2013.0]
CPU times: user 12.2 ms, sys: 3.98 ms, total: 16.2 ms
Wall time: 1.39 s
"""
```
```pyython
%%time
print('with accuracy 0.001: ',df.approxQuantile('Year', [0.1*i for i in range(1,10)], 0.00001))

"""
RETURNS ->

with accuracy 0.001:  [1917.0, 1937.0, 1949.0, 1957.0, 1966.0, 1975.0, 1984.0, 1993.0, 2003.0]
CPU times: user 11.6 ms, sys: 3.24 ms, total: 14.8 ms
Wall time: 1.09 s
"""
```

### Reading rows selectively from Parquet
Suppose we are only interested in snow measurements. We can apply an SQL query directly to the 
parquet files. As the data is organized in columnar structure, we can do the selection efficiently without loading the whole file to memory.
 
Here the file is small, but in real applications it can consist of hundreds of millions of records. In such cases loading the data first to memory and then filtering it is very wasteful.

```python
query="""SELECT station,measurement,year 
FROM parquet.`%s` 
WHERE measurement=\"SNOW\" """%weather_parquet
print(query)
df2 = sqlContext.sql(query)
print(df2.count(),df2.columns)
df2.show(5)

"""
RETURNS ->

SELECT station,measurement,year 
FROM parquet.`NY.parquet` 
WHERE measurement="SNOW" 
15629 ['station', 'measurement', 'year']
+-----------+-----------+----+
|    station|measurement|year|
+-----------+-----------+----+
|USC00308600|       SNOW|1932|
|USC00308600|       SNOW|1956|
|USC00308600|       SNOW|1957|
|USC00308600|       SNOW|1958|
|USC00308600|       SNOW|1959|
+-----------+-----------+----+
only showing top 5 rows
"""
```

## Summary

* Dataframes can be manipulated decleratively, which allows for more optimization.
* Dataframes can be stored and retrieved from Parquet files.
* It is possible to refer directly to a parquet file in an SQL query.
* See you next time!


## References
* For an introduction to Spark SQL and Dataframes see: [Spark SQL, DataFrames](https://spark.apache.org/docs/latest/sql-programming-guide.html#spark-sql-dataframes-and-datasets-guide)
* Also [spark-dataframe-and-operations](https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/) from [analyticsvidhya.com](https://www.analyticsvidhya.com)
 
For complete API reference see
* [SQL programming guide](https://spark.apache.org/docs/latest/sql-programming-guide.html) For Java, Scala and Python (Implementation is first in Scala and Python, later pyspark)
* [pyspark API for the DataFrame class](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame) 
* [pyspark API for the pyspark.sql module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module)


--- end {5.2_notebook.md} ---
--- start{5.2_transcript.txt} ---
(gentle music) (intro frame whooshes) - Okay, so we just talked a
little bit about DataFrames. What they are, how to load
them and write them back. And now we're going to talk
about DataFrame operations, which are the reason that
we like to use DataFrames, because they can do things
much more efficiently than RDD. So this notebook, again,
has much more detail, then you can have links to
the official guide and so on. But let's start. Okay, so the first thing
that I want to tell you is when you write- When you run, When you run Spark, you can run it either in local mode. So it just runs in your
environment on your computer and then you're restricted to whatever files you have
directly in your computer. Or you can run it in remote mode, and then it really uses multiple cores, and the remote storage
that is usually bigger, and you can do things much faster. Okay, so here we do the
standard loading of context. And then we start the SparkContext. Okay? So here is the SparkContext
and some details about it. Here it's running local. And then we load the next level, which is the SQL content, okay? So in order to have DataFrames, we have to have also the SQLContext, and that we generate by SQL, okay, You generate an SQLContext
from the SparkContext. Okay. So now we're going to read "parquet_file", a pretty large one. And this is the command for doing that. And we're going to look at the schema. So how is- what is in this DataFrame? And what is in this DataFrame
is a column called "Station", a column called "Measurement",
a column called "Year", and column called "Values". So the special one here is "Values", which it says it's binary. So it's really just a long
binary string or a binary array. And we need to provide a way
to map it to what we need. And what we, what we need here is, is an array of 365 floating point numbers. And the mapping from that of that to a binary array and back
is something that we need to we need to take care of. So the "values" is basically if people are familiar with database this is what's sometimes
called a block, a binary block. Okay? So here we look at
how many rows we have. So we have something like 12 million rows and we look at just the
first row and you see that it has a station name,
measurement type, a year, and then it has these
binary values basically byte by byte in hexadecimal. So the first kind of
thing that we want to do is know basic statistics
about each column. So we can use count, mean,
standard deviation, min and max and then this is the command
described that generate it. And here is how it looks. You will have station, we have the, the number of rows, and we have we have the minimum and the maximum. That's the, the mean and the average the mean and the standard
deviation are not defined because it's a string. And here we have the first string and the last string in
alpha numerical order. Similar here. And here in year we have more
value because it is a number. So it's the same number of rows but this is the mean of the years. 1977, it is the standard deviation. And this is this beginning
year, 1764 to 2022. So 1764 is the first year that
is recorded in this database. Now obviously 1764
didn't have any computers but still people logged the temperatures and those made it eventually
into this database. Okay? So another operation
that we can do efficiently on DataFrames are group by and aggregate. So group by basically collects groups. The the row by some value, by some column. And then the aggregate performs and performs some aggregation
for each, for each one. So here we are doing group by measurement. So we have the different types of measurement and we
look at, in the aggregate we look at the minimum for
year and the count of stations how many stations and how many years. So what we see here is that
the counts are very, very very different from each other, right? So this TMIN had a very large number of measurements in the
first year that was that in the first year that
it was measured, 1764. So it's a very long-standing thing which goes well with the fact that we have so many stations
that we counted in this case stations simply mean how many how many rows we have for each one. Okay? So basically we can use SQL on DataFrames and there are
just two styles doing that. The first is the imperative style. So we basically write the
commands that we want to execute on the DataFrame as as methods that operate on the DataFrame and the sequence of methods
of select or group by. And the advantage is that
you know what you're writing in terms of order of operation, right? You're basically saying
exactly what order to do the operation. The disadvantage is again,
that you draw the order of operations because it
might be doing the operations in a different order will
give you the same result but more efficiently. So that's a disadvantage of doing things in this imperative way. A declarative way is SQL, if you maybe know about SQL,
it's a declarative language. So what you need to
describe is only what it is that you want to calculate. And you don't need to, you don't
need to calculate the exact or to define the best way to do it, right? So you leave that to be
optimizer, but on the other hand you are putting yourself in a narrower box so you have something that
you can do very efficiently and some things that are
close to impossible to do. So that's, that's the trade off. Okay? So let's look at an
example counting the number of occurrences of each
measurement imperative. So we take the DataFrame
and we group by measurement and then we collect those into L. Okay? So now we have a list. So we, we group, then we
count, then we collect into L. And then when we look into
L and we sort it, right? So we take, we take B and
then we sort B according to the count we see which are
the counts that are measured that we get the most of right? So precipitation, TMIN, TMAX, snow, snow depth are the
things that we get a lot of measurements in our table. On the other hand, if we look at the beginning of the
list or the other end we see the things that we have
very, very few measurements. So there is like this SN57, SX15 you can go and read the description of these in the documentation. But the point is that even though there
are millions of rows very few of these rows are
any one of these measurements. So when you do statistics you'd usually want to
avoid trying to do analysis of the measurement that
appears so few times. Okay, let's do now the same thing in terms of by using SQL,
by declaratively using it. So in order to uses SQL on your on your DataFrame, you
need to register it. There's a kind of a formal operation in which you take your DataFrame and you make it into
a table in a database. So this is the operation
and all that is done here is that you register the DataFrame
as a table. DataFrame is DF, and weather is the name that
you will use in your SQL query. So here is an example of that. So we basically have a query in SQL which says select measurement and then count measurement at count. So how many measurements
you call it count. And then you have the minimum
of year as many years. So you define it, define count and min year as local variable. And you do it from
weather, this weather table that we just registered and we group by measurements and we
order by count descending. Okay? So what you see
here is a declaration of what it is that we want to compute. And then when we do the, we execute this query. We get a new table, a new DataFrame and then we can show this DataFrame. Okay? So, so here it
is, select measurement. So this is the command and now here is the result of what we did. And you see we get the exact same result. But in a way you can say
this is more understandable and easier to, to debug than the way that you do the using declarative. Okay, so one of the command that we had we used a lot in RDD is a map, right? We take an RDD of item then we map each item to a new item. So this command is not available to us when we do DataFrames. We can't do a map command
on each row, let's say. So in order to, to do that we first need to translate it into an RDD. So basically once it's
now translated into RDD it's an RDD approach and now we can do whatever
operations we want to do in the map, then we can do reduce then we can map it back
to maybe a DataFrame. So this is a quick and dirty solution. In general, it is best,
better to not do that but use whatever is available
directly for DataFrames. And that's in this link here, you can get to a list of those and if you want to then you create a user
defined function or UDF which is your function, but that operates in the context of a DataFrame. And that is quite an advanced topic. So that's kind of one
way that you can do it but in general you would do it only when you tried already to do
the RDD and it's too slow. Okay, so here is how do
you do with the math. You basically DF dot RDD the RDD makes an RDD
out of your DataFrame. And then you have the
map where the function is you take the row and you map it into row station and row year, okay? And then you take five. So you get these, just these two columns. So this is exactly like select but we did it through map operations. Okay, aggregations are
ways of doing statistics in the Spark SQL and, and
it's a fast way to do those. So here are some of these operations. So these are operations that live inside the Sparks SQL environment. And so you, so they will be,
they don't require moving back to an RDD and they are count approximate count distinct,
average, max and min. So these are just some of
them, there's a longer list and the one that I'm going to look at is the approximate count distinct which basically allows you
to create a sample, right? So, and use an approximate count, okay? So So here is an approximate
count distinct of station and it gives you the number of stations but it's not the exact
number because it just because it just, it's just an estimate. And another thing that you can
do is approximate quantile. So suppose we want to partition the years into ranges, 10 ranges
such that in each range we have approximately the
same number of records. So we have 10%, 20%, 30%, and so on. So approximate quantile
will do this for us and we give it the level of approximation. So here is the first time that we run it the approximate quantile,
it's with accuracy 0.1. And you get these numbers and that took about one second. And then if we want to do
an accuracy that is much much higher, 0.001, then we get different different numbers than
before because the ac- the estimation is much more accurate. So for instance, 1931 instead of 1764 that's a big difference. 1951 versus 1948, that's
three year different one year different, one year different two year different and so on. So you get a more accurate estimate but it takes you more computation. So here this was five seconds. Okay? So the accuracy, the accuracy is basically how
many examples you'd sample before you count how many
different ones there are. Okay? So last thing I want
to describe in general about about DataFrames and Parquet is that one of the really nice thing about Parquet is that the data can be all on this and you don't need to bring it all in in order to get the answer
to a, to an SQL query. So here is, here is an example of that. We basically are doing select
station measurement gear from parquet dot s. So that's the, the actual file. And this lives in Parquet path. And so this whole thing is a query and we can run this query directly on SQL context without
reading the file explicitly. So this is what we're doing,
select these variables from parquet datasets, weather
datasets, weather parquet and where the measurement is snow and and these are the columns. And here is what, what we, what we got. Okay? So we basically got just the the measurement when
the measurement is snow we got all of those elements. So that's a much smaller
set of rows I think. So we just selected only
the roles that I have to do with snow and that, and that basically was giving us it gives us the right, that that gives us the number, just these rows about 1 million out of the
12 million that we have. And these are the, these
are the top five rows. Okay? So the nice thing
about this command is that this way of writing a command is that this parquet path
is not going to be read in whole into memory and then manipulated. So that makes a very big difference when you have very large files. Okay, so to summarize DataFrames can be manipulated
declaratively, which allows for more optimization.
DataFrames can be stored and retrieved from Parquet files and it is possible to refer directly to the Parquet file in an SQL query. Okay? And we talked
about aggregation methods and how to do them
efficiently inside DataFrame. Okay, so I'll see you next time.
--- end {5.2_transcript.txt} ---
--- start{6.1_lec.pdf} ---
4.4 Spark
Architecture


DSC 232R, Class 4 : Spark - 2
April 13th
Hardware Organization




      In local installation, cores serve as master & slaves
Spatial Software Organization
• The Cluster Master                                        • The driver runs on the
  manages the                       Spark Driver              master
  computation                                               • It executes the “main()”
  resources.                                                  code of your program
• Each worker manages
  a single core
                                  Cluster Master
                                  Mesos, YARN, or
                                    Standalone



 Cluster Worker                    Cluster Worker                    Cluster Worker

     Executor                          Executor                           Executor

                • Each RDD is partitioned among the workers
                • Workers manage partitions and Executors
                • Executors execute task on their partition, are myopic
Spatial Organization (more detail)
                                                    Worker Node
                                                      Executor   Cache

 Driver Program                                        Task      Task


     SparkContext
                           Cluster Manager
                                                    Worker Node
                                                      Executor   Cache


                                                        Task     Task




 • SparkContext (sc) is the abstraction that encapsulates the cluster for
   the driver node (and the programmer)
 • Worker nodes manage resources in a single slave machine
 • Worker nodes communicate with the cluster manager
 • Executors are the processes that can perform tasks.
 • Cache refers to the local memory on the slave machine
Materialization

   Consider RDD1
   -> Map (x: x*x) -> RDD2
   ->Reduce (x,y:x+y)-> float (in head node)


   RDD1 -> RDD2 is a lineage
   RDD2 can be consumed as it is being generated.
   Does not have to be materialized = stored in memory
RDD Processing




 • RDDs, by default, are not materialized
 • They do materialize if cached or
   otherwise persisted.
 Temporal Organization


Recall Spatial
Organization




                         A stage ends when
                         the RDD needs to be
                         materialized
Terms and Concepts of Execution

   RDDs are partitioned across workers.
   RDD graph defines the Lineage of the RDDs.
   SparkContext divides the RDD graph into stages which define the execution plan (or
    physical plan)
   A task corresponds to the to one stage, restricted to one partition.
   An executor is a process that performs tasks.
Summary

   Spark computation is broken into tasks
   Spatial organization: different data
   partitions on different machine
   Temporal organization: Computation
   is broken into stages. a sequence of stages.
   Next: persistence and checkpointing

--- end {6.1_lec.pdf} ---
--- start{6.1_transcript.txt} ---
(bright notes) (title whooshes) - Hi. We talked about how to program in Spark and how to, basically,
take advantage of RDDs, but the real question
is what does this buy us in terms of speed, okay? So, in order to understand that, we need to delve a little bit deeper into the architecture of
Spark and how it works. So, roughly speaking, Spark is one of those architectures that is based on a head node, a master node, and which communicates with slave nodes. And the slave nodes do all the work while the master node
just coordinates the work. And the question is how
to coordinate the work so that the slaves would
always have work to do, useful work to do, and would make as much as advantage of the parallelism as possible. So, we're going to talk about
partitioning the process of computation in time, or temporal, and in space, or spatial. Okay? So, we're going to start by the spatial. And in the spatial, we have basically, the logical image of what we just saw. We have the driver, that's this head node, and that through a set of software, that is the Cluster Master, the Mesos, YARN, or Standalone, it communicates with the worker nodes. Okay? So, these worker nodes have the parts that actually do the execution of the code to do the computation. Okay. So, to elaborate on this a little bit, the driver runs on the master, okay? Also, it executes the
"main" code of your program. So, that's like the part of the code that is really just the organizing part. The Cluster Master manages
computer resources. So, that's what you communicate with when you do Spark Cluster, when you create a Spark Cluster. And each worker is another manager which manages just a single core, okay? So, you have management at multiple levels just to make sure that
resources are used efficiently. And the Executor is just
running on its own little piece. So, the RDD is partitioned across the workers. So, you have parts of the RDD being stored and processed in different workers, and the worker manages partitions. So those are the, these data parts, and Executors. So, those are the pieces of code that run to manipulate the partitions. And the Executors execute
tasks on their partition. And so, they're myopic. They don't know anything
about the overall structure of how this computation is organized. They just get a piece of code to be executed on a piece of data. So, the piece of code is the task and the piece of data is the partition. Here is the same in a
little bit more detail. You have the driver node, which, through the SparkContext, talks with a Cluster Manager. The Cluster Manager is aware of all of the different resources, the amount of memory and the amount of CPUs that
you have in each machine. And it spawns Executors,
one for each core. And in the Executor, you have a part that is the cache, which is the same cache
as we talked about before, which is storing intermediate
solutions and tasks, sequence of tasks to
execute on this cache. So, the SparkContext is an abstraction that encapsulates the
cluster for the driver node and the programmer. And the worker nodes manage resources in a single slave machine. So, each machine is actually
Java virtual machine. And in it, you have various resources, amount of memory and so on. And so, that's managed by the worker node. Worker nodes communicate
with the cluster manager and the Executors are the
processes that can perform tasks. Okay? So, as I said before, the Executors are these slave processes that execute a particular sequence of commands called the task
on a particular partition. And the cache refers to the local memory on the slave machine. So, that's where you
really pay for having cache and storing intermediate results. So, materialization is
the name of the game. When we're running long and
complicated processes or tasks, we want the use of the cache to be minimal because our data is so big. So, for instance, to consider an RDD that you map x to x squared to RDD2 and then reduce from x, from RDD2 to the sum of all of the elements. So, RDD1 to RDD2 is a lineage and RDD2 can be consumed
as it is being generated. So, it doesn't need to
be materialized, okay? Does not have to be materialized
or stored in memory. So, how does this look
like as a general process? We have an RDD or we have a whole sequence of RDDs, one calculated from the other. The first one is maybe
created from a file on disk and then we calculate the
next one from that one, the next one from that one, and so on. And going backwards, that's what we call the lineage. This RDD comes from a
lineage of RDDs before it. And the transformations are
these processes that move us from one RDD to the next. Then, eventually, we take an action and that results in something
that is smaller than an RDD and which we take in the head node. So, all of this maintained
in the worker nodes, partitioned and maintained
in the worker nodes. And some of these might be materialized and others might not. So, by default, these
RDDs are not materialized. They're just conceptual. They're just a pointer to
a place in the computation. They do materialize if they're cached or otherwise persistent. And we'll talk about the difference between caching and general
persistence in a few slides. Now we go to the next, to the other dimension, which is temporal organization, or how the computation
is performed across time. Okay? And so, suppose that we
have this computation that we're starting with
reading a text file, then doing a map, then a filter, and then reduced by key. If we think about what needs
to be materialized here, the only thing that
needs to be materialized is after the filtered. After the filtered, when we want to do reduced by key, we need a materialized version. But all of these previous ones do not need to be materialized. So, what does that mean? It means that these RDDs
that are not materialized are simply an internal part of our task. They're not something
that writes into memory. Okay? So, this is what we call, sorry. This is what we call a stage. Stage is the part of the
computation that can be done without materialization
other than at the end. So, this stage ends when the
RDD needs to be materialized. And now, imagine this temporal
partitioning being crossed with this spatial partitioning. And that's where we get the
actual working environment of Spark. So, RDDs are partitioned across workers and the RDD graph defines
the lineage of the RDD. So, what RDD comes from
previous RDD and so on. SparkContext divides the
RDD graph into stages. So, these stages are those
pieces that can be executed without writing back to memory. And that is what is
called the physical plan. Okay? So that's, these stages
are the temporal pieces and a task corresponds to one stage, restricted to one partition. Okay? So we have the stage, which is a little piece
of software that is, gets actually sent to the worker and the worker also has a
partition it's working on. And it tells the Executor, "Do this task on, do this
stage, on this partition." And that stage and partition
is what is called a task. Okay? So, the Executor is a
process that performs tasks. So, to summarize, Spark
computation is broken into tasks. Spatial organization, different data is partitioned
on to different machines and temporal organization, computation is broken into stages, a sequence of stages. And next we're going to
talk about persistence and checkpointing. So, I'll see you then.
--- end {6.1_transcript.txt} ---
--- start{6.2_lec.pdf} ---
Levels of Persistence

   Caching is useful for retaining intermediate results
   On the other hand, caching can consume a lot of memory
   If memory is exhausted, caches can be eliminated, spilled to disk etc.
   If needed again, cache is recomputed or read from disk.
   The generalization of .cache() is called .persist() which has many options.
Storage Levels




  http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence
Checkpointing

•   Spark is fault tolerant. If a slave machine crashes, it's RDD's will be recomputed.
•   If hours of computation have been completed before the crash, all the computation
    needs to be redone.
•   Checkpointing reduces this problem by storing the materialized RDD on a remote
    disk.
•   On Recovery, the RDD will be recovered from the disk.
•   It is recommended to cache an RDD before checkpointing it.
           http://takwatanabe.me/pyspark/generated/generated/pyspark.RDD.checkpoint.html​
Checkpoint vs. Persist to Disk

•   Cache materializes the RDD and keeps it in memory (and/or disk). But the lineage
    （ computing chain ） of RDD (that is, seq of operations that generated the
    RDD) will be remembered, so that if there are node failures and parts of the
    cached RDDs are lost, they can be regenerated.
•   Checkpoint saves the RDD to an HDFS file and actually forgets the lineage
    completely. This allows long lineages to be truncated and the data to be saved
    reliably in HDFS, which is naturally fault-tolerant by replication.
•   https://github.com/JerryLead/SparkInternals/blob/master/markdown/english/6-Cac
    heAndCheckpoint.md

--- end {6.2_lec.pdf} ---
--- end {Week 3 Material} ---
--- start {Week 3 Quiz} ---
**Help prep here**
--- end {Week 3 Quiz} ---
--- end {Week 4 Material} ---
--- start{7.1_lec.pdf} ---
7.1 Weather
Analysis-
Massachusett
s
DSC 232R, Class 7 : Weather Data
Week 4
Weather Data: Initial Visualization

Notebook Link


For MA


In [1]: state=“MA”
Data Source


   The data is from
    NOAA and is called GHCND,
    you can get the details and
    download the
    data from AWS S3 Bucket
   We translate the raw data
    from the S3 Bucket into
    parquet files that store tables
    called “weather” and
    “stations”
Read Data


Show 3 rows of joined weather+stations table
Read Data
Read Data




            Vectors of 365 values
Read Data
Read Data
Read or Compute Statistics
  Information for State
Read or Compute Statistics Information for
Sate
Read or Compute Statistics Information for
Sate
Print Statistics for TOBS
Print Statistics for TOBS

      Temp Observed




                        TMAX
                        TMIN




            Celsius
Print Statistics for TOBS
Distribution of Undefined Elements in
        Yearly Measurements
Distribution of Undefined Elements in Yearly
Measurements
Distribution of Undefined Elements in Yearly
Measurements
Find Out the Definition of
   Measurement Types
Find Out the Definition of
Measurement types
  Find the Number of
Measurements (years) for
  Each Station in State
Find the Number of Measurements
(years) for Each Station in State
 Get All Measurements for
        year=1945,
measurement=“TOBS” and
        state=“MA”
5 of the 35 ; TOBS measurements




                 July     Jan
Plot all 35 the TOBS measurements
Zooming Into a Small Number of Days the Correlations between Stations are Clear
How Different Measurements Behave, Relate to Each Other




                                        TMAX
                                        TMIN
                                        TOBS
Script for Plotting Yearly Plots
                 Statistics Across the State
                             Distribution of Missing Observations
   • The distribution of missing observations is not uniform throughout the year. We visualize it
                                                below




Valid
Measurement
s




                          Day of the
                          Year
Statistics Across the State:
T Observed & Precipitation
Statistics Across the State: Snow Depth
Plots of Mean and Standard
      Deviation(STD) of
        Observations
Plots of Mean and STD of Observations
Plots of Mean and STD of Observations
Plots of Mean and STD of Observations (Precipitation and
                         Snow)
Plots of Mean and STD of Observations (Snow Depth)
Conclusion
• We loaded the weather data from Parquet files
• We explored statistics for the data
• We explored where there are a lot of empty cells – limits the
  accuracy of the statistics
• We visualized different measurements as a function of the
  day of the year

NEXT: Using PCA for more refined analysis

--- end {7.1_lec.pdf} ---
--- start{7.1_notebook.md} ---
Here are the converted Markdown files based on your instructions. I have combined the weather notebooks into `7.1_notebook.md` and separated the others as requested, preserving all details and outputs.

### 7.1_notebook.md

*(Contains: Processing Weather Files, Weather Analysis - VT, and Weather Analysis - MA)*

```markdown
# Processing Weather Files

### NOAA Global Historical Climatology Network Daily (GHCN-D)

the noaa data was downloaded from an S3 bucket descried here

https://registry.opendata.aws/noaa-ghcn/

#### The files and subdirs in noaa

### Main Data Directory
* **98G	csv.gz**

### Documentation
* 36K	ghcn-daily-by_year-format.rtf
* 6.5K	ghcnd-countries.txt
* 32M	ghcnd-inventory.txt
* 6.5K	ghcnd-states.txt
* 9.9M	ghcnd-stations.txt
* 6.5K	ghcnd-version.txt
* 36K	index.html
* 4.1M	mingle-list.txt
* 6.5K	readme-by_year.txt
* 30K	readme.txt
* 6.5K	status-by_year.txt
* 41K	status.txt

### Logs
* 37M	download.log

```python
import pandas as pd

```

```python
!head -4 file.sizes
"""
RETURNS ->

total 102208244
-rw-r--r-- 1 yfreund root      532654 Apr  5 15:59 1750.csv
-rw-r--r-- 1 yfreund root       25276 Apr  5 16:00 1763.csv
-rw-r--r-- 1 yfreund root       25283 Apr  5 16:00 1764.csv
"""

```

```python
!tail -4 file.sizes
"""
RETURNS ->

-rw-r--r-- 1 yfreund root  1220722777 Apr  5 15:57 2020.csv
-rw-r--r-- 1 yfreund root  1207975107 Apr  5 15:58 2021.csv
-rw-r--r-- 1 yfreund root   276207286 Apr  5 15:59 2022.csv
-rw-r----- 1 yfreund users          0 Apr  9 16:22 file.sizes
"""

```

```python
i=0
D={}
with open('file.sizes','r') as S:
    S.readline(); S.readline()
    for line in S.readlines():
        if 'file' in line:
            continue
        #print(line[:-1])
        L=line.split()
        size=int(L[4])
        year=int(L[-1][:-4])
        #print(year,size)
        D[year]=size

```

```python
years=[]
sizes=[]
for year in sorted(D.keys()):
    years.append(year)
    sizes.append(D[year])

```

```python
%pylab inline
"""
RETURNS ->

%pylab is deprecated, use %matplotlib inline and import the required libraries.
Populating the interactive namespace from numpy and matplotlib
"""

```

```python
#figure(figsize=[15,10])
plot(years,sizes)
grid()
title('Size of GHCN-Daily data files (in Bytes) by year')
ylabel('Bytes')
xlabel('Year')
"""
RETURNS ->

[Plot/Image Output: Size of GHCN-Daily data files by year]
"""

```

---

# Weather Analysis - VT Voc Quiz 4

## Weather Data : Initial Visualization

### For VT

```python
state="VT"

```

```python
#sc.stop()

```

```python
import pandas as pd
import numpy as np
import sklearn as sk
%pylab inline
"""
RETURNS ->

%pylab is deprecated, use %matplotlib inline and import the required libraries.
Populating the interactive namespace from numpy and matplotlib
"""

```

```python
%%time
%run lib/startup-voc.py
"""
RETURNS ->

172.17.0.2
sparkContext= <SparkContext master=local[2] appName=pyspark-shell>

    pandas as    pd 	version=2.0.3 	required version>=0.19.2

     numpy as    np 	version=1.24.4 	required version>=1.12.0

   sklearn as    sk 	version=1.3.1 	required version>=0.18.1

module urllib has no version
   pyspark as pyspark 	version=3.5.0 	required version>=2.1.0

ipywidgets as ipywidgets 	version=8.1.1 	required version>=6.0.0

version of ipwidgets= 8.1.1

measurements is a Dataframe (and table) with 12720632 records
stations is a Dataframe (and table) with 119503 records
weather is a Dataframe (and table) which is a join of measurements and stations with 12720632 records
CPU times: user 181 ms, sys: 33.1 ms, total: 214 ms
Wall time: 5.4 s
"""

```

```python
import warnings  # Suppress Warnings
warnings.filterwarnings('ignore')

_figsize=(10,7)

```

## Read Data

### show 3 rows of joined weather+stations table

```python
%%time
### Total number of stations
stations.count()
"""
RETURNS ->

CPU times: user 675 µs, sys: 77 µs, total: 752 µs
Wall time: 55.2 ms
119503
"""

```

```python
%%time
weather=measurements.join(stations,on='station')
weather.show(3)
"""
RETURNS ->

+-----------+-----------+----+--------------------+--------+---------+---------+------------+-------------------+-----+-------+
|    Station|Measurement|Year|              Values|latitude|longitude|elevation|  dist2coast|               name|state|country|
+-----------+-----------+----+--------------------+--------+---------+---------+------------+-------------------+-----+-------+
|AG000060390|       TAVG|2022|[79 00 6C 00 66 0...| 36.7167|     3.25|     24.0|   8.0234375| ALGER-DAR EL BEIDA|     |Algeria|
|AGE00147716|       TAVG|2022|[85 00 83 00 7C 0...|    35.1|    -1.85|     83.0|0.5224609375|NEMOURS (GHAZAOUET)|     |Algeria|
|AGM00060360|       TMIN|2022|[5A 00 19 FC 4B 0...|  36.822|    7.809|      4.9|  3.16015625|             ANNABA|     |Algeria|
+-----------+-----------+----+--------------------+--------+---------+---------+------------+-------------------+-----+-------+
only showing top 3 rows

CPU times: user 2.43 ms, sys: 1.19 ms, total: 3.61 ms
Wall time: 892 ms
"""

```

```python
weather.count()
"""
RETURNS ->

12720632
"""

```

```python
sqlContext.registerDataFrameAsTable(weather,'weather')

```

```python
ms=['TMAX', 'SNOW', 'SNWD', 'TMIN', 'PRCP', 'TOBS']
# ms=['TMAX', 'TMIN', 'TOBS']
cms='or\n'.join(['Measurement="%s" '%(m) for m in ms])

##  read all data for state
Query="""
SELECT *
FROM weather
WHERE state="%s" and 
(%s)
"""%(state,cms)
print(Query)
"""
RETURNS ->


SELECT *
FROM weather
WHERE state="VT" and 
(Measurement="TMAX" or
Measurement="SNOW" or
Measurement="SNWD" or
Measurement="TMIN" or
Measurement="PRCP" or
Measurement="TOBS" )

"""

```

```python
%%time
weather_df=sqlContext.sql(Query)
print('number of rows in result=',weather_df.count())
weather_df.show(2)
"""
RETURNS ->

number of rows in result= 27068
+-----------+-----------+----+--------------------+--------+---------+---------+----------+----------------+-----+-------------+
|    Station|Measurement|Year|              Values|latitude|longitude|elevation|dist2coast|            name|state|      country|
+-----------+-----------+----+--------------------+--------+---------+---------+----------+----------------+-----+-------------+
|US1VTAD0019|       SNOW|2022|[00 00 00 00 0D 0...| 44.1006| -73.1172|    151.5|     222.0|NEW HAVEN 2.4 SE|   VT|United States|
|US1VTAD0019|       SNWD|2022|[33 00 00 00 19 F...| 44.1006| -73.1172|    151.5|     222.0|NEW HAVEN 2.4 SE|   VT|United States|
+-----------+-----------+----+--------------------+--------+---------+---------+----------+----------------+-----+-------------+
only showing top 2 rows

CPU times: user 1.02 ms, sys: 2.18 ms, total: 3.2 ms
Wall time: 1.52 s
"""

```

### read or compute statistics information for state.

```python
%%time
import os.path
from lib.computeStatistics import computeStatistics
from pickle import dump,load

weather_dir=parquet_root+'/weather_statistics/'
if not os.path.isdir(weather_dir):
    os.mkdir(weather_dir)

pkl_filename=parquet_root+'weather-statistics/'+state+'-'+','.join(ms)+'.pkl'
print(pkl_filename)
!ls $pkl_filename
"""
RETURNS ->

../Data/Weather/weather/datasets/weather-statistics/VT-TMAX,SNOW,SNWD,TMIN,PRCP,TOBS.pkl
../Data/Weather/weather/datasets/weather-statistics/VT-TMAX,SNOW,SNWD,TMIN,PRCP,TOBS.pkl
CPU times: user 4.95 ms, sys: 3.54 ms, total: 8.49 ms
Wall time: 215 ms
"""

```

```python
%%time
if os.path.isfile(pkl_filename):   
    print('precomputed statistics file exists')
    with open(pkl_filename,'br') as pkl_file:
        STAT=load(pkl_file)
else:
    print('computing statistics')
    STAT=computeStatistics(sqlContext,weather_df,measurements=ms)
    with open(pkl_filename,'bw') as pkl_file:
        dump(STAT,pkl_file)

STAT.keys()
"""
RETURNS ->

precomputed statistics file exists
CPU times: user 0 ns, sys: 21.2 ms, total: 21.2 ms
Wall time: 34.9 ms
dict_keys(['TMAX', 'SNOW', 'SNWD', 'TMIN', 'PRCP', 'TOBS'])
"""

```

```python
ms=['TMAX', 'SNOW', 'SNWD', 'TMIN', 'PRCP', 'TOBS']
# ms=['TMAX', 'TMIN', 'TOBS']
cms='or\n'.join(['Measurement="%s" '%(m) for m in ms])
cms
"""
RETURNS ->

'Measurement="TMAX" or\nMeasurement="SNOW" or\nMeasurement="SNWD" or\nMeasurement="TMIN" or\nMeasurement="PRCP" or\nMeasurement="TOBS" '
"""

```

```python
print("   Name  \t                 Description             \t  Size")
print("-"*80)
print('\n'.join(["%10s\t%40s\t%s"%(s[0],s[1],str(s[2])) for s in STAT_Descriptions]))
"""
RETURNS ->

   Name  	                 Description             	  Size
--------------------------------------------------------------------------------
     UnDef	      sample of number of undefs per row	vector whose length varies between measurements
        NE	         count of defined values per day	(366,)
SortedVals	                        Sample of values	vector whose length varies between measurements
      mean	                              mean value	()
       std	                                     std	()
    low100	                               bottom 1%	()
   high100	                                  top 1%	()
   low1000	                             bottom 0.1%	()
  high1000	                                top 0.1%	()
         E	                   Sum of values per day	(366,)
      Mean	                                    E/NE	(366,)
         O	                   Sum of outer products	(366, 366)
        NO	               counts for outer products	(366, 366)
       Cov	                                    O/NO	(366, 366)
       Var	  The variance per day = diagonal of Cov	(366,)
    eigval	                        PCA eigen-values	(366,)
    eigvec	                       PCA eigen-vectors	(366, 366)
"""

```

### print statistics for TOBS

```python
S=STAT['TOBS']
for key in ['mean', 'std', 'low100', 'high100']:
    element=S[key]
    print(key,'=',end='')
    if type(element)==numpy.float64 or type(element)==numpy.float16:
        print('%6.2f'%element)
    elif type(element)==numpy.ndarray:
        print (element)
    else:
        print('unidentified type=',type(element))
"""
RETURNS ->

mean =  5.40
std = 11.71
low100 =-19.41
high100 =-19.41
"""

```

```python
%pylab inline
measurement='TOBS'
Sobs=STAT[measurement]['SortedVals']

#figure(figsize=[15,10])
n_obs=Sobs.shape[0]
p=arange(0,1,1/n_obs)
plot(Sobs,p)
title('CDF of '+measurement)
grid()
"""
RETURNS ->

%pylab is deprecated, use %matplotlib inline and import the required libraries.
Populating the interactive namespace from numpy and matplotlib
[Plot/Image Output: CDF of TOBS]
"""

```

### distribution of undefined elements in yearly measurements.

```python
for m in ms:
    figure()
    undef=STAT[m]['UnDef']
    hist(undef,bins=arange(0,400,10))
    title(m)
    grid()
"""
RETURNS ->

[Plot/Image Output: Histograms for TMAX, SNOW, SNWD, TMIN, PRCP, TOBS]
"""

```

### Mean and STD of temperature

```python
def plot_mean_std(m):
    #figure(figsize=[15,10])
    Mean=STAT[m]['Mean']
    Var=STAT[m]['Var']
    plot(Mean,label='Mean')
    plot(Mean+sqrt(Var),label='Mean+std')
    plot(Mean-sqrt(Var),label='Mean-std')
    grid()
    legend()
    title(m)

```

```python
plot_mean_std('TMAX')
"""
RETURNS ->

[Plot/Image Output: Mean/STD for TMAX]
"""

```

```python
def plot_pair(meas,plot_function):
    figure(figsize=(12,4))
    subplot(1,2,1); plot_function(meas[0])
    subplot(1,2,2); plot_function(meas[1])

```

```python
plot_pair(['TMIN', 'TMAX'],plot_mean_std)
"""
RETURNS ->

[Plot/Image Output: Pair plots for TMIN and TMAX]
"""

```

```python
plot_pair(['SNOW', 'SNWD'],plot_mean_std)
"""
RETURNS ->

[Plot/Image Output: Pair plots for SNOW and SNWD]
"""

```

---

# Weather Analysis - MA Voc Quiz 4

## Weather Data : Initial Visualization

### Massachusetts

```python
state="MA"

```

```python
#sc.stop()

```

```python
import pandas as pd
import numpy as np
import sklearn as sk
%pylab inline
"""
RETURNS ->

%pylab is deprecated, use %matplotlib inline and import the required libraries.
Populating the interactive namespace from numpy and matplotlib
"""

```

```python
%%time
%run lib/startup-voc.py
"""
RETURNS ->

172.17.0.2
sparkContext= <SparkContext master=local[2] appName=pyspark-shell>

    pandas as    pd 	version=2.0.3 	required version>=0.19.2

     numpy as    np 	version=1.24.4 	required version>=1.12.0

   sklearn as    sk 	version=1.3.1 	required version>=0.18.1

module urllib has no version
   pyspark as pyspark 	version=3.5.0 	required version>=2.1.0

ipywidgets as ipywidgets 	version=8.1.1 	required version>=6.0.0

version of ipwidgets= 8.1.1

measurements is a Dataframe (and table) with 12720632 records
stations is a Dataframe (and table) with 119503 records
weather is a Dataframe (and table) which is a join of measurements and stations with 12720632 records
CPU times: user 202 ms, sys: 16.9 ms, total: 219 ms
Wall time: 5.71 s
"""

```

```python
sc
"""
RETURNS ->

<SparkContext master=local[2] appName=pyspark-shell>
"""

```

```python
import warnings  # Suppress Warnings
warnings.filterwarnings('ignore')

_figsize=(10,7)

```

## Read Data

### show 3 rows of joined weather+stations table

```python
### Total number of stations
stations.count()
"""
RETURNS ->

119503
"""

```

```python
%%time
weather=measurements.join(stations,on='station')
weather.show(3)
"""
RETURNS ->

+-----------+-----------+----+--------------------+--------+---------+---------+------------+-------------------+-----+-------+
|    Station|Measurement|Year|              Values|latitude|longitude|elevation|  dist2coast|               name|state|country|
+-----------+-----------+----+--------------------+--------+---------+---------+------------+-------------------+-----+-------+
|AG000060390|       TAVG|2022|[79 00 6C 00 66 0...| 36.7167|     3.25|     24.0|   8.0234375| ALGER-DAR EL BEIDA|     |Algeria|
|AGE00147716|       TAVG|2022|[85 00 83 00 7C 0...|    35.1|    -1.85|     83.0|0.5224609375|NEMOURS (GHAZAOUET)|     |Algeria|
|AGM00060360|       TMIN|2022|[5A 00 19 FC 4B 0...|  36.822|    7.809|      4.9|  3.16015625|             ANNABA|     |Algeria|
+-----------+-----------+----+--------------------+--------+---------+---------+------------+-------------------+-----+-------+
only showing top 3 rows

CPU times: user 2.15 ms, sys: 0 ns, total: 2.15 ms
Wall time: 875 ms
"""

```

```python
sqlContext.registerDataFrameAsTable(weather,'weather')

```

```python
ms=['TMAX', 'SNOW', 'SNWD', 'TMIN', 'PRCP', 'TOBS']
# ms=['TMAX', 'TMIN', 'TOBS']
cms='or\n'.join(['Measurement="%s" '%(m) for m in ms])

##  read all data for state
Query="""
SELECT *
FROM weather
WHERE state="%s" and 
(%s)
"""%(state,cms)
print(Query)
"""
RETURNS ->


SELECT *
FROM weather
WHERE state="MA" and 
(Measurement="TMAX" or
Measurement="SNOW" or
Measurement="SNWD" or
Measurement="TMIN" or
Measurement="PRCP" or
Measurement="TOBS" )

"""

```

```python
%%time
weather_df=sqlContext.sql(Query)
print('number of rows in result=',weather_df.count())
weather_df.show(2)
"""
RETURNS ->

number of rows in result= 49710
+-----------+-----------+----+--------------------+--------+---------+---------+-------------+--------------------+-----+-------------+
|    Station|Measurement|Year|              Values|latitude|longitude|elevation|   dist2coast|                name|state|      country|
+-----------+-----------+----+--------------------+--------+---------+---------+-------------+--------------------+-----+-------------+
|US1MABA0017|       PRCP|2022|[00 00 77 00 00 0...| 41.5828| -70.5803|     12.8| 0.9072265625|EAST FALMOUTH 1.2...|   MA|United States|
|US1MABA0018|       SNOW|2022|[00 00 00 00 19 F...| 41.5818| -70.5257|      9.8|0.94091796875|     WAQUOIT 0.6 SSW|   MA|United States|
+-----------+-----------+----+--------------------+--------+---------+---------+-------------+--------------------+-----+-------------+
only showing top 2 rows

CPU times: user 2.8 ms, sys: 0 ns, total: 2.8 ms
Wall time: 1.5 s
"""

```

### read or compute statistics information for state.

```python
%%time
import os.path
from lib.computeStatistics import computeStatistics
from pickle import dump,load

pkl_filename=parquet_root+'weather-statistics/'+state+'-'+','.join(ms)+'.pkl'
print(pkl_filename)
!ls $pkl_filename
"""
RETURNS ->

../Data/Weather/weather/datasets/weather-statistics/MA-TMAX,SNOW,SNWD,TMIN,PRCP,TOBS.pkl
../Data/Weather/weather/datasets/weather-statistics/MA-TMAX,SNOW,SNWD,TMIN,PRCP,TOBS.pkl
CPU times: user 2.44 ms, sys: 5.91 ms, total: 8.36 ms
Wall time: 217 ms
"""

```

```python
%%time
if os.path.isfile(pkl_filename):   
    print('precomputed statistics file exists')
    with open(pkl_filename,'br') as pkl_file:
        STAT=load(pkl_file)
else:
    print('computing statistics')
    STAT=computeStatistics(sqlContext,weather_df,measurements=ms)
    with open(pkl_filename,'bw') as pkl_file:
        dump(STAT,pkl_file)

STAT.keys()
"""
RETURNS ->

precomputed statistics file exists
CPU times: user 0 ns, sys: 27.7 ms, total: 27.7 ms
Wall time: 38 ms
dict_keys(['TMAX', 'SNOW', 'SNWD', 'TMIN', 'PRCP', 'TOBS'])
"""

```

```python
print("   Name  \t                 Description             \t  Size")
print("-"*80)
print('\n'.join(["%10s\t%40s\t%s"%(s[0],s[1],str(s[2])) for s in STAT_Descriptions]))
"""
RETURNS ->

   Name  	                 Description             	  Size
--------------------------------------------------------------------------------
     UnDef	      sample of number of undefs per row	vector whose length varies between measurements
        NE	         count of defined values per day	(366,)
SortedVals	                        Sample of values	vector whose length varies between measurements
      mean	                              mean value	()
       std	                                     std	()
    low100	                               bottom 1%	()
   high100	                                  top 1%	()
   low1000	                             bottom 0.1%	()
  high1000	                                top 0.1%	()
         E	                   Sum of values per day	(366,)
      Mean	                                    E/NE	(366,)
         O	                   Sum of outer products	(366, 366)
        NO	               counts for outer products	(366, 366)
       Cov	                                    O/NO	(366, 366)
       Var	  The variance per day = diagonal of Cov	(366,)
    eigval	                        PCA eigen-values	(366,)
    eigvec	                       PCA eigen-vectors	(366, 366)
"""

```

### print statistics for TOBS

```python
S=STAT['TOBS']
for key in ['mean', 'std', 'low100', 'high100']:
    element=S[key]
    print(key,'=',end='')
    if type(element)==numpy.float64 or type(element)==numpy.float16:
        print('%6.2f'%element)
    elif type(element)==numpy.ndarray:
        print (element)
    else:
        print('unidentified type=',type(element))
"""
RETURNS ->

mean =  8.31
std = 10.31
low100 =-11.70
high100 = 30.59
"""

```

```python
%pylab inline
measurement='TOBS'
Sobs=STAT[measurement]['SortedVals']

#figure(figsize=[15,10])
n_obs=Sobs.shape[0]
p=arange(0,1,1/n_obs)
plot(Sobs,p)
title('CDF of '+measurement)
grid()
"""
RETURNS ->

%pylab is deprecated, use %matplotlib inline and import the required libraries.
Populating the interactive namespace from numpy and matplotlib
[Plot/Image Output: CDF of TOBS]
"""

```

### distribution of undefined elements in yearly measurements.

```python
for m in ms:
    figure()
    undef=STAT[m]['UnDef']
    hist(undef,bins=arange(0,400,10))
    title(m)
    grid()
"""
RETURNS ->

[Plot/Image Output: Histograms for TMAX, SNOW, SNWD, TMIN, PRCP, TOBS]
"""

```

### Mean and STD of temperature

```python
def plot_mean_std(m):
    #figure(figsize=[15,10])
    Mean=STAT[m]['Mean']
    Var=STAT[m]['Var']
    plot(Mean,label='Mean')
    plot(Mean+sqrt(Var),label='Mean+std')
    plot(Mean-sqrt(Var),label='Mean-std')
    grid()
    legend()
    title(m)

```

```python
plot_mean_std('TMAX')
"""
RETURNS ->

[Plot/Image Output: Mean/STD for TMAX]
"""

```

```python
def plot_pair(meas,plot_function):
    figure(figsize=(12,4))
    subplot(1,2,1); plot_function(meas[0])
    subplot(1,2,2); plot_function(meas[1])

```

```python
plot_pair(['TMIN', 'TMAX'],plot_mean_std)
"""
RETURNS ->

[Plot/Image Output: Pair plots for TMIN and TMAX]
"""

```

```python
plot_pair(['SNOW', 'SNWD'],plot_mean_std)
"""
RETURNS ->

[Plot/Image Output: Pair plots for SNOW and SNWD]
"""

```

## Conclusion

* We loaded the weather data from Parquet files.
* We explored statistics for the data.
* We explored where there are a lot of empty cells - limits the accuracy of the statistics.
* We visualized different measurements as a function of the day of the year.
* **next** Using PCA for more refined analysis.



--- end {7.1_notebook.md} ---
--- start{7.1_transcript.txt} ---
(soft music) - [Instructor] Hi. So we did a little overview of PCA, principal component analysis, and so now you should have a rough idea about how it is used. We're going to now use it for a particular interesting and large
dataset, the weather dataset. But before we start with that, we're just going to look at this dataset in a more simple way to get an idea of how things are laid out. So this is weather data for
specifically Massachusetts. So we have a setting
of this variable state to be Massachusetts. On the whole, I'm going
to skip many of the cells, maybe most of the cells in this notebook. I'm just going to give
you the high level cells that are important for the
high level understanding. But it really would help you
if you go into this notebook and run each cell and see the results in order to learn how
to work with this data. So where does this data come from? It comes from NOAA, okay, so the National Oceanic and
Atmospheric Administration in the United States, which collects various data sets about weather and the ocean. And this is one of the more simple ones that we can use to do our analysis. Now it's from NOAA. It's called GHCND. And you can get the details and downloads from this AWS S3 bucket. Okay, so it's in an S3 bucket, and the data in that
bucket is in a raw form, in text form, in CSV file. And what we are doing in the background is we translate the raw
data into parquet files that store this information, the information consists
of weather and stations. Okay, so that's our starting
point with this notebook is these two very large data frames. So let's see how we read this
data from the parquet file. We, first of all, look at the
data frame that is stations, see how many stations we have. We have about 120,503. And then we're looking at
how, at a few of the rows in the measurements table, in the measurements table after we join it with the stations. Okay, so now we have the
join based on the station ID. Okay, so here's a station ID. And the station ID, on the one hand, has measurement type, T average, year that it's measured, 2022, then here are values that
I'll talk about in a minute. And then here are the
latitude of the station, longitude of the station,
elevation, distance to coast, and name of the station, and which country and state it comes from. So Algeria, Algeria doesn't
have states, so state is empty. And what about this central piece? Okay, so these are vectors, vectors of 365 values, okay. And they're stored in what's called a blob in the database terminology. And basically it's a compressed version of the data that we want to use. And this data is really the center part. It basically gives for
each day of the year, what is the value. Okay, so we want to, so this weather data frame holds all the information
that we need to use, and in order to be able to
use it inside SQL queries, we need to register it into the data, as a table. So that's this command, sqlContext.registerDataFrameTable, weather, that's this weather, and we are just going to give
it the same name, weather. And that allows us to do, to create queries of the following form. So if we look here at the query, we say select all of the
roads from the table weather, where the state is Massachusetts and the measurement is either TMAX, SNOW, SNOWD, TMIN, precipitation, or TOBS. Okay, what are all of these? We will talk about some of them later, but basically this is a query, a compact query that
extracts from our data frame only those type of measurements for the state of Massachusetts. So the other part of the information that we have is statistics. Okay, so we have a statistics
file which is a pkl file, which basically stores in
it the information about, statistical information
about Massachusetts. So those are various averages and distribution variables and so on. And we're going to look at them. What is written here is that first you check whether this exists, then it just reads the
file, that's very quick. Or if it doesn't exist, then it runs this function
called computeStatistics which basically computes
all of these statistics, and that takes on the order of
five to 10 minutes per state. So it's significant and we don't want to do it over and over. That's why we store it into a file that later we check about it. Okay, so here are the fields
in this statistics data. Let's look at some simple ones. Mean is the mean value over all of the records in that file for each day of the year. So this is 366 entries. And then we have, then we have the, similarly
to that, the covariants, covariants for each one of the 365 days. And then we have
quantities that have to do with aggregating all of the days regardless of which day of the year. So we have the mean value
and the standard deviation. So that gives us just overall distribution for this variable in Massachusetts. And then we have some information about the distribution itself. So the bottom one percentile,
top one percentile, bottom 0.1 percentile, top 0.1 percentile. And then we have a sample of the values. So all of the values, that would be a pretty big data structure. We don't want to keep all of them. We just want to have a sample. So that's what we keep in here. And that would allow us to look at what's the overall distribution. Then we have interesting
things that have to do with how many of the entries
are actually defined. So one thing that you would
find in large data set like this is that there are many rows
and many columns defined, but many of them, maybe a large percentage is actually empty, so it doesn't have an actual value. So even though it's defined, it's not going to be very useful for us. So we'll see that in several ways. Here we're just saying how
many undefs you have per row, so how many, in a typical year, how many undefined
variables you have per row. So that's a sample of those. Then we have account of
the defined values per day. So are some days more, having more defined
values than other days? And then we have, and that's
basically it at this point. So this is a collection of statistics that we can now visualize
to have a better idea about what's going on in this data set. Okay. So here is some basic statistics. We are using TOBS. TOBS is temperature observed, and that's typically a temperature at 1:00 p.m. in the afternoon. Why is it important to have temperature at 1:00 p.m. in the afternoon? Because relatively that
is a stable quantity. At 1:00 p.m. in the
afternoon, you have quantity that doesn't change very
much for our day-to-day. The other two quantities that we have are going to be TMAX and TMIN. Those are quantities that vary significantly more from day-to-day because they happen at
different times of the day, and so there's more opportunity
for TMAX to be large and for TMIN to be small. Okay, so if we just look at TOBS, the mean value is 8.31,
standard deviation is 10.31, and the low 1% is minus 11, and high 1% is 30.59. Worthwhile saying at this
point that this is in Celsius. And Celsius 30 degrees is pretty warm. So that shouldn't be too surprising. It would be surprising
if it was Fahrenheit. Okay, the next we can look at how does this value of
the T observed vary? So what we see here is the
cumulative distribution function for T observed, and we see that it starts
from around minus 10 and increases more or less
linearly until it hits 30, which is consistent with what
we just saw in our values. But now we have the whole distribution. Now let's look at a little
bit at undefined values. As I said, in real data sets, many of the values that are declared are not really available. They were not available for measurement. So what we have here is the number of undefined entries in
the sample of the years. Okay, so we see that
there's a big peak at zero, meaning all of these are ones that the whole year is well-defined. There's no empty part. But then you see that there's
little bits even up to 350. So at 350, you basically have something like 15 values that are defined. And the question then is do you really want to use
a year with measurements where only 15 of the
measurements are defined? Whether they're distributed throughout or whether they're bundled,
bunched up in one place, they're not a good one to use anyway. Next, let's look at how many of the different
measures we have. So it turns out that there
are 64 types of measurements. Okay, so there's measurements
like precipitation, snow, snow depth, that are very common, and we have a lot of measurements of those for Massachusetts. But then when we look at the bottom range, we see that there are
a lot of measurements for which we have only
one or two year stations that correspond to them. So those are probably not ones that we want to try to analyze, because we have too little data. Okay, so what are the definitions
of these measurements? We can use a simple grip
command to see what they are. So SNWD is snow depth, okay? And then WT and then two numbers is weather type where has one of the following values, and there's a continuation in the table of all of these different values. So this is weather type. It is a discrete
description of the weather. Okay, let's see the
number of measurements, the number of years for
each station in the state. Okay, so here we use a little SQL query to find out for T observed, how many values we have, and that didn't work. This works better. Okay, let me make it a little smaller. Okay, so here we have these T observed, and we have basically extracted all of the queries that are for this, all of the, for a particular year. And this state, Massachusetts, we want to find all of
the TOBS measurements. And so here we have the stations, and here we have all of the
measurements that we got for these stations. So now let's plot them. And now we can see that here we have 35, five of the 35 measurements that we have and these are the temperatures. So you see that the
temperatures across the state, they behave in a similar way, where not surprisingly
July is the warmest month and then January is the coldest. If we look at all 35, then what we see is we have a much denser picture
with all of the graphs. And the point that I'm
making here is that, yes, if you have a million stations, you can plot them one on top of the other, but you won't be able
to see very much, right? Because simply the data is so dense that you can't really
see what is going on. But one way to see what's
going on is to zoom in to basically look at one small
piece of the measurements. That's what we have here. And what you see, these are like 25 days, and you see that in this range of dates, all of the temperatures
were almost the same. And if you go a little bit back, you see that there's a cluster of places where the temperature went
up and down like this. Okay, so there's several
that this happened. And so what you can see is
phenomenologically you see, as an example, you see
that there are correlations between particular stations. And I would bet that if you look at where these stations are, they're probably close to each other, so their weather patterns
are similar to each other, not identical, and sometimes
they deviate quite a lot, but sometimes they are very similar. Okay, so let's start to look at how different measurements
behave, relate to each other. Here we're taking the
TMAX, TMIN, and TOBS, so the maximum temperature over the day, the minimum, and the
observed at 1:00 p.m., and see how they look. And what you see is that the TMAX is at the top, it's the blue line. TMIN is at the bottom,
it's the orange line. And TOBS is the middle is the
green line, which makes sense. So here is just the type
of script that we use in order to draw plots like
this and an example of it. And now we're going to look
at other types of statistics across the state of Massachusetts. Okay, so, so, first, what we're looking at is the number of valid measurements for each day of the year. So here you see the number
of valid measurements, and here is the day of the year starting from January
and ending in January. Okay, so what do we see? We see that an interesting thing is that there's more or less in the same amount, then it jumps here, more
or less the same amount, and then it jumps down. That's a curious phenomena. What does it mean? Well, if you look, these switches are often right on the boundary between months. So what does that mean? That probably means that
this has to do with people that are taking the measurements being changed for other people. And so these other people maybe didn't take the
measurements as much or so on. And at the end of the day, the differences are not very big, right? So they look big because of the scaling, but they vary between five, seven, 500, 5,700 to 5,900. And here is another one for
T observed and precipitation. And here is another one for snow depth. So for snow depth, the behavior
is very extreme, right? You see that on the boundary of this month between April, between May and April, and then between April and
June, and June and July, there are these jumps like I said before, and you get that during the winter months, you get more measurements from the people. So these are just the type
of things that you want to do in order to make sure that
your data is not corrupted, or if it's corrupted,
it's not corrupted so much that you can't trust the statistics. Next, let's see what we see if we take the mean and standard deviation of the TMAX and TMIN. So we see that we have
the mean is in the middle, and then you have the mean
plus standard deviation, minus standard deviation. And it's similar for min and
max, and they look similar, but notice that here it goes up to 30, and here it goes only up to 20, okay. So the minimum temperature goes in the summer months only up to 20. And here we have T observed. And here's the T observed that behaves in this
picture quite similarly. It's just the value here is
somewhere in the middle, 25. On the other hand, let's
look at a plot pair for, let's look at the plots of the same kind of the mean plus minus standard deviation for precipitation and for snow. So for precipitation, what
we see is that the mean is almost always the same. It's about four millimeter. But the standard deviation
is very large around it. Of course, values of precipitation below zero are impossible, but if you just take the plus
minus standard deviation, that's what you get. So what does that mean? It means that it's more or less on average rains the same amount throughout the year, but the reason that this looks so flat is because it rains in some days and it doesn't rain in the following days, and then another day it rains, another day, few days it doesn't rain. So when you average that over the years, you just get this very,
very flat distribution. And similarly you get for
snow, how much snow comes down. On the other hand, if
you look at snow depth, you see that there is, this is snow depth, so this is the depth of
the snow at every day measured across from the top of the snow to the earth underneath. You see that there is actually a very nice performance
for the average, right? And that makes sense because
from the months of April till the months of October, there is no snow. So basically you get a value that is very, very close to zero. And then in the days of the winter, you get an amount of snow and you get standard
deviation around it, okay. Okay, so what did we do? We loaded data weather
from the parquet files, we explored the statistics of the data, we explore where and when
there are cells that are empty. Okay, so basically
where is the data sparse and where is the data dense? We visualize different
measurements of the year. And then next we're going to go and do principal
component analysis of this in order to identify more of the structure of what is going on. Okay, I'll see you then.
--- end {7.1_transcript.txt} ---
--- start{8.1_notebook.md} ---
### 8.1_notebook.md

```markdown
# Review of linear algebra

This notebook is a quick review of the concepts and notation of linear algebra and their implementation in the python library `numpy`.

It is not intended as a course in linear algebra. For an excellent elementary introduction to vectors and linear algebra, see [Gilbert Strang's course on linear algebra](http://web.mit.edu/18.06/www/videos.shtml)

```python
%pylab inline
import numpy as np
from scipy import *
import warnings
warnings.filterwarnings("ignore") #,category=matplotlib.cbook.mplDeprecation)
"""
RETURNS ->

Populating the interactive namespace from numpy and matplotlib
"""

```

## Vectors

* arrows
* velocity and direction
* location in the plane or in 3D space.
* many many other things.

Vectors spaces are the at the basis of linear algebra. They can be used to describe many things: from points in the plane, to time series to the configuration of electrons in an atom. This notebook is a brief review of some of the main concepts regarding vectors in finite dimensional Euclidean space.

### A 2D vector

<img style="width:400px" src="images/vector.png">

### A 3D vector

<img style="width:300px" src="images/vectorGeom1.png">

### Vector notation

* We will denote vectors by letters with a little arrow on top: 
* Vectors are grouped by **dimension **, the set of all  dimensional (euclidean) vectors is denoted .
* A 2D vector is an element of  and **can** be described by a sequence of **two** real numbers:  or 
* The description **is not unique**, it depends on the choice of coordinates,
* A 3D vector is an element of 
described by a sequence of **three** numbers:
 or 
* A  dimensional vector is an element of  and is described by a sequence of  real numbers: 

### Lists vs Numpy Arrays

The numpy library (we will reference it by np) is the workhorse library for linear algebra in python.  To creat a vector simply surround a python list () with the np.array function:

```python
x_vector = np.array([1,2,3])
print(x_vector)
"""
RETURNS ->

[1 2 3]
"""

```

The function `np.array` converts a python list and converts it to an array:

```python
c_list = [1,2]
print("The list:",c_list)
print("Has length:", len(c_list))

c_vector = np.array(c_list)
print("The vector:", c_vector)
print("Has shape:",c_vector.shape)
"""
RETURNS ->

The list: [1, 2]
Has length: 2
The vector: [1 2]
Has shape: (2,)
"""

```

```python
z = [5,6] # a list
print("This is a list, not an array:",z)
print(type(z))
"""
RETURNS ->

This is a list, not an array: [5, 6]
<class 'list'>
"""

```

```python
zarray = np.array(z)
print("This is an array, not a list",zarray)
print(type(zarray))
"""
RETURNS ->

This is an array, not a list [5 6]
<class 'numpy.ndarray'>
"""

```

### Arrays as Vectors

One way to define vectors in numpy is to use a one dimensional array.

```python
v1=np.array([1,2])
v2=np.array([-1,1])
print(v1,v2)
"""
RETURNS ->

[1 2] [-1 1]
"""

```

#### Vector dimension vs. Array dimension

The word **dimension** has two different meanings in this context.

* The array `np.array([1,2])` has **one** array dimension.
* The vector `[1,2]` has **two** vector dimensions.

We will try to use **dimension** for the vector dimension and **shape** or **index** for the array dimension.

## Operations on vectors

There are two basic operations on vectors:

1. Multiplying a vector by a scalar (a number)
2. Adding two vectors

### 1. Multiplying a vector by a scalar

Multiplying a vector  by a scalar  results in a vector in the same direction as  but of a different length.
The length is multiplied by .
If  the direction is reversed.

```python
print("v1=",v1)
print("2*v1=",2*v1)
"""
RETURNS ->

v1= [1 2]
2*v1= [2 4]
"""

```

### 2. Adding two vectors

We add vectors by adding their components:


```python
print("v1=",v1)
print("v2=",v2)
print("v1+v2=",v1+v2)
"""
RETURNS ->

v1= [1 2]
v2= [-1  1]
v1+v2= [0 3]
"""

```

#### Geometry of vector addition

Geometrically, adding  to  is equivalent to placing the tail of  on the head of . The sum is the vector from the tail of  to the head of .

<img style="float: left;width:200px" src="images/Parallelogram.svg.png">

## Linear Combinations

Combining the two operations, we can form **linear combinations**:


```python
3*v1-2*v2
"""
RETURNS ->

array([5, 4])
"""

```

### Span

The set of all linear combinations of a set of vectors  is called the **span** of the vectors.


Usually, the span of  vectors in  is a -dimensional subspace of .
However, if the vectors are **linearly dependent**, the dimension of the span is less than .

## Norms and Dot Products

### The Norm (Length) of a vector

The Euclidean Norm (or length) of a vector  is defined as:


```python
from numpy.linalg import norm
print("v1=",v1)
print("norm(v1)=",norm(v1))
print("sqrt(1^2+2^2)=",sqrt(1**2+2**2))
"""
RETURNS ->

v1= [1 2]
norm(v1)= 2.2360679775
sqrt(1^2+2^2)= 2.2360679775
"""

```

### The Dot Product (Inner Product)

The dot product of two vectors  is defined as:


```python
print("v1=",v1)
print("v2=",v2)
print("v1.v2=",np.dot(v1,v2))
"""
RETURNS ->

v1= [1 2]
v2= [-1  1]
v1.v2= 1
"""

```

#### Geometric interpretation

The dot product is related to the angle  between the vectors:


This implies that if two vectors are **orthogonal** (perpendicular), their dot product is 0.

```python
v_orth = np.array([2,-1])
print("v1=",v1)
print("v_orth=",v_orth)
print("dot(v1,v_orth)=",np.dot(v1,v_orth))
"""
RETURNS ->

v1= [1 2]
v_orth= [ 2 -1]
dot(v1,v_orth)= 0
"""

```

### Matrices

A matrix is a 2D array of numbers.


In numpy, we can create a matrix using `np.array` with a list of lists.

```python
A = np.array([[1,2],[3,4]])
print(A)
print("Shape:",A.shape)
"""
RETURNS ->

[[1 2]
 [3 4]]
Shape: (2, 2)
"""

```

#### Matrix-Vector Multiplication

We can multiply a matrix  by a vector .


This is equivalent to a linear combination of the columns of .

```python
x = np.array([1,2])
print("A=",A)
print("x=",x)
print("Ax=",np.dot(A,x))
"""
RETURNS ->

A= [[1 2]
 [3 4]]
x= [1 2]
Ax= [ 5 11]
"""

```

#### Matrix-Matrix Multiplication

We can multiply two matrices  and .



The element  is the dot product of the -th row of  and the -th column of .

```python
B = np.array([[1,0],[0,1]]) # Identity matrix
print("A=",A)
print("B=",B)
print("AB=",np.dot(A,B))
"""
RETURNS ->

A= [[1 2]
 [3 4]]
B= [[1 0]
 [0 1]]
AB= [[1 2]
 [3 4]]
"""

```

### Transpose

The transpose of a matrix , denoted , is obtained by swapping rows and columns.

```python
print("A=",A)
print("A^T=",A.T)
"""
RETURNS ->

A= [[1 2]
 [3 4]]
A^T= [[1 3]
 [2 4]]
"""

```

### Solving Linear Equations

We often want to solve the equation  for .
If  is invertible, the solution is .

```python
from numpy.linalg import inv, solve
b = np.array([5,11])
print("A=",A)
print("b=",b)
x_sol = solve(A,b)
print("solution x=",x_sol)
print("Check Ax=",np.dot(A,x_sol))
"""
RETURNS ->

A= [[1 2]
 [3 4]]
b= [ 5 11]
solution x= [1. 2.]
Check Ax= [ 5. 11.]
"""

```



--- end {8.1_notebook.md} ---
--- start{8.1_transcript.txt} ---
(thoughtful music) - [Instructor] So in
order to do the analysis we're going to do on weather data, we need some linear algebra. Specifically, we need to understand what is the principle
component analysis like. So to help you review that,
I have these few slides. This is not intended to be the full course in linear algebra. For that, I direct you to go
to Gilbert Strang's course, but it's more of a refresh. So what are vectors? Vectors are the basic unit of
analysis in linear algebra, and they can represent many things. They can represent arrows, they can represent velocity and direction, they can represent location
in plane or 3D space, and many, many other things as well. So here is a representation
of a 2D vector, simple 2D vector, and
it's basically this arrow, and the arrow has a tail,
that is, in general, in the origin at zero, and then it has a length and a direction. So that's one way of
thinking about 2D vector. 3D vector is a similar thing, but now, we have three
coordinates, one, two, three, and the vector represents a point in this three dimensional space. So let's talk a little bit about notation. So what we're going to describe
vectors with is a letter, let's say a, with a little arrow above it. So a, b, v1, v2, and so on. Vectors are grouped in a dimension. So all d dimensional
vectors are denoted by Rd. So that's the Euclidean
space of dimension d. A 2D vector and element of R2, the plane, can be described as a
sequence of two real numbers. So a can be, let's say one and pi, or b can be -1.56 and 1.2. So this is one way of describing, but that way of describing it depends on the choice of coordinates. So it's not a unique way. For the same vector, you can have many
different representations. A 3D vector, similarly, is
made out of three numbers, can be represented as three numbers. And again, it's not a
unique representation. So with d dimensional vector in Rd, is represented by d numbers. Simple generalization. So numpy allows us to define vectors in a one dimensional array. So v1 is a vector with
the coordinates 1 and 2, v2 is a vector with coordinate -1 and 1. And here, they're represented as an array, but more specifically, as
a one dimensional array, where the number of
elements is the dimension. So here, we have these
two vectors represented. Note that the vector dimension is not the same as the array dimension. So this array is a one dimensional array, but it defines a vector in R4, it has a vector of dimension four. So keep in mind that the
dimension of an array is different than the
dimension of the vector that is represented by this array. On the other hand, this
array is, of course, just a one dimensional array. So it's a list of numbers. The array 1, 2, and 3, 4,
is a two-dimensional array, and it's a rectangle of numbers. And we have a special name for
these rectangle of numbers, we call them matrix. So a matrix is basically
a rectangle of numbers. So here is such an array, and Python knows how to write it nicely, as a two dimensional array. So in terms of visualizing 2D vectors, here are two vectors. 1, 2, -1, 1, and 0, -2. And this here is the
representation of these vectors. So 0, -2, 1, 2, and -1, 1. So what this basically means is the first coordinate is the X direction, the second coordinate is the Y direction. So these numbers describe
uniquely each vector in the plane. What operations can you do on vectors? You can do addition, inversion,
multiplying by scalar. So here, we have the vectors
from before, v1 and v2, that are 1, 2, and -1, 1. We can write the sum of the
vectors, here's the sum. we can write four times the vector, and we can write minus a vector. And basically what that means is that we do it coordinate by coordinate. So 1, 2 plus -1, 1, 1 + -1 gives you 0, and 2 + 1 gives you 3. Similarly, if you
multiply v2, -1, 1, by 4, you get -4 and 4. And if you take minus
of v1, you get -1, -2. So this is just to show
that you can add vectors, and you can multiply a
vector by any number, including negative numbers. So what does this look like
in the 2D representation? So what we have here is we have
a vector v1 and a vector v2. So this is v1, this is v2,
and when we want to add them, we simply take this vector, the v2, and we put its tail on the head of v1, and we see to where we get. So you can easily verify
that that's equivalent to what we did in terms of adding and subtracting coordinates. So two vectors can only be summed if they have the same dimension, right? So here, basically, I'm
trying to sum the array 1, 1, with the array 1, 1, 1,
and it has an exception, and the exception is because they don't have the same dimension, you can't sum them together if they don't have the same dimension. So we said that you can
multiply a vector by a number, positive or negative. Here, we're taking the vector
v, so this vector, 1, 2, and we're multiplying it by -0.5. So we basically are taking the vector, and we're keeping the
direction of the vector, but we're multiplying the length by -0.5. That's how we get this vector. So we talked about these operations that you can do between vectors. One more operation that
is a very useful one is the inner product, or dot product. And the mathematical
notation for it is a dot b. And here is a calculation of it. So basically, I'm doing the
dot product of v1 and v2. And what I can see is that you can write it in different ways. You can basically take the product, v1, first coordinate and
the second coordinate, the first coordinate of v1,
the first coordinate of v2, then the second coordinate of v1 times the second coordinate
of v2, and then sum them. So this is basically what's
written here, v, 0, v, 1. And then the sum of those, that gives you the the dot product. So all of these different operations give you the same result. So basically, let me write it. If I write a dot product with b, that is equal to the sum, i
equals one to the dimension of ai times bi. The numbers, I just
multiply them pairwise, and then I add them all up. So that's basically the dot product. The norm of a vector is the dot product of
the vector with itself. The square root of that, sorry. So the dot product of
the vector with itself, taking the square root, that gives you basically
the sum of the vi squared, and that gives you the norm of the vector. And basically, the norm is
the same as the magnitude. So the length of the vector, that is the same thing as the norm. So here are two ways
to calculate the norm, and you can basically see that they give you the same answer. So here is the dot
product of v1 with itself, and you take the square root of two, square root of it, and then
you just use the function norm, and it gives you the same number. So what are unit vectors? Particularly important are
vectors whose norm is one, these are unit vectors, and basically, those are vectors such
that their norm is one. So their length is one,
and you can normalize them. So we can sum two vectors,
but we cannot sum two vectors if they have different dimensions, they're just incompatible that way. So here, what happens is we
try to sum the vector 1, 1, with a vector 1, 1, 1,
and we get an exception. It is not possible to
sum these two together. Here is what happens when
we multiply the vector by a constant. Here, the vector is 1, 2,
and the constant is -.5. So here is the vector 1, 2, and when we multiply it by -5, we multiply each component
by -5, or you can say we keep the same direction,
but we make the length be - .5 of the original length. So that basically gives us this vector. So multiplying by a scalar means you keep the direction as it
is, but you change the length. Next, we'll talk about the inner product. So that's an operation
between two vectors, and it takes two vectors
from the same dimension, and it basically gives you
back a number, a scalar. So that's a dot b. So there are three different ways to describe what a dot product is, at least two, and at least three ways. And here is the way as
it is defined in numpy, you just say numpy dot,
you take the dot product of v1 and v2, and the other ones are that you multiply v1,
the first coordinate, by v2, the first coordinate, and multiply v1, the second coordinate, by v2, the second coordinate,
and then you sum the result, or you can write it in this
way if you want to be fancy. So in any case, when
you do this dot product of 1, 2, and -1, 1, you get the answer 1. Next, let's talk about
the norm of a vector. So the norm of a vector
is simply the square root of the dot product of
the vector with itself. We also think about the norm of the vector as the length of the vector,
which we talked about. And so that was our example here. And so the magnitude is
the norm of the vector. And it's computed exactly
like regular dot product. You take the first component squared, plus the second component squared, plus the third component
squared, and so on, and then you take the
square root of the result. So here, we have these two ways to calculate the norm of a vector v1. So the vector is 1, 2, and we can either take the
dot product of v1 with itself, and then take square root
of two, square root of that, or we can just ask for the norm of v1. In any case, we get 2.23. So unit vector. Of particular importance is
vectors whose norm is one. Those vectors are called unit vectors, and play an important
role in linear algebra. So here is how we can get a unit vector. Let's say we have a vector v1, and its norm is 2.236, and
we want to get a unit vector. So what should we do? We should multiply, which
should divide the vector by the length of the vector, and that will give us a
vector in the same direction, but of length one. So that's basically what we
call here as normalizing. u1 is v1 divided by the norm of v1. And then what we get is
that u1 is this vector, and its norm is 1, up to
the accuracy of numpy. Projections. So projections are one of
the operations that we do when we talk about coordinates. So taking the dot product
of an arbitrary vector with a unit vector has a simple
geometric interpretation. So here is our interpretation. We have a unit vector, that is u1. So u1 is a unit vector, and v2
is just an arbitrary vector. So that's v2, that's u1, unit vector. And then, what is the dot product? The dot product is basically
the result of taking v2 and projecting it on this direction, meaning having this 90 degree angle here. And then this vector now is the projection of v1, v2 onto u1. So in the sense it basically
is the component of v2 that is in the direction of u1. So next, we're going to talk
about orthogonal vectors. So two vectors are orthogonal
if their dot product is zero. In other words, if their vectors are 90 degrees to each other. So that can be seen in this way. Here are two vectors that are orthogonal, meaning that their dot product is zero, or that the angle between
them is 90 degrees. So now, we can talk about bases. So we say that vectors u1, u2, and ud, up to ud in Rd form a basis if you can write any vector, you can take any vector in Rd, and you can write it as a
sum of a scalar coefficient times these basis vectors. So the coefficients here are
the only thing that changes. And so you can think about
the coefficients here as a representation of the vector x. Of course, the representation will change if the basis changes. Now, we have a very special kind of bases. These are bases that are particularly easy to compute and interpret. And those are bases where the u1s form an orthonormal basis,
and that has two properties. The first is that the vectors
themselves are unit vectors, and the second is that every
pair of vectors is orthogonal. So you have orthonormal basis. So the vectors are
orthogonal to each other, and they are of unit length. So now, we're talking
about representing a vector using an orthonormal basis, and it's just like any
other basis that we use. However, if it's an orthonormal basis, we have a particularly easy
way to compute the coefficient, and that is that we take the dot product between the unit vector
and the original vector, just like what we said before, where v2 was projected on the unit vector. So in other words, we can
write an expression like this. Basically, these are the coefficients, and these are the basis vectors. So you see that the basis
vector is used twice. First, we multiplied by the
vector to get the coefficient, and then we multiply
that by the vector itself to get a vector with that length. So there's a standard basis, which is what we think
about when we think about usually about vectors, and that is that the basis is simply the one followed by all of these zeros, then 0, 1, then 0, 0, 1. So it's one in a single place,
and all the rest are zero. You can easily check that
they are an orthonormal basis, and the dot product
using the standard basis is just taking the coordinate of v, the appropriate coordinate
of v, which is vi, right? So that is a very simple operation where we have all of the vector
coefficients given to us, and then each one of
them can be represented as the dot product with
a standard unit vector. So here is such an example. Here is the vector, 5, 6, 3, 4, and here is a unit vector, 0, 1, 0, 0. And taking the dot product
of these two, we get six, which is exactly the second
location in the vector. So that second location is
picked up by the unit vector. Now, we can talk about reconstruction using an orthonormal basis. So an orthonormal basis is basically what we think about as coordinates, but coordinate systems are not unique. So you can have the same set of vectors described in a different
coordinate system, has a different representation. So let's see what that means. So basically, the vector v is represented as this list of numbers,
which are the coefficients. And if you want to reconstruct it, you use each one of these coefficient, and you multiply it by the
corresponding unit vector, and now, you basically
reconstructed the vector v. So you had the components, and then you combined these
components to make the vector v. Now, in the standard unit vectors, this is kind of a very trivial operation. However, this works for
any orthonormal basis, and that's the important thing. So representing a vector
v using the standard basis is just the same representation as before, but we can represent v using
a different representation, a different basis, and that's what is
called a change of basis. So we wanna visualize
what we mean by that, and we're going to look at that in R2. So here, we have a vector. The vector is v, this is the vector. And we have the standard
coordinate system, which is this one, the black one. And then we have another
coordinate system, which is this one, right? So they're both orthonormal basis systems, and they can both be used
to represent the vector. So the standard
representation we already know is basically using e1 and e2. It's basically taking the
projection here, that is -2, and taking the projection
here, that is -1, and so we get that representation. For the other direction, we basically project on this vector, and project on the u1 and u2, and we get a different way
to represent the same vector. So that's the important
thing is that when we do change of coordinates, we're
not changing the vector. We're not just changing
how we represent the vector using orthonormal coordinates. So this was the introduction, and next, we're going to talk about matrix notation.
--- end {8.1_transcript.txt} ---
--- start{8.2_notebook.md} ---
### 8.2_notebook.md

```markdown
# Matrices  -----  Notation and Operations

## Matrix notation
<a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)">Matrix Notation</a> is a notation system that allows succinct representation of complex operations, such as a change of basis. 
<img style="float: left;width:500px;height:500px" src="images/Matrix.svg.png">

* **Matlab** is based on Matrix Notation.

* **Python**: similar functionality by using **numpy**

Recall that a **vector** can be represented as a one dimensional array of numbers.

A **matrix** can be represented as a two dimensional array of numbers.

Typically:
* **Vectors** are denoted by lower case letters: $x,y,a,b,\ldots$
* **Matrices** are denoted by upper case letters: $X,Y,A,B,\ldots$
* **Scalars** are denoted by greek letters: $\alpha, \sigma, \pi$

### Indices
* The $i$th element of a vector $x$ is denoted $x_i$
* The element in row $i$ and column $j$ of matrix $A$ is denoted $A_{i,j}$

### Shapes
* A vector $x \in \mathbb{R}^d$ has $d$ elements (a.k.a. dimensions).
* A matrix $A \in \mathbb{R}^{n,m}$ has $n$ rows and $m$ columns.

### Transpose
* The **transpose** of a vector $x$ is denoted $x^\top$ (or $x'$).
* The transpose of a row vector is a column vector and vice versa.
* The transpose of a matrix $A$ is denoted $A^\top$ and is defined by $(A^\top)_{i,j} = A_{j,i}$

```python
import numpy as np
A=np.array([[1,2,3],[4,5,6]])
print('A=\n',A)
print('A.T=\n',A.T)
"""
RETURNS ->

A=
 [[1 2 3]
 [4 5 6]]
A.T=
 [[1 4]
 [2 5]
 [3 6]]
"""

```

### Dot Product

The **dot product** (or **inner product**) of two vectors of the same dimension  is defined as


```python
x=np.array([1,2,3])
y=np.array([-1,0,1])
print('x=',x)
print('y=',y)
print('x.dot(y)=',x.dot(y))
"""
RETURNS ->

x= [1 2 3]
y= [-1  0  1]
x.dot(y)= 2
"""

```

### Matrix Product

The product of a matrix  and a matrix  is a matrix  defined by



Note that the number of columns of  must be equal to the number of rows of .

In numpy `A.dot(B)` or `A @ B` computes the matrix product.

```python
A=np.array([[1,2],[3,4]])
B=np.array([[1,0],[0,1]])
print('A=\n',A)
print('B=\n',B)
print('A@B=\n',A@B)
"""
RETURNS ->

A=
 [[1 2]
 [3 4]]
B=
 [[1 0]
 [0 1]]
A@B=
 [[1 2]
 [3 4]]
"""

```

### Outer Product

The **outer product** of two vectors  and  is the matrix .


```python
x=np.array([1,2])
y=np.array([3,4,5])
print('x=',x)
print('y=',y)
# We need to reshape x and y to be 2D arrays to compute outer product using matrix multiplication
print('outer(x,y)=\n', np.outer(x,y))
"""
RETURNS ->

x= [1 2]
y= [3 4 5]
outer(x,y)=
 [[ 3  4  5]
 [ 6  8 10]]
"""

```

### Element-wise product

Sometimes we want to multiply elements of two matrices (or vectors) of the same shape element by element.
This is denoted by  or simply  in many programming languages.


```python
A=np.array([[1,2],[3,4]])
B=np.array([[1,0],[0,1]])
print('A=\n',A)
print('B=\n',B)
print('A*B=\n',A*B)
"""
RETURNS ->

A=
 [[1 2]
 [3 4]]
B=
 [[1 0]
 [0 1]]
A*B=
 [[1 0]
 [0 4]]
"""

```

### Norms

The **Euclidean norm** (or  norm) of a vector  is defined as


```python
x=np.array([3,4])
print('x=',x)
print('norm(x)=',np.linalg.norm(x))
"""
RETURNS ->

x= [3 4]
norm(x)= 5.0
"""

```

### Special Matrices

* **Zero matrix**: Matrix of all zeros.
* **Identity matrix** (): Square matrix with 1s on the diagonal and 0s elsewhere. .
* **Diagonal matrix**: Square matrix with non-zero elements only on the diagonal.
* **Symmetric matrix**: Square matrix equal to its transpose ().

```python
print('Zeros:\n',np.zeros((2,2)))
print('Identity:\n',np.eye(2))
print('Diagonal:\n',np.diag([1,2]))
"""
RETURNS ->

Zeros:
 [[0. 0.]
 [0. 0.]]
Identity:
 [[1. 0.]
 [0. 1.]]
Diagonal:
 [[1 0]
 [0 2]]
"""

```


--- end {8.2_notebook.md} ---
--- start{8.2_transcript.txt} ---
(gentle music) (air whooshing) - [Instructor] Hi. In the previous video,
we talked about vectors; dot products between vectors, multiplying a vector by a scalar, and then ultimately bases
and orthonormal bases. And then we ended with a simple expression for representing any vector
using an orthonormal basis. Now we will do something similar, but we will do it in
the way that you do it typically in a computer, and that is using matrix notation. So what is matrix notation? So a matrix is nothing
but a rectangle of numbers with m rows and n columns, so we say an m by n matrix. And the elements in the
matrix are indexed by, have two indices each, one is the row number and the
second is the column number. So this is nothing but a convenient way of representing a rectangle of numbers. We use this in software. If people are familiar with MATLAB, MATLAB is based on this kind of notation. And in Python you can get similar notation and similar functionality
when you use NumPy. Okay, so the first operation
that we want to describe is basically the transposition. So transposing a matrix, that is denoted by T in the superscript, is basically replacing the rows by columns and the columns by rows. So a matrix that is three
rows and two columns becomes a matrix of two
rows by three columns just by replacing the indices, the index of the row with
the index of the column. So here is how this looks in NumPy. We have here a matrix
that is two by three, zero, one, two, and this is this B matrix. And then we show that
if the original matrix has shape two by three, the transpose of B is this matrix here that has shape three by two. And all we needed to do in order to take the
transpose is this dot T, okay? So it's a very standard operation. You use it a lot in order to
get things in the right order. Now, by default, we represent vectors as what we call a column vector, where the values that are associated with the different coordinates are written in a column, okay? So this is a representation of a vector that is a column vector. And then this transposed is the corresponding row vector, okay? So these two vectors have
exactly the same information, it just that one is a
row and one is a column. And that will be important when we want to calculate
things with them. So if we think about a vector as a matrix, then a column vector is a d by one matrix and a row vector is a
one by d matrix, okay? So they're both matrices, just skinny matrices or flat matrices. Now, next is that you
can think about a matrix as a collection of vectors, right? So here is a matrix that we have, a two by three matrix, and we can represent it
as vectors in two ways. So one way is to
basically take each column to be a vector, okay? And that way we have
three vectors: c1, c2, c3, and these are their values, okay? So this is a completely
legitimate representation of the matrix as three vectors. We can do it also another way. We can represent the
same matrix as two rows. So here is a row number
one, row number two, and this is what's in the row. So the row here has dimension three, because that's a number of columns, while the columns had dimension two, because that's the number of rows, okay? So you can either represent the matrix by row vectors or by column vectors. So here is a way to split
A into columns, okay? So there is this operation, split, and it says that I want to
split it into three parts on axis one. And then this is what we get, these little matrices, okay, that are two by one. We can also take the vectors and combine them into the original matrix, and basically we do that by concatenating. So we take the columns and we concatenate them along axis one so we get back the original
matrix that we have. So we can break the
matrix apart into vectors and we can put these vectors together. And this is just checking that what we got is the same as what we
had in the original. Okay, now some operations
between matrices. So we can add or subtract matrices. That is a very simple operation. The matrices have to have
the same shape, okay? And then we take the elements,
the first row, first column, and the first row, first column here and we take the difference, okay? So we do this element by element and we get the difference matrix. Next, we're going to do
matrix-matrix products. Okay, so if you want to set
a dot product of two vectors, well we already know how to do that. Let's say we want to take 1,
2, 3, dot product in 1, 1, 0, then we multiply one by one,
two by one and three by zero, and we get that sum that
is equal to three, okay? So the dot product is equal to three, we know how to do that. When we do it in matrix notation, there is this extra
bit of work that we do, which is, what is a row column? What is a row vector? And what is a column vector? So, by convention, when we have a row vector
multiplied by a column vector, that means that we're taking
the dot product, okay? So one times one, two times
one, three times zero. That's kind of the standard operation, first with a first, second with a second, third with a third, okay? So we're going down on the second one and across on the first one, and that gives us, again, the same result. And that is indeed the dot product, which is sometimes
called the inner product. As we will see, it is sometimes useful to have a different kind of product, which is the outer product. What is the outer product? Instead of multiplying the
elements pairwise and summing so that at the end you get a scalar, you actually get a matrix that is, the number of elements
in the column vector defines the number of rows, and the number of
elements in the row vector defines the number of columns. And what we have is basically we, each element here is the product
of the first element here, the first element here, and then the first element here
and the second element here. So we get 1, 2, 3, 1, 2, 3, and then zero times 1, 2, 3, 0, 0, 0. Okay. Now let's go the next step and
ask what is the dot product of a matrix and a vector, okay? So we want to multiply this matrix A with this column vector C, okay? So how is that defined? We basically think about A as
consisting of two row vectors, so this row and this row, and then we basically take the dot product between this row vector
and the column vector and this row vector and
the column vector, okay? So these are the row vectors, and then we take row one times
the column vector dot product and r2 two times the column
vector, also dot product. And that gives us two numbers, first here is the dot product, and here is the second dot product, okay? Okay, so that's what a dot product between a matrix and a vector is, now for a dot product
between two matrices, okay? So when you multiply two matrices, you basically have to have, the number of rows in the first one has to be the number of
columns in the second one. So when we want to do this
product, we take the matrix A and we think about it
as three row vectors, and we take the matrix C and we think about it
as three column vectors. And so now when we wanna
take the product of A and C, we have these three row vectors and we multiply them by
these three column vectors, and we get basically a
three by three matrix where each element here is the dot product between the corresponding elements here. Okay? So this is the more
explicit way to to write it. So we can basically start with the dot product between vectors, and using this convention
of row times column, we can define dot products
between a vector and a matrix and the dot product between
a matrix and another matrix. Okay, so how is that useful? We're going to use now this
mechanism that we developed of matrix notation,
multiplication, and so on, to represent what we already talked about, which is a change of basis
using an orthonormal basis. Okay. So suppose that we have
an orthonormal basis, so a set of unit vectors that
are orthogonal to each other, and we are thinking about these
unit vectors as row vectors. So here is the row vector i,
it's a d-dimensional space, and if we can collect all
of these vectors together into a matrix by basically
just piling them up, making each row one of
the orthonormal vectors, so this gives us what is
called an orthonormal matrix. And the way that you can check that it is really an orthonormal matrix is that you take the product
of U and U transposed, and what you should
get is the unit matrix. The unit matrix is simply the matrix that has ones on the
diagonal and zero elsewhere. And that corresponds to
basically the dot product between any two different vectors is zero and the dot product of any
vector with itself is one, because the length of the vector, the norm of the vectors is one. So that gives you this simple relationship between this orthonormal matrix and what is called the unit matrix. So what is this unit matrix? The unit matrix is something that behaves like the number one in multiplication. It basically doesn't
change anything, right? So if you have any matrix A and you take A times I or I
times A, you get back A, okay? So it's the matrix that doesn't... It's basically the matrix
that acts as a unit, as an identity matrix. It doesn't change the
matrix it multiplies with. Okay, so what about changing
the coordinate basis? So here is the standard basis, we wrote it here. And then suppose that we have some other orthonormal basis U, okay? So we have vectors that are given to us in the kind of standard basis, which is just the coordinates
as they're given to us, and we have some other
basis that we want to use to represent these vectors. So what did we say that we do? When we want to calculate the coefficients that are in front of the vectors, then we basically just take
the dot product of the vector with each one of the basis vectors. And if we want to use the matrix U, then we take the dot product with the unit vectors in that matrix. Okay? So these are the
two representations. Now that we have that,
suppose that we have vs and vu that are represented as column vectors and we want to transform vs into vu. So we want to basically get the vector in its standard basis, like 1, 0, 0, 0, and then transform it into this other one. So how is that done? We basically take these
products to get the coefficient. And then what we do is we multiply this, this vector... Oh this is... Sorry. We basically transform
vs to vu is this way. And if we want to transform
in the other direction, we basically have to do
the opposite transform, which turns out to be
just U transposed, okay? So U transposed transposes from, transforms the vector
from the second basis back to the first. Now, as you might remember, this was the formula,
using vector notation, that is how to re-represent
v in this form. And so using the orthonormal basis U, and so we take the dot product, we multiply it by the first unit vector, the dot product multiplied
by the second, and so on. And so we can write that in this notation. So this long thing we can write
in a much more succinct way, which is to say U times
U transposed of V, okay? So you can check that
basically U transposed of V is simply this notation. And what does it do? It basically re-represents
v in a different basis but essentially gives you the same vector. So that equivalent to that
is that U U transposed is the identity matrix. Now, as we will see,
sometimes we don't want to use all of the vectors, all
of the basis vectors, we just want to use
somehow the basis vectors that have most information
about our vector v. So what we do is simply
we take this product here and we just use the first
k, let's say, basis vectors, and we leave outside the basis vectors that are beyond k, okay? So this doesn't give us
an exact reconstruction but it gives us an
approximate reconstruction. And the residual error, which is the difference
between the actual vector and the reconstruction, is simply the norm of this
difference vector, okay? So the norm square of
the difference vector, which is some of ri squared, that's basically the part of the vector
that is not explained by this reconstruction, or the leftover part, the residual part. So this gives us the basic tools for doing this change
of basis using matrices. And next time, we will
see how to apply that to principle component analysis, which is what we will use later on. See you then.
--- end {8.2_transcript.txt} ---
--- start{8.3_notebook.md} ---
### 8.3_notebook.md

```markdown
# Principal components analysis

```python
%pylab inline
import pandas as pd
from numpy import arange,array,ones,linalg
import warnings
warnings.filterwarnings("ignore")
"""
RETURNS ->

%pylab is deprecated, use %matplotlib inline and import the required libraries.
Populating the interactive namespace from numpy and matplotlib
"""

```

## Small example

Suppose we have 9 points on the plane, defined by their  coordinates

```python
data = array([
    [ 2.4,  0.7],
    [ 2.9, -0.7],
    [ 2.2, -1.6],
    [ 3. ,  1.2],
    [ 2.4, -0.2],
    [ 4.6,  1.1],
    [ 2.3, -2.1],
    [ 0.7, -1.3],
    [ 1.1, -0.2]])
plt.scatter(data[:,0],data[:,1])
plt.xlim(-10,10); plt.ylim(-10,10)
plt.title('original data')
grid()
"""
RETURNS ->

[Plot/Image Output: Scatter plot of original data]
"""

```

### PCA - step by step

#### 1. Mean centering

We first subtract the mean from each coordinate to center the data around the origin.

```python
mn=mean(data,axis=0)
data-=mn
plt.scatter(data[:,0],data[:,1])
plt.xlim(-4,4); plt.ylim(-4,4)
plt.title('Mean centered data')
grid()
"""
RETURNS ->

[Plot/Image Output: Scatter plot of mean centered data]
"""

```

#### 2. Covariance matrix

We calculate the covariance matrix of the data.



where  is the data matrix (n samples x d features).

```python
C = cov(data.T)
print(C)
"""
RETURNS ->

[[1.26763889 0.73902778]
 [0.73902778 1.41111111]]
"""

```

#### 3. Eigen decomposition

We compute the eigenvalues and eigenvectors of the covariance matrix.

```python
evals,evecs=linalg.eig(C)
print("Eigenvalues:",evals)
print("Eigenvectors:\n",evecs)
"""
RETURNS ->

Eigenvalues: [0.59726978 2.08148022]
Eigenvectors:
 [[-0.74106918 -0.67142751]
 [ 0.67142751 -0.74106918]]
"""

```

#### 4. Projection

The principal components are the eigenvectors corresponding to the largest eigenvalues.
We can project the data onto the principal components to reduce dimensionality.

```python
# Sort by eigenvalue in descending order
idx = argsort(evals)[::-1]
evecs = evecs[:,idx]
evals = evals[idx]

print("Sorted Eigenvalues:",evals)
print("Sorted Eigenvectors:\n",evecs)

# Project data
projected_data = dot(data, evecs)

plt.scatter(projected_data[:,0],projected_data[:,1])
plt.xlim(-4,4); plt.ylim(-4,4)
plt.title('Data projected onto PC1 and PC2')
grid()
"""
RETURNS ->

Sorted Eigenvalues: [2.08148022 0.59726978]
Sorted Eigenvectors:
 [[-0.67142751 -0.74106918]
 [-0.74106918  0.67142751]]
[Plot/Image Output: Scatter plot of projected data]
"""

```

### Explanation

* The **first principal component** (PC1) corresponds to the direction of maximum variance in the data.
* The **second principal component** (PC2) is orthogonal to PC1 and captures the remaining variance.
* The **eigenvalues** indicate the amount of variance explained by each principal component.

```python
var_explained = evals / sum(evals)
print("Variance explained by PC1:", var_explained[0])
print("Variance explained by PC2:", var_explained[1])
"""
RETURNS ->

Variance explained by PC1: 0.7770172605886616
Variance explained by PC2: 0.2229827394113384
"""

```

```

```

--- end {8.3_notebook.md} ---
--- start{8.3_transcript.txt} ---
(gentle tune plays) - [Instructor] Principle
Component Analysis, Part one, Take six. So today we're going to talk about principle component analysis. This is one of the oldest and most useful methods
in statistics, way earlier than neural networks or boosting
or anything of that type. And indeed it is still a very
powerful and useful method. And so it is worth our
while to understand how to use it and what the results mean. So what is principle component analysis? The idea basically is that you have very high dimensional data let's say 1,000
dimensions, and you want to represent the data using
something much smaller let's say 10 dimensions. So sometimes you can do that, not always, but when you can do that this kind of representation
can be found using PCA. So dimensionality reduction,
we want to reduce the number of features or dimensions in the dataset. Why do we want to do that? First, because it reduces
storage and computation. It gives us a more succinct
way to store the data and therefore to process
the data more quickly. And the high dimensional data
often has strong dependencies, strong correlations
between different features. And so we can hope to
find a representation that would have less of that redundancy
and therefore be smaller. Finally, if we have features
that have no relation to other features, those
are often if not always, features that are really just noise. So we would do better if we remove them and in
a sense, clean the data. So here is an example of a situation where we
might want to use PCA. We have images of digits. Each pixel here is a value
between zero and 255. And together they represent the image of a particular digit, handwritten digit. And the dimension of this data point is the number of pixels
that you have, which is, I believe 764 here. Okay? So the question is, are all
of these pixels really equally informative or can we remove some of them and not encode them at all? So it is pretty clear that if
we look close to the corners those pixels tend to be always black. So they vary very little
from image to image and therefore maybe we can just throw them out and not incur much of
a distortion of our image. Okay? So what we're going to try and do is remove those pixels
that have small variance. So the data is a classical
data set, it's called MNIS. And we want to know what
fraction of the total variance over all of the pixels, what are the pixels that
have the least variance? How many of them and so on. So what we can do is we
can simply sort them, we can calculate the
variance for each pixel, and then we can sort them from the least variance
to the most variance. And this is the graph that we get. Okay? So we see that the
smallest variance pixels up to like 300, they really
have very, very small variance and maybe we can just remove these pixels and just have the pixels that
are above like 300 to 764. Okay? So that is a legitimate way of going but as we'll see, we can
do significantly better. And that is by not looking just at pixels but at linear combinations of pixels. So to understand how the
redundancy or the correlation between data points
gives us the ability to reduce the dimension, let's look at the simple
two-dimensional example. So we have here two coordinates the horizontal and the vertical and the data points are
the blue points, okay? So we see that the data
points clearly have direction in which they tend to follow and their variation in the
other direction is small. So this is this direction that
has the largest variation. And what we can basically
think of is let's take each one of these points and project
it on this direction and use that as our approximation our approximate reconstruction
of the data point. Okay? That's the basic idea. So if the distance between the points and their projection and
the line, the distance between the point and the
line is simply the distance between the point and the
projection of the point. If those are all small, then we will have small
distortion in our reconstruction. And the direction that
gives us the smallest distortion is the direction
that gives us maximum variance. Okay. So to just think a little bit about these reconstruction, let's think about this same data set and think about two possible directions on which we would like
to project the data. So one direction is the green line it's a poor direction
to project the data in. And the other direction is the red line which is the best direction
to project the data. Okay, so what do we have? If we look at the red line and we look at the particular
point that we're trying to reconstruct, we see that this is the vector that
is the projection, right? So this is the point here
that is the reconstruction of the point, and then
this vector is the error. Okay? So the length of this one is the error in our reconstruction. So we see that it's pretty small. If we look at this line, on
the other hand, the green line we see that the projection is this, but the error is very large, okay? So now of course some points
will have larger error and some points have smaller error. But what we want is that on
the average, the length square of these errors would be
as small as possible, okay? So the errors will tend
to be small and that can be shown to be exactly the direction that has the maximum variance. So when we talk about
projection mathematically if you recall from the
previous slide or video, you have two ways to think
about the projection. So here's the line and
here are the data points. So if we basically just
take the dot product between the direction of
the line, which is defined by some unit vector and the
vector that defines the point then we just get a number, a scaler and that's here is the number. And if we take this number and we multiply it by the unit
vector that defines the line then we get the projection
on the line, right? So this is really the
reconstruction, okay? This is just a number, this is in here, a point in two dimensional space. So again, just as a reminder,
what is it that we're doing? We have vector X that we are projecting
onto a unit vector U. And so here is U the unit
vector and here is X. And X dot U gives us the
length of this line segment. And if we multiply this X dot
U by U again, so we take X and we multiply by U, we
get actually this vector the vector that defines
the point right here. Okay? So that point is the
projection of X onto the line. Okay. So to remind you again, we're using when we do this kind of
projection on unit vectors, we like to use an orthonormal basis which is basically a set of unit vectors that are each orthonormal to each other. And what we have is, if we write these vectors
as a1 a2 and so on we can write it as a column
vector of row vectors or as a row vector of column vectors. Okay? So this is A and
this is A transposed and it's easy to check that A times A transposed is
exactly the unit matrix, okay? The matrix that has ones along the diagonal and zeros elsewhere. And why does it have
to be that unit vector? Because we know that for
any I not equal to j ai times AI itself is one that means that it's a unit vector and AI dot times AJ is equal to zero. Meaning that each pair
of vectors is orthogonal. Okay. So a set of n
orthogonal unit vectors in our end defines an orthonormal basis and multiplying the vector by an orthonormal matrix corresponds to expressing it in terms
of the orthonormal basis. Okay? We talked about a change of basis using an orthonormal matrix. So here is how this looks as a picture. Again, we've seen this
before, but worth repeating. Suppose that we have a particular vector this red line here, that's
the vector we're interested in and we have it expressed
in the first basis. Okay? So the first basis is the black one and we have the two
coordinates, this coordinate and this coordinate that
are defining the vector in the basis one, and we
want to move it to basis two. So basis two is now a different set of two
unit vectors represented as a matrix that is a
two by two matrix here. And by using this matrix
we can re-represent the same vector using these unit vectors. Okay? The dotted blue lines
describe the coordinate in the new basis. So this is a change of basis in R 2. Okay. So that's what we're shooting for. We're going to look for a
basis, for our data such that the first, there is one coordinate. The first coordinate is going
to have the largest variance and then the second one is going to have the largest variance, but
so that it is orthogonal to the first and so on and so forth. So that would be what we're looking for. And now we're going to talk about some of the algebra
that goes into that. Okay. So concept that is
important here is the concept of an eigenvector. So a eigenvector of a matrix
M with eigenvalue lambda is vector A such that if you take m times A, the matrix times A, that simply changes just the
length of A, so it multiplies A by a scaler lambda and
doesn't change its direction. Okay? That's basically what an
eigenvector and eigenvalue are. And we have a general
theorem that is called a spectral decomposition
theorem, which says that if you have a matrix that is symmetric, symmetric matrix means that the coordinate that each cell is equal to
the cell just across from it. So aij is equal aji for any Inj. If that is the case and it's a real value, we don't need to worry about this here. Then if you, then the
matrix M can be written as A transposed times a
diagonal matrix times A. And the diagonal matrix consists of the eigen values and the orthonormal matrix
consists of the eigen vectors. Okay? So this is a general
theorem that tells us here is a way that we can always
change the coordinate of symmetric matrix so that in the new coordinate system,
the matrix is just diagonal, okay? And the standard is to say
that lambda one is the largest, lambda two is the second
largest and so on. So this is just something that holds for symmetric matrices that
are real value in general. Okay. So how can we think about this? What does it mean that this, what does this decomposition mean? It basically means that you are moving from one coordinate system to the other coordinate
system and back again. So we have this operation ATDAa, right? So this ATDA is, M is our original matrix and it represented it the following way. First we take the matrix A
and we multiply it by the, we take the vector A and we multiply it by the matrix A and we get the vector now in the a coordinate system,
in the A orthonormal basis and then we multiply each
coordinate just by lambda I. So we take each one of the coordinate and we multiply it by
the appropriate scaler and then we transform it
back to the original space. Okay? So start at the original space, move to the basis representing,
represented by A, scale each one of the coordinates
and then transpose back into the original coordinate system or the original basis. Okay. So that is the
tool that we're going to use, this decomposition. But now what is our symmetric matrix? So our symmetric matrix is going
to be the covariance matrix and I'm going to explain
what it is right now. It's basically a generalization of the notion of covariance between two random variables
to say that it's a covariance between any pair of
coordinates in our vector. Okay? So here is what we will
call our observation matrix. Okay? So each row here is one vector of observation and we
have n observations, okay? So in general, this would
be a skinny and long matrix but not so skinny because p
is the number of coordinates. So maybe P would be 365 in our examples but n would be maybe 10,000 or 100,000. And now we want to
calculate the covariance between every two coordinates. So we're going to calculate
the covariance matrix and the covariance matrix
is a symmetric matrix that captures the pairwise relationships or the pairwise correlations
between pairs of coordinates. Okay? So here is how it is calculated. We start with our
observation matrix, okay? And then we call each
observation each two-dimensional observation, we call it o. So this is the o 1 is
the first observation. Now we take the average
observation vector, okay? So that's the vector
that is simply the mean of all of these Os all
of the observations. And it gives us the mean
for the first coordinate the mean for the second
coordinate and so on. Now we center our data, we basically subtract from each coordinate we subtract the mean for that coordinate and now we get oi O1 minus mu O2 minus mu up to ON minus mu. So this is the average
corrected observation matrix. It has a zero vector as
its mean vector, okay? So we subtracted the mean.
So now it's the mean. Now the mean is zero. And now we take what we
call the outer product. We talked about that before. The outer product is basically rather than taking the dot product A, so the inner product, A
dot A is simply the sum of the dot ai but the outer dot product is
where you take each coordinate of A and you multiply it by
each other, coordinate of A and you get now an N by N
matrix where each coordinate so where the dimension here, the the output is a
matrix that has the square of the dimension of the vector A okay? So that's just multiplying
all possible pairs. And you see here in the
diagonal it's multiplying just the variable with itself. So that basically gives us the definition of the covariance matrix. So it means we take x i
minus mu, do an outer product of that vector with itself and then we average
the resulting matrices. Okay? So this gives us a matrix. Each one of these matrices
in the sum is symmetric. It's easy to see. And so therefore, covariance
X is also symmetric. Okay? So we have a symmetric
matrix like we wanted and because of that we
can apply the orthonormal decomposition, spectral decomposition, which means that we can
write Cov X as ATDA. Okay? So we can write it in
this kind of diagonal form and let's see what that means. Okay, so how do we interpret this? So the distribution of the random vector X
defines the covariance matrix and the decomposition
changes the coordinate system of x and defines a new
coordinate system where Y is A of X. Okay? So we take the original
features that we have or coordinates and we do a
orthonormal transformation and we find what is a new representation. And this new representation is nice in the sense that the covariance of that new representation
is simply a diagonal matrix. Okay? So that, that basically means what? It means that every pair of coordinates in the new basis are uncorrelated and the variance of each
one, which is the elements in the diagonal that basically is equal to the eigen vectors to the lambda eye. Okay? So we have on the Lambda
eyes we have a description of what is the variance of each vector and we know that the rest of
the covariances are all zero. So using that, we can define the best K
dimensional projection. So suppose that we have a P
times P covariance matrix of X, it's eigendecomposition can
be computed in order of P to the cubed. And what it gives us it gives us these lambda one to lambda P and it gives us the
corresponding eigenvectors mu one to mu P. Okay? So each one of those
is a unit vector length one and UI UJ is zero. So basically now we have a new basis for representing our data. Okay. So Ethereum says, that suppose we want to map
data X to just K dimensions. So we don't want all of
the original P dimensions we just want K and we
want to capture as much as possible of the variance of X. Then the best projection is to
take the K first coordinates in the order where lambda one
is larger equal to lambda two and so on. Okay? So basically by doing
this eigendecomposition we find the directions
of maximum variance. And here is what happens
if we look at the variance when we use PCA rather than using just the
individual coordinates. So this is the blue line is
the individual coordinate and we saw that after we
pass about 400, 500 here then the rest are very small. But if we use the PCA, we see
that it's even more extreme. So basically most of the variance is
explained by the first 100 and then the rest is explained
less and less and less. Okay? So this part of the distribution where we take a few coordinates and it explains most of the variance, that's exactly what
we're after when we want to do dimensionality reduction. So here is an example of image
reconstruction using PCA. So here is an image, this
is the original image. And here we have different
Ks, different approximations. So we see that if K's 50 then the approximation looks pretty poor. K is 100, it's already pretty good. It's good enough that we can
say clearly that it's a two. Okay? And then when we go to
150 and 200 it becomes better. And at 200 it's really quite
hard to see a difference from the original image,
even though it uses only 200 out of the 764. So what are these reconstructions? We talked about them in general earlier. It's basically a reconstruction
where you use only the first K coordinates after doing the orthonormal
transformation using the PCA. And they look something like this. This is just to kind of
recap, here is our data. Here it's two-dimensional,
so it's very simple and we use an ellipse to
represent the covariance okay? So because the ellipse can
represent the direction the and length of two
orthogonal directions, we can use it to represent
our covariance matrix. And what this, the
eigendecomposition gives us are two orthogonal unit vectors, U1 and U2. And we are going to use U1 and
U2 to reconstruct the data. Actually what is used here
is slightly different. These are not unit vectors but the unit vector times the lambda I so this U1 has larger length than U2. This is a standard kind of
way of representing the PCA. Okay? So to summarize, what
we have is we have a model of our distribution and
this model splits the data into the low dimensional part
and the high dimensional part. And the high dimensional
part has small variance per coordinate and we
think about it as noise. And the low dimensional part
is the part that contains most of the useful information. Now we can take that low dimensional part and use reconstruction to come up with a reconstruction
of the original vector and it would be a good
reconstruction if the distance between the reconstruction and
the original vector is small.
--- end {8.3_transcript.txt} ---
--- end {Week 4 Material} ---
--- start {Week 4 Quiz} ---
**Help prep here**
--- end {Week  Quiz} ---
